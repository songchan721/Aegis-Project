# ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª…ì„¸ì„œ (Data Pipeline Specification)

| í•­ëª© | ë‚´ìš© |
|------|------|
| ë¬¸ì„œ ID | AEG-SPC-20250917-1.0 |
| ë²„ì „ | 1.0 |
| ìµœì¢… ìˆ˜ì •ì¼ | 2025ë…„ 9ì›” 17ì¼ |
| ì‘ì„±ì | Dr. Aiden (ìˆ˜ì„ AI ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸) |
| ìƒíƒœ | í™•ì • (Finalized) |

## 1. ì„œë¹„ìŠ¤ ê°œìš” (Service Overview)

ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤ëŠ” ì´ì§€ìŠ¤ ì‹œìŠ¤í…œì˜ **ì´ì¤‘ íŠ¸ë™ íŒŒì´í”„ë¼ì¸(Dual-Track Pipeline)**ì„ êµ¬í˜„í•˜ëŠ” í•µì‹¬ ì„œë¹„ìŠ¤ì´ë‹¤. ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬(Hot Path)ì™€ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬(Cold Path)ë¥¼ í†µí•´ ë°ì´í„° ë¬´ê²°ì„±ê³¼ ì‹¤ì‹œê°„ì„±ì„ ëª¨ë‘ ë³´ì¥í•œë‹¤.

### 1.1. í•µì‹¬ ì±…ì„ (Core Responsibilities)
- ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ ì—°ë™ ë° ìˆ˜ì§‘
- ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (Hot Path)
- ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ ë° ê²€ì¦ (Cold Path)
- ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§
- ë°ì´í„° ë³€í™˜ ë° í‘œì¤€í™”

### 1.2. ì„œë¹„ìŠ¤ ê²½ê³„ (Service Boundaries)
**í¬í•¨í•˜ëŠ” ê¸°ëŠ¥:**
- ë°ì´í„° ìˆ˜ì§‘ ë° ETL í”„ë¡œì„¸ìŠ¤
- ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- ë°ì´í„° ë™ê¸°í™” ê´€ë¦¬

**í¬í•¨í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥:**
- ë°ì´í„° ì €ì¥ì†Œ ê´€ë¦¬ (ê° DB ì„œë¹„ìŠ¤ ë‹´ë‹¹)
- ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬ (ê° ë„ë©”ì¸ ì„œë¹„ìŠ¤ ë‹´ë‹¹)
- ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (Frontend ë‹´ë‹¹)

## 2. ì´ì¤‘ íŠ¸ë™ íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜

### 2.1. ì „ì²´ ì•„í‚¤í…ì²˜
```mermaid
graph TB
    subgraph "External Data Sources"
        A[ê³µê³µë°ì´í„°í¬í„¸ API]
        B[ì§€ìì²´ ì›¹ì‚¬ì´íŠ¸]
        C[ê¸ˆìœµê¸°ê´€ API]
        D[RSS/XML í”¼ë“œ]
    end
    
    subgraph "Data Collection Layer"
        E[API Collectors]
        F[Web Scrapers]
        G[File Processors]
    end
    
    subgraph "Hot Path (Real-time)"
        H[Kafka Streams]
        I[Event Processors]
        J[Real-time Validation]
    end
    
    subgraph "Cold Path (Batch)"
        K[Apache Airflow]
        L[Batch Processors]
        M[Data Quality Checks]
    end
    
    subgraph "Data Storage"
        N[(PostgreSQL)]
        O[(Milvus)]
        P[(Neo4j)]
        Q[(Elasticsearch)]
    end
    
    A --> E
    B --> F
    C --> E
    D --> G
    
    E --> H
    F --> H
    G --> H
    
    H --> I
    I --> J
    J --> N
    
    N --> K
    K --> L
    L --> M
    M --> O
    M --> P
    M --> Q
```

### 2.2. Hot Path vs Cold Path
| íŠ¹ì„± | Hot Path | Cold Path |
|------|----------|-----------|
| **ì²˜ë¦¬ ë°©ì‹** | ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° | ë°°ì¹˜ ì²˜ë¦¬ |
| **ì§€ì—° ì‹œê°„** | < 1ì´ˆ | ë¶„/ì‹œê°„ ë‹¨ìœ„ |
| **ë°ì´í„° í’ˆì§ˆ** | ê¸°ë³¸ ê²€ì¦ | ì™„ì „í•œ ê²€ì¦ |
| **ì²˜ë¦¬ëŸ‰** | ë†’ìŒ | ë§¤ìš° ë†’ìŒ |
| **ë³µì¡ì„±** | ë‹¨ìˆœ | ë³µì¡ |
| **ìš©ë„** | ì¦‰ì‹œ ë°˜ì˜ | ì •í™•ì„± ë³´ì¥ |

## 3. Hot Path êµ¬í˜„ (ì‹¤ì‹œê°„ ì²˜ë¦¬)

### 3.1. Kafka Streams ì²˜ë¦¬
```python
from kafka import KafkaProducer, KafkaConsumer
import asyncio
import json

class HotPathProcessor:
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        self.consumer = KafkaConsumer(
            'raw_policy_data',
            bootstrap_servers=['kafka:9092'],
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
    
    async def process_policy_event(self, event: dict):
        """ì •ì±… ì´ë²¤íŠ¸ ì‹¤ì‹œê°„ ì²˜ë¦¬"""
        try:
            # 1. ê¸°ë³¸ ê²€ì¦
            validated_data = await self.basic_validation(event)
            
            # 2. ë°ì´í„° ë³€í™˜
            transformed_data = await self.transform_data(validated_data)
            
            # 3. PostgreSQLì— ì¦‰ì‹œ ì €ì¥
            await self.save_to_postgres(transformed_data)
            
            # 4. ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸ ë°œí–‰
            await self.publish_downstream_event(transformed_data)
            
            logger.info(f"Hot path processed policy: {transformed_data['policy_id']}")
            
        except Exception as e:
            logger.error(f"Hot path processing failed: {e}")
            # Dead Letter Queueë¡œ ì „ì†¡
            await self.send_to_dlq(event, str(e))
    
    async def basic_validation(self, data: dict) -> dict:
        """ê¸°ë³¸ ë°ì´í„° ê²€ì¦"""
        required_fields = ['title', 'content', 'issuing_organization']
        
        for field in required_fields:
            if not data.get(field):
                raise ValidationError(f"Missing required field: {field}")
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        if not isinstance(data.get('title'), str):
            raise ValidationError("Title must be string")
        
        return data
    
    async def transform_data(self, data: dict) -> dict:
        """ë°ì´í„° ë³€í™˜"""
        return {
            'policy_id': str(uuid.uuid4()),
            'title': data['title'].strip(),
            'content': data['content'].strip(),
            'issuing_organization': data['issuing_organization'].strip(),
            'source_system': data.get('source_system', 'unknown'),
            'collected_at': datetime.utcnow().isoformat(),
            'processing_path': 'hot',
            'raw_data': data
        }
    
    async def save_to_postgres(self, data: dict):
        """PostgreSQLì— ì €ì¥"""
        async with self.db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO policies (
                    policy_id, title, content, issuing_organization,
                    source_system, collected_at, raw_data
                ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                ON CONFLICT (policy_id) DO UPDATE SET
                    title = EXCLUDED.title,
                    content = EXCLUDED.content,
                    updated_at = NOW()
            """, 
            data['policy_id'], data['title'], data['content'],
            data['issuing_organization'], data['source_system'],
            data['collected_at'], json.dumps(data['raw_data'])
            )
```

### 3.2. ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬
```python
class RealTimeEventProcessor:
    def __init__(self):
        self.kafka_consumer = KafkaConsumer('policy_events')
        self.event_handlers = {
            'policy_created': self.handle_policy_created,
            'policy_updated': self.handle_policy_updated,
            'policy_deleted': self.handle_policy_deleted
        }
    
    async def start_processing(self):
        """ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œì‘"""
        async for message in self.kafka_consumer:
            try:
                event = json.loads(message.value)
                event_type = event.get('event_type')
                
                if event_type in self.event_handlers:
                    await self.event_handlers[event_type](event)
                else:
                    logger.warning(f"Unknown event type: {event_type}")
                    
            except Exception as e:
                logger.error(f"Event processing failed: {e}")
                await self.handle_processing_error(message, e)
    
    async def handle_policy_created(self, event: dict):
        """ì •ì±… ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        policy_data = event['data']
        
        # ë²¡í„° ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸°)
        asyncio.create_task(self.generate_embedding(policy_data))
        
        # ê²€ìƒ‰ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        await self.update_search_index(policy_data)
        
        # ìºì‹œ ë¬´íš¨í™”
        await self.invalidate_cache(policy_data)
```

## 4. Cold Path êµ¬í˜„ (ë°°ì¹˜ ì²˜ë¦¬)

### 4.1. Apache Airflow DAG
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'aegis-data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'aegis_data_pipeline',
    default_args=default_args,
    description='Aegis Cold Path Data Pipeline',
    schedule_interval='@daily',
    catchup=False,
    max_active_runs=1
)

def extract_external_data(**context):
    """ì™¸ë¶€ ë°ì´í„° ì¶”ì¶œ"""
    from data_pipeline.extractors import PolicyDataExtractor
    
    extractor = PolicyDataExtractor()
    
    # ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
    public_data = extractor.extract_from_public_portal()
    
    # ì§€ìì²´ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
    local_gov_data = extractor.extract_from_local_gov()
    
    # ê¸ˆìœµê¸°ê´€ APIì—ì„œ ë°ì´í„° ì¶”ì¶œ
    financial_data = extractor.extract_from_financial_apis()
    
    # ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ì„ì‹œ ì €ì¥ì†Œì— ì €ì¥
    total_extracted = len(public_data) + len(local_gov_data) + len(financial_data)
    
    logger.info(f"Extracted {total_extracted} policy records")
    
    return {
        'public_data_count': len(public_data),
        'local_gov_data_count': len(local_gov_data),
        'financial_data_count': len(financial_data)
    }

def transform_and_validate(**context):
    """ë°ì´í„° ë³€í™˜ ë° ê²€ì¦"""
    from data_pipeline.transformers import PolicyDataTransformer
    from data_pipeline.validators import DataQualityValidator
    
    transformer = PolicyDataTransformer()
    validator = DataQualityValidator()
    
    # ì›ì‹œ ë°ì´í„° ë¡œë“œ
    raw_data = transformer.load_raw_data()
    
    # ë°ì´í„° ë³€í™˜
    transformed_data = transformer.transform_policies(raw_data)
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    validation_results = validator.validate_batch(transformed_data)
    
    # ê²€ì¦ ì‹¤íŒ¨í•œ ë°ì´í„° ì²˜ë¦¬
    failed_records = [r for r in validation_results if not r.is_valid]
    if failed_records:
        logger.warning(f"Found {len(failed_records)} invalid records")
        # ì‹¤íŒ¨í•œ ë ˆì½”ë“œë¥¼ ë³„ë„ í…Œì´ë¸”ì— ì €ì¥
        transformer.save_failed_records(failed_records)
    
    # ê²€ì¦ í†µê³¼í•œ ë°ì´í„°ë§Œ ë‹¤ìŒ ë‹¨ê³„ë¡œ
    valid_data = [r.data for r in validation_results if r.is_valid]
    transformer.save_validated_data(valid_data)
    
    return {
        'total_processed': len(transformed_data),
        'valid_records': len(valid_data),
        'failed_records': len(failed_records)
    }

def load_to_databases(**context):
    """ê²€ì¦ëœ ë°ì´í„°ë¥¼ ê° ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ"""
    from data_pipeline.loaders import DatabaseLoader
    
    loader = DatabaseLoader()
    
    # PostgreSQL ì—…ë°ì´íŠ¸
    postgres_results = loader.load_to_postgres()
    
    # Milvus ë²¡í„° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
    milvus_results = loader.load_to_milvus()
    
    # Neo4j ì§€ì‹ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
    neo4j_results = loader.load_to_neo4j()
    
    # Elasticsearch ê²€ìƒ‰ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
    es_results = loader.load_to_elasticsearch()
    
    return {
        'postgres_loaded': postgres_results['loaded_count'],
        'milvus_loaded': milvus_results['loaded_count'],
        'neo4j_loaded': neo4j_results['loaded_count'],
        'elasticsearch_loaded': es_results['loaded_count']
    }

def data_quality_check(**context):
    """ìµœì¢… ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
    from data_pipeline.quality import DataQualityChecker
    
    checker = DataQualityChecker()
    
    # ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬
    consistency_results = checker.check_cross_database_consistency()
    
    # ë°ì´í„° ì™„ì„±ë„ ê²€ì‚¬
    completeness_results = checker.check_data_completeness()
    
    # ë°ì´í„° ì‹ ì„ ë„ ê²€ì‚¬
    freshness_results = checker.check_data_freshness()
    
    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    overall_quality_score = checker.calculate_overall_quality_score(
        consistency_results, completeness_results, freshness_results
    )
    
    # í’ˆì§ˆ ì ìˆ˜ê°€ ì„ê³„ê°’ ì´í•˜ë©´ ì•Œë¦¼
    if overall_quality_score < 0.8:
        checker.send_quality_alert(overall_quality_score)
    
    return {
        'quality_score': overall_quality_score,
        'consistency_score': consistency_results['score'],
        'completeness_score': completeness_results['score'],
        'freshness_score': freshness_results['score']
    }

# Task ì •ì˜
extract_task = PythonOperator(
    task_id='extract_external_data',
    python_callable=extract_external_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform_and_validate',
    python_callable=transform_and_validate,
    dag=dag
)

load_task = PythonOperator(
    task_id='load_to_databases',
    python_callable=load_to_databases,
    dag=dag
)

quality_check_task = PythonOperator(
    task_id='data_quality_check',
    python_callable=data_quality_check,
    dag=dag
)

# Task ì˜ì¡´ì„± ì„¤ì •
extract_task >> transform_task >> load_task >> quality_check_task
```

### 4.2. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬
```python
class DataQualityManager:
    def __init__(self):
        self.quality_rules = [
            RequiredFieldRule(['title', 'content', 'issuing_organization']),
            DataTypeRule({'title': str, 'content': str}),
            LengthRule({'title': (1, 512), 'content': (10, None)}),
            FormatRule({'url': r'https?://.*'}),
            BusinessRule('funding_amount_positive'),
            ConsistencyRule('cross_database_sync')
        ]
    
    async def validate_batch(self, data_batch: List[dict]) -> List[ValidationResult]:
        """ë°°ì¹˜ ë°ì´í„° ê²€ì¦"""
        results = []
        
        for record in data_batch:
            validation_result = ValidationResult(
                record_id=record.get('policy_id'),
                is_valid=True,
                errors=[],
                warnings=[],
                quality_score=1.0
            )
            
            # ê° ê·œì¹™ ì ìš©
            for rule in self.quality_rules:
                rule_result = await rule.validate(record)
                
                if not rule_result.is_valid:
                    validation_result.is_valid = False
                    validation_result.errors.extend(rule_result.errors)
                
                validation_result.warnings.extend(rule_result.warnings)
                validation_result.quality_score *= rule_result.score
            
            results.append(validation_result)
        
        return results
    
    async def check_cross_database_consistency(self) -> ConsistencyReport:
        """ë°ì´í„°ë² ì´ìŠ¤ ê°„ ì¼ê´€ì„± ê²€ì‚¬"""
        # PostgreSQLì˜ ì •ì±… ìˆ˜
        pg_count = await self.postgres.count_active_policies()
        
        # Milvusì˜ ë²¡í„° ìˆ˜
        milvus_count = await self.milvus.count_vectors()
        
        # Neo4jì˜ ë…¸ë“œ ìˆ˜
        neo4j_count = await self.neo4j.count_policy_nodes()
        
        # Elasticsearchì˜ ë¬¸ì„œ ìˆ˜
        es_count = await self.elasticsearch.count_documents()
        
        # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        counts = [pg_count, milvus_count, neo4j_count, es_count]
        max_count = max(counts)
        min_count = min(counts)
        
        consistency_score = min_count / max_count if max_count > 0 else 0
        
        return ConsistencyReport(
            postgres_count=pg_count,
            milvus_count=milvus_count,
            neo4j_count=neo4j_count,
            elasticsearch_count=es_count,
            consistency_score=consistency_score,
            is_consistent=consistency_score > 0.95
        )
```

---

**ğŸ“‹ ê´€ë ¨ ë¬¸ì„œ**
- [ì´ì¤‘ íŠ¸ë™ íŒŒì´í”„ë¼ì¸](../../02_CORE_COMPONENTS/01_DUAL_TRACK_PIPELINE.md)
- [ë°ì´í„° ì•„í‚¤í…ì²˜](../../01_ARCHITECTURE/03_DATA_ARCHITECTURE.md)
- [ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ](../../03_DATA_AND_APIS/03_EVENT_SCHEMA.md)
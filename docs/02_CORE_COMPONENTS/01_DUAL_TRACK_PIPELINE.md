# ì´ì¤‘ íŠ¸ë™ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìƒì„¸ ëª…ì„¸ì„œ

| í•­ëª© | ë‚´ìš© |
|------|------|
| ë¬¸ì„œ ID | AEG-CMP-20250917-2.0 |
| ë²„ì „ | 2.0 |
| ìµœì¢… ìˆ˜ì •ì¼ | 2025ë…„ 9ì›” 17ì¼ |
| ì‘ì„±ì | Dr. Aiden (ìˆ˜ì„ AI ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸) |
| ê²€í† ì | ë°ì´í„° ì•„í‚¤í…ì²˜ íŒ€ |
| ìŠ¹ì¸ì | CTO |
| ìƒíƒœ | í™•ì • (Finalized) |

## 1. ê°œìš” (Overview)

ë³¸ ë¬¸ì„œëŠ” ì´ì§€ìŠ¤ 4ëŒ€ í•µì‹¬ ì›ì¹™ ì¤‘ **ì œ 1ì›ì¹™: ë°ì´í„° ë¬´ê²°ì„±**ì„ êµ¬í˜„í•˜ëŠ” **ì´ì¤‘ íŠ¸ë™ ë°ì´í„° íŒŒì´í”„ë¼ì¸**ì˜ ìƒì„¸ ì„¤ê³„ ëª…ì„¸ë¥¼ ì •ì˜í•œë‹¤. ì´ íŒŒì´í”„ë¼ì¸ì€ **ì‹¤ì‹œê°„ì„±**ê³¼ **ìµœì¢…ì  ì¼ê´€ì„±**ì„ ëª¨ë‘ ë³´ì¥í•˜ëŠ” Lambda Architecture íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ëœë‹¤.

### 1.1. ì„¤ê³„ ëª©í‘œ
- **ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬**: ë°ì´í„° ë³€ê²½ì„ ìˆ˜ ì´ˆ ë‚´ì— ì‹œìŠ¤í…œ ì „ì²´ì— ë°˜ì˜
- **ìµœì¢…ì  ì¼ê´€ì„±**: ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ ë°ì´í„° ì •í•©ì„± ë³´ì¥
- **ì¥ì•  ë³µêµ¬**: ìë™í™”ëœ ì˜¤ë¥˜ ê°ì§€ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- **í™•ì¥ì„±**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ë° ìˆ˜í‰ì  í™•ì¥ ì§€ì›
- **ê´€ì°°ê°€ëŠ¥ì„±**: ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì˜ ì¶”ì  ë° ëª¨ë‹ˆí„°ë§

### 1.2. ì•„í‚¤í…ì²˜ ì›ì¹™
- **ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì› (SSoT)**: PostgreSQLì´ ëª¨ë“  ë°ì´í„°ì˜ ìµœì¢… ê¶Œìœ„
- **ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜**: ëª¨ë“  ë°ì´í„° ë³€ê²½ì„ ì´ë²¤íŠ¸ë¡œ ì²˜ë¦¬
- **ë©±ë“±ì„± (Idempotency)**: ë™ì¼í•œ ì‘ì—…ì˜ ë°˜ë³µ ì‹¤í–‰ì´ ì•ˆì „
- **ë³´ìƒ íŠ¸ëœì­ì…˜ (Compensating Transaction)**: ì‹¤íŒ¨ ì‹œ ìë™ ë¡¤ë°±

## 2. ì´ì¤‘ íŠ¸ë™ ì•„í‚¤í…ì²˜ ê°œìš”

### 2.1. ì „ì²´ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "Data Sources"
        A[ê³µê³µë°ì´í„°í¬í„¸]
        B[ì§€ìì²´ ì›¹ì‚¬ì´íŠ¸]
        C[ê¸ˆìœµê¸°ê´€ API]
    end
    
    subgraph "Data Ingestion"
        D[Data Collectors]
        E[API Clients]
        F[Web Scrapers]
    end
    
    subgraph "Single Source of Truth"
        G[(PostgreSQL<br/>Primary Database)]
    end
    
    subgraph "Change Data Capture"
        H[Debezium Connector]
        I[Kafka Connect]
    end
    
    subgraph "Event Streaming"
        J[Apache Kafka]
        K[Policy Events Topic]
        L[User Events Topic]
        M[System Events Topic]
    end
    
    subgraph "Hot Path (Real-time)"
        N[Event Consumers]
        O[Stream Processors]
        P[Real-time Transformers]
    end
    
    subgraph "Cold Path (Batch)"
        Q[Apache Airflow]
        R[ETL Jobs]
        S[Data Validators]
        T[Reconciliation Jobs]
    end
    
    subgraph "Derived Data Stores"
        U[(Milvus<br/>Vector DB)]
        V[(Neo4j<br/>Graph DB)]
        W[(Redis<br/>Cache)]
    end
    
    A --> D
    B --> E
    C --> F
    
    D --> G
    E --> G
    F --> G
    
    G --> H
    H --> I
    I --> J
    
    J --> K
    J --> L
    J --> M
    
    K --> N
    L --> N
    M --> N
    
    N --> O
    O --> P
    P --> U
    P --> V
    P --> W
    
    G --> Q
    Q --> R
    R --> S
    S --> T
    
    T --> U
    T --> V
    T --> W
```

### 2.2. ë°ì´í„° íë¦„ íŒ¨í„´

#### 2.2.1. í•« íŒ¨ìŠ¤ (Hot Path) - ì‹¤ì‹œê°„ ì²˜ë¦¬
```
ë°ì´í„° ë³€ê²½ â†’ CDC â†’ Kafka â†’ Stream Processing â†’ íŒŒìƒ ë°ì´í„° ì €ì¥ì†Œ
     â†“           â†“      â†“           â†“                    â†“
  PostgreSQL â†’ Debezium â†’ Events â†’ Consumers â†’ Milvus/Neo4j/Redis
  (ìˆ˜ ì´ˆ)      (ìˆ˜ ì´ˆ)   (ìˆ˜ ì´ˆ)   (ìˆ˜ ì´ˆ)        (ìˆ˜ ì´ˆ)
```

#### 2.2.2. ì½œë“œ íŒ¨ìŠ¤ (Cold Path) - ë°°ì¹˜ ì²˜ë¦¬
```
ìŠ¤ì¼€ì¤„ íŠ¸ë¦¬ê±° â†’ ì „ì²´ ë°ì´í„° ê²€ì¦ â†’ ë¶ˆì¼ì¹˜ ê°ì§€ â†’ ìˆ˜ì • â†’ íŒŒìƒ ë°ì´í„° ë™ê¸°í™”
      â†“              â†“              â†“         â†“           â†“
   Airflow â†’ PostgreSQL ìŠ¤ìº” â†’ ì°¨ì´ ë¶„ì„ â†’ ë°ì´í„° ìˆ˜ì • â†’ ì¬ì²˜ë¦¬
   (ì¼ ë‹¨ìœ„)      (ë¶„ ë‹¨ìœ„)        (ë¶„ ë‹¨ìœ„)   (ë¶„ ë‹¨ìœ„)    (ë¶„ ë‹¨ìœ„)
```

## 3. í•« íŒ¨ìŠ¤ (Hot Path) ìƒì„¸ ì„¤ê³„

### 3.1. Change Data Capture (CDC) êµ¬ì„±

#### 3.1.1. Debezium ì»¤ë„¥í„° ì„¤ì •
```json
{
  "name": "aegis-postgres-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "postgres-primary",
    "database.port": "5432",
    "database.user": "debezium_user",
    "database.password": "${DEBEZIUM_PASSWORD}",
    "database.dbname": "aegis",
    "database.server.name": "aegis-db",
    "table.include.list": "public.policies,public.users,public.business_rules",
    "plugin.name": "pgoutput",
    "slot.name": "aegis_slot",
    "publication.name": "aegis_publication",
    "transforms": "unwrap",
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
    "transforms.unwrap.drop.tombstones": "false",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "topic.prefix": "aegis"
  }
}
```

#### 3.1.2. PostgreSQL ì„¤ì •
```sql
-- WAL ë ˆë²¨ ì„¤ì •
ALTER SYSTEM SET wal_level = logical;
ALTER SYSTEM SET max_replication_slots = 10;
ALTER SYSTEM SET max_wal_senders = 10;

-- Publication ìƒì„±
CREATE PUBLICATION aegis_publication FOR TABLE policies, users, business_rules;

-- Debezium ì‚¬ìš©ì ìƒì„±
CREATE USER debezium_user WITH REPLICATION PASSWORD 'secure_password';
GRANT SELECT ON ALL TABLES IN SCHEMA public TO debezium_user;
GRANT USAGE ON SCHEMA public TO debezium_user;
```

### 3.2. ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ì •ì˜

#### 3.2.1. ì •ì±… ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ
```json
{
  "schema": {
    "type": "struct",
    "fields": [
      {"field": "before", "type": "struct", "optional": true},
      {"field": "after", "type": "struct", "optional": true},
      {"field": "source", "type": "struct"},
      {"field": "op", "type": "string"},
      {"field": "ts_ms", "type": "int64"},
      {"field": "transaction", "type": "struct", "optional": true}
    ]
  },
  "payload": {
    "before": null,
    "after": {
      "policy_id": "POL-2025-001",
      "title": "ì²­ë…„ ì°½ì—… íŠ¹ë¡€ ë³´ì¦",
      "content": "ì²­ë…„ ì°½ì—…ìë¥¼ ìœ„í•œ íŠ¹ë³„ ë³´ì¦ í”„ë¡œê·¸ë¨...",
      "issuing_organization": "ê¸°ìˆ ë³´ì¦ê¸°ê¸ˆ",
      "target_regions": ["11", "26", "27"],
      "target_industries": ["J", "M", "N"],
      "funding_amount_min": 10000000,
      "funding_amount_max": 500000000,
      "interest_rate": 2.5,
      "application_start_date": "2025-01-01",
      "application_end_date": "2025-12-31",
      "created_at": "2025-09-17T10:30:00Z",
      "updated_at": "2025-09-17T10:30:00Z"
    },
    "source": {
      "version": "1.9.0",
      "connector": "postgresql",
      "name": "aegis-db",
      "ts_ms": 1663405368000,
      "snapshot": "false",
      "db": "aegis",
      "schema": "public",
      "table": "policies",
      "txId": 12345,
      "lsn": 67890
    },
    "op": "c",
    "ts_ms": 1663405368000,
    "transaction": {
      "id": "12345",
      "total_order": 1,
      "data_collection_order": 1
    }
  }
}
```

### 3.3. ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬

#### 3.3.1. ì´ë²¤íŠ¸ ì»¨ìŠˆë¨¸ êµ¬í˜„
```python
import asyncio
import json
from typing import Dict, Any, List
from kafka import KafkaConsumer
from kafka.errors import KafkaError
import logging

class PolicyEventConsumer:
    """ì •ì±… ì´ë²¤íŠ¸ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì»¨ìŠˆë¨¸"""
    
    def __init__(self, kafka_config: Dict[str, Any]):
        self.kafka_config = kafka_config
        self.consumer = None
        self.processors = {
            'c': self.handle_create_event,
            'u': self.handle_update_event,
            'd': self.handle_delete_event
        }
        self.milvus_client = MilvusClient()
        self.neo4j_client = Neo4jClient()
        self.redis_client = RedisClient()
        
    async def start_consuming(self):
        """ì´ë²¤íŠ¸ ì†Œë¹„ ì‹œì‘"""
        self.consumer = KafkaConsumer(
            'aegis.public.policies',
            bootstrap_servers=self.kafka_config['bootstrap_servers'],
            group_id='policy-processor-group',
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            enable_auto_commit=False,
            max_poll_records=100
        )
        
        try:
            for message in self.consumer:
                await self.process_message(message)
        except KafkaError as e:
            logging.error(f"Kafka error: {e}")
            raise
        finally:
            self.consumer.close()
    
    async def process_message(self, message):
        """ê°œë³„ ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            event_data = message.value
            operation = event_data['payload']['op']
            
            if operation in self.processors:
                await self.processors[operation](event_data)
                
            # ìˆ˜ë™ ì»¤ë°‹ (ì²˜ë¦¬ ì™„ë£Œ í›„)
            self.consumer.commit()
            
        except Exception as e:
            logging.error(f"Message processing failed: {e}")
            # ì—ëŸ¬ ì²˜ë¦¬ ë¡œì§ (DLQ ì „ì†¡ ë“±)
            await self.handle_processing_error(message, e)
    
    async def handle_create_event(self, event_data: Dict[str, Any]):
        """ì •ì±… ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        policy_data = event_data['payload']['after']
        policy_id = policy_data['policy_id']
        
        try:
            # 1. ë²¡í„° ì„ë² ë”© ìƒì„± ë° Milvus ì €ì¥
            await self.create_vector_embedding(policy_data)
            
            # 2. ì§€ì‹ ê·¸ë˜í”„ ë…¸ë“œ ìƒì„± ë° Neo4j ì €ì¥
            await self.create_graph_node(policy_data)
            
            # 3. ìºì‹œ ë¬´íš¨í™”
            await self.invalidate_related_cache(policy_id)
            
            logging.info(f"Policy created successfully: {policy_id}")
            
        except Exception as e:
            logging.error(f"Failed to process create event for {policy_id}: {e}")
            # ë³´ìƒ íŠ¸ëœì­ì…˜ ì‹¤í–‰
            await self.compensate_create_failure(policy_id)
            raise
    
    async def handle_update_event(self, event_data: Dict[str, Any]):
        """ì •ì±… ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        before_data = event_data['payload']['before']
        after_data = event_data['payload']['after']
        policy_id = after_data['policy_id']
        
        try:
            # ë³€ê²½ëœ í•„ë“œ ê°ì§€
            changed_fields = self.detect_changed_fields(before_data, after_data)
            
            # ë²¡í„° ê´€ë ¨ í•„ë“œ ë³€ê²½ ì‹œ ì„ë² ë”© ì—…ë°ì´íŠ¸
            if self.requires_vector_update(changed_fields):
                await self.update_vector_embedding(after_data)
            
            # ê·¸ë˜í”„ ê´€ë ¨ í•„ë“œ ë³€ê²½ ì‹œ ë…¸ë“œ ì—…ë°ì´íŠ¸
            if self.requires_graph_update(changed_fields):
                await self.update_graph_node(after_data)
            
            # ìºì‹œ ë¬´íš¨í™”
            await self.invalidate_related_cache(policy_id)
            
            logging.info(f"Policy updated successfully: {policy_id}")
            
        except Exception as e:
            logging.error(f"Failed to process update event for {policy_id}: {e}")
            await self.compensate_update_failure(policy_id, before_data)
            raise
    
    async def handle_delete_event(self, event_data: Dict[str, Any]):
        """ì •ì±… ì‚­ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        policy_data = event_data['payload']['before']
        policy_id = policy_data['policy_id']
        
        try:
            # 1. Milvusì—ì„œ ë²¡í„° ì‚­ì œ
            await self.delete_vector_embedding(policy_id)
            
            # 2. Neo4jì—ì„œ ë…¸ë“œ ë° ê´€ê³„ ì‚­ì œ
            await self.delete_graph_node(policy_id)
            
            # 3. ìºì‹œì—ì„œ ì‚­ì œ
            await self.delete_from_cache(policy_id)
            
            logging.info(f"Policy deleted successfully: {policy_id}")
            
        except Exception as e:
            logging.error(f"Failed to process delete event for {policy_id}: {e}")
            # ì‚­ì œ ì‹¤íŒ¨ ì‹œ ë³µêµ¬ëŠ” ì½œë“œ íŒ¨ìŠ¤ì—ì„œ ì²˜ë¦¬
            raise
```

#### 3.3.2. ë²¡í„° ì„ë² ë”© ì²˜ë¦¬
```python
class VectorEmbeddingProcessor:
    """ë²¡í„° ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬"""
    
    def __init__(self):
        self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        self.milvus_client = MilvusClient()
    
    async def create_vector_embedding(self, policy_data: Dict[str, Any]):
        """ì •ì±… ë°ì´í„°ì˜ ë²¡í„° ì„ë² ë”© ìƒì„±"""
        policy_id = policy_data['policy_id']
        
        # ì„ë² ë”© ëŒ€ìƒ í…ìŠ¤íŠ¸ êµ¬ì„±
        embedding_text = self.compose_embedding_text(policy_data)
        
        # ë²¡í„° ì„ë² ë”© ìƒì„±
        embedding = self.embedding_model.encode(embedding_text)
        
        # Milvusì— ì €ì¥í•  ë°ì´í„° êµ¬ì„±
        milvus_data = {
            'policy_id': policy_id,
            'embedding': embedding.tolist(),
            'title': policy_data['title'],
            'target_regions': ','.join(policy_data.get('target_regions', [])),
            'target_industries': ','.join(policy_data.get('target_industries', [])),
            'funding_amount_min': policy_data.get('funding_amount_min', 0),
            'funding_amount_max': policy_data.get('funding_amount_max', 0),
            'interest_rate': policy_data.get('interest_rate', 0.0),
            'created_at': policy_data['created_at']
        }
        
        # Milvus ì»¬ë ‰ì…˜ì— ì‚½ì…
        await self.milvus_client.insert('policy_embeddings', [milvus_data])
        
        logging.info(f"Vector embedding created for policy: {policy_id}")
    
    def compose_embedding_text(self, policy_data: Dict[str, Any]) -> str:
        """ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ êµ¬ì„±"""
        components = [
            policy_data.get('title', ''),
            policy_data.get('content', ''),
            policy_data.get('issuing_organization', ''),
            f"ì§€ì›ê¸ˆì•¡: {policy_data.get('funding_amount_min', 0)}ì› ~ {policy_data.get('funding_amount_max', 0)}ì›",
            f"ê¸ˆë¦¬: {policy_data.get('interest_rate', 0)}%"
        ]
        
        return ' '.join(filter(None, components))
```

### 3.4. ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬

#### 3.4.1. ë³´ìƒ íŠ¸ëœì­ì…˜ íŒ¨í„´
```python
class CompensationManager:
    """ë³´ìƒ íŠ¸ëœì­ì…˜ ê´€ë¦¬"""
    
    def __init__(self):
        self.milvus_client = MilvusClient()
        self.neo4j_client = Neo4jClient()
        self.redis_client = RedisClient()
    
    async def compensate_create_failure(self, policy_id: str):
        """ìƒì„± ì‹¤íŒ¨ ì‹œ ë³´ìƒ íŠ¸ëœì­ì…˜"""
        try:
            # ë¶€ë¶„ì ìœ¼ë¡œ ìƒì„±ëœ ë°ì´í„° ì •ë¦¬
            await self.milvus_client.delete('policy_embeddings', f'policy_id == "{policy_id}"')
            await self.neo4j_client.delete_node('Policy', {'policy_id': policy_id})
            await self.redis_client.delete(f'policy:{policy_id}')
            
            logging.info(f"Compensation completed for failed create: {policy_id}")
            
        except Exception as e:
            logging.error(f"Compensation failed for {policy_id}: {e}")
            # ë³´ìƒ ì‹¤íŒ¨ëŠ” ì½œë“œ íŒ¨ìŠ¤ì—ì„œ ì²˜ë¦¬
    
    async def compensate_update_failure(self, policy_id: str, original_data: Dict[str, Any]):
        """ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ì‹œ ë³´ìƒ íŠ¸ëœì­ì…˜"""
        try:
            # ì›ë³¸ ë°ì´í„°ë¡œ ë³µì›
            await self.restore_original_data(policy_id, original_data)
            
            logging.info(f"Compensation completed for failed update: {policy_id}")
            
        except Exception as e:
            logging.error(f"Update compensation failed for {policy_id}: {e}")
```

## 4. ì½œë“œ íŒ¨ìŠ¤ (Cold Path) ìƒì„¸ ì„¤ê³„

### 4.1. Apache Airflow DAG êµ¬ì„±

#### 4.1.1. ë©”ì¸ ë°ì´í„° ë™ê¸°í™” DAG
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
from datetime import timedelta

default_args = {
    'owner': 'aegis-data-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=4)
}

dag = DAG(
    'aegis_data_consistency_check',
    default_args=default_args,
    description='Daily data consistency verification and correction',
    schedule_interval='0 3 * * *',  # ë§¤ì¼ 03:00 KST
    catchup=False,
    max_active_runs=1,
    tags=['data', 'consistency', 'batch']
)

# Task 1: ë°ì´í„° ì†ŒìŠ¤ ìˆ˜ì§‘
extract_external_data = PythonOperator(
    task_id='extract_external_data',
    python_callable=extract_policy_data_from_sources,
    dag=dag
)

# Task 2: ë°ì´í„° ê²€ì¦ ë° ìŠ¤í…Œì´ì§•
validate_and_stage = PythonOperator(
    task_id='validate_and_stage',
    python_callable=validate_and_stage_data,
    dag=dag
)

# Task 3: PostgreSQL ë™ê¸°í™”
sync_postgresql = PythonOperator(
    task_id='sync_postgresql',
    python_callable=sync_primary_database,
    dag=dag
)

# Task 4: ë²¡í„° ë°ì´í„° ë™ê¸°í™”
sync_vector_data = PythonOperator(
    task_id='sync_vector_data',
    python_callable=sync_milvus_data,
    dag=dag
)

# Task 5: ê·¸ë˜í”„ ë°ì´í„° ë™ê¸°í™”
sync_graph_data = PythonOperator(
    task_id='sync_graph_data',
    python_callable=sync_neo4j_data,
    dag=dag
)

# Task 6: ì¼ê´€ì„± ê²€ì¦
verify_consistency = PythonOperator(
    task_id='verify_consistency',
    python_callable=verify_data_consistency,
    dag=dag
)

# Task 7: ì •ë¦¬ ì‘ì—…
cleanup_orphaned_data = PythonOperator(
    task_id='cleanup_orphaned_data',
    python_callable=cleanup_orphaned_records,
    dag=dag
)

# Task ì˜ì¡´ì„± ì •ì˜
extract_external_data >> validate_and_stage >> sync_postgresql
sync_postgresql >> [sync_vector_data, sync_graph_data]
[sync_vector_data, sync_graph_data] >> verify_consistency >> cleanup_orphaned_data
```

#### 4.1.2. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ë¡œì§
```python
class DataConsistencyVerifier:
    """ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
    
    def __init__(self):
        self.postgres_client = PostgreSQLClient()
        self.milvus_client = MilvusClient()
        self.neo4j_client = Neo4jClient()
        self.redis_client = RedisClient()
    
    async def verify_data_consistency(self):
        """ì „ì²´ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
        consistency_report = {
            'timestamp': datetime.utcnow().isoformat(),
            'checks': {},
            'inconsistencies': [],
            'actions_taken': []
        }
        
        # 1. PostgreSQL vs Milvus ì¼ê´€ì„± ê²€ì¦
        milvus_consistency = await self.verify_milvus_consistency()
        consistency_report['checks']['milvus'] = milvus_consistency
        
        # 2. PostgreSQL vs Neo4j ì¼ê´€ì„± ê²€ì¦
        neo4j_consistency = await self.verify_neo4j_consistency()
        consistency_report['checks']['neo4j'] = neo4j_consistency
        
        # 3. ìºì‹œ ì¼ê´€ì„± ê²€ì¦
        cache_consistency = await self.verify_cache_consistency()
        consistency_report['checks']['cache'] = cache_consistency
        
        # 4. ë¶ˆì¼ì¹˜ í•´ê²°
        if milvus_consistency['inconsistent_count'] > 0:
            await self.resolve_milvus_inconsistencies(milvus_consistency['inconsistencies'])
        
        if neo4j_consistency['inconsistent_count'] > 0:
            await self.resolve_neo4j_inconsistencies(neo4j_consistency['inconsistencies'])
        
        # 5. ë³´ê³ ì„œ ìƒì„±
        await self.generate_consistency_report(consistency_report)
        
        return consistency_report
    
    async def verify_milvus_consistency(self) -> Dict[str, Any]:
        """Milvus ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
        # PostgreSQLì—ì„œ ëª¨ë“  ì •ì±… ID ì¡°íšŒ
        postgres_policies = await self.postgres_client.fetch_all(
            "SELECT policy_id, updated_at FROM policies WHERE is_active = true"
        )
        
        # Milvusì—ì„œ ëª¨ë“  ì •ì±… ID ì¡°íšŒ
        milvus_policies = await self.milvus_client.query(
            collection_name='policy_embeddings',
            expr='',
            output_fields=['policy_id', 'created_at']
        )
        
        postgres_ids = {p['policy_id']: p['updated_at'] for p in postgres_policies}
        milvus_ids = {p['policy_id']: p['created_at'] for p in milvus_policies}
        
        # ë¶ˆì¼ì¹˜ ê°ì§€
        missing_in_milvus = set(postgres_ids.keys()) - set(milvus_ids.keys())
        orphaned_in_milvus = set(milvus_ids.keys()) - set(postgres_ids.keys())
        
        inconsistencies = []
        
        # ëˆ„ë½ëœ ë°ì´í„°
        for policy_id in missing_in_milvus:
            inconsistencies.append({
                'type': 'missing_in_milvus',
                'policy_id': policy_id,
                'action': 'create_embedding'
            })
        
        # ê³ ì•„ ë°ì´í„°
        for policy_id in orphaned_in_milvus:
            inconsistencies.append({
                'type': 'orphaned_in_milvus',
                'policy_id': policy_id,
                'action': 'delete_embedding'
            })
        
        return {
            'total_postgres': len(postgres_ids),
            'total_milvus': len(milvus_ids),
            'missing_count': len(missing_in_milvus),
            'orphaned_count': len(orphaned_in_milvus),
            'inconsistent_count': len(inconsistencies),
            'inconsistencies': inconsistencies
        }
```

---

**ğŸ“‹ ê´€ë ¨ ë¬¸ì„œ**
- [Interactive AI Core](./02_INTERACTIVE_AI_CORE.md)
- [ë°ì´í„° ì•„í‚¤í…ì²˜](../01_ARCHITECTURE/03_DATA_ARCHITECTURE.md)
- [ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ](../03_DATA_AND_APIS/03_EVENT_SCHEMA.md)
- [ë°ì´í„° íŒŒì´í”„ë¼ì¸ Spec](../07_SPECIFICATIONS/DATA_PIPELINE/01_DATA_PIPELINE_OVERVIEW.md)
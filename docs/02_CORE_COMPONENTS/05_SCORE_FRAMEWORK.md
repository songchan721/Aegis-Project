# S.C.O.R.E. í”„ë ˆì„ì›Œí¬ ìƒì„¸ ëª…ì„¸

| í•­ëª© | ë‚´ìš© |
|------|------|
| ë¬¸ì„œ ID | AEG-CMP-20250917-1.0 |
| ë²„ì „ | 1.0 |
| ìµœì¢… ìˆ˜ì •ì¼ | 2025ë…„ 9ì›” 17ì¼ |
| ìƒíƒœ | í™•ì • (Finalized) |

## 1. S.C.O.R.E. í”„ë ˆì„ì›Œí¬ ê°œìš”

### 1.1. ì •ì˜ ë° ëª©ì 
**S.C.O.R.E. í”„ë ˆì„ì›Œí¬**ëŠ” ì´ì§€ìŠ¤ ì‹œìŠ¤í…œì˜ ëª¨ë“  AI íŒë‹¨ê³¼ ì¶”ì²œ ê²°ê³¼ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ì„¤ëª… ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” í•µì‹¬ ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” "ë¸”ë™ë°•ìŠ¤" AIë¥¼ "í™”ì´íŠ¸ë°•ìŠ¤" AIë¡œ ì „í™˜í•˜ì—¬ ì‚¬ìš©ìì™€ ì‹œìŠ¤í…œ ìš´ì˜ì ëª¨ë‘ì—ê²Œ ì™„ì „í•œ íˆ¬ëª…ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

### 1.2. 5ê°€ì§€ í•µì‹¬ ì›ì¹™

#### **S - Specificity (êµ¬ì²´ì„±)**
- **ì •ì˜**: ëª¨ë“  ì…ë ¥ê³¼ ì¶œë ¥ì´ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì •ì˜ë˜ì–´ì•¼ í•¨
- **ëª©ì **: ëª¨í˜¸í•¨ì„ ì œê±°í•˜ê³  ì •í™•í•œ ì²˜ë¦¬ ë³´ì¥
- **ì¸¡ì •**: ì…ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ìœ¨, ë°ì´í„° íƒ€ì… ì •í™•ì„±

#### **C - Consistency (ì¼ê´€ì„±)**
- **ì •ì˜**: ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ í•­ìƒ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë³´ì¥
- **ëª©ì **: ì‹œìŠ¤í…œì˜ ì‹ ë¢°ì„±ê³¼ ì˜ˆì¸¡ ê°€ëŠ¥ì„± í™•ë³´
- **ì¸¡ì •**: ì¬í˜„ìœ¨, ê²°ê³¼ ë³€ë™ì„± ì§€í‘œ

#### **O - Observability (ê´€ì°°ê°€ëŠ¥ì„±)**
- **ì •ì˜**: ëª¨ë“  ì²˜ë¦¬ ê³¼ì •ê³¼ ì˜ì‚¬ê²°ì • ë‹¨ê³„ë¥¼ ì¶”ì í•˜ê³  ê´€ì°° ê°€ëŠ¥
- **ëª©ì **: ì‹œìŠ¤í…œ ë™ì‘ì˜ ì™„ì „í•œ ê°€ì‹œì„± ì œê³µ
- **ì¸¡ì •**: ë¡œê·¸ ì»¤ë²„ë¦¬ì§€, ë©”íŠ¸ë¦­ ìˆ˜ì§‘ë¥ , ì¶”ì  ì™„ì„±ë„

#### **R - Reproducibility (ì¬í˜„ê°€ëŠ¥ì„±)**
- **ì •ì˜**: ë™ì¼í•œ ì¡°ê±´ì—ì„œ ê²°ê³¼ë¥¼ ì™„ë²½í•˜ê²Œ ì¬í˜„í•  ìˆ˜ ìˆì–´ì•¼ í•¨
- **ëª©ì **: ë””ë²„ê¹…, ê°ì‚¬, í’ˆì§ˆ ë³´ì¦ ì§€ì›
- **ì¸¡ì •**: ì¬í˜„ ì„±ê³µë¥ , í™˜ê²½ ì¼ê´€ì„± ì§€í‘œ

#### **E - Explainability (ì„¤ëª…ê°€ëŠ¥ì„±)**
- **ì •ì˜**: ëª¨ë“  íŒë‹¨ì˜ ê·¼ê±°ì™€ ê³¼ì •ì„ ì‚¬ìš©ìê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…
- **ëª©ì **: ì‚¬ìš©ì ì‹ ë¢° êµ¬ì¶• ë° ì˜ì‚¬ê²°ì • ì§€ì›
- **ì¸¡ì •**: ì„¤ëª… ì™„ì„±ë„, ì‚¬ìš©ì ì´í•´ë„, ê·¼ê±° ì •í™•ì„±

## 2. ê° ì›ì¹™ë³„ êµ¬í˜„ ìƒì„¸

### 2.1. Specificity (êµ¬ì²´ì„±) êµ¬í˜„

#### ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
```python
from pydantic import BaseModel, Field
from typing import List, Optional
from enum import Enum

class BusinessType(str, Enum):
    SMALL_BUSINESS = "ì†Œìƒê³µì¸"
    MEDIUM_ENTERPRISE = "ì¤‘ì†Œê¸°ì—…"
    INDIVIDUAL = "ê°œì¸ì‚¬ì—…ì"

class UserProfile(BaseModel):
    """ì‚¬ìš©ì í”„ë¡œí•„ - êµ¬ì²´ì  ìŠ¤í‚¤ë§ˆ ì •ì˜"""
    user_id: str = Field(..., description="ì‚¬ìš©ì ê³ ìœ  ID")
    business_type: BusinessType = Field(..., description="ì‚¬ì—…ì ìœ í˜•")
    industry_code: str = Field(..., regex=r"^\d{2,5}$", description="í•œêµ­í‘œì¤€ì‚°ì—…ë¶„ë¥˜ì½”ë“œ")
    annual_revenue: Optional[int] = Field(None, ge=0, description="ì—°ë§¤ì¶œì•¡ (ì›)")
    employee_count: int = Field(..., ge=0, le=1000, description="ì§ì› ìˆ˜")
    establishment_date: str = Field(..., regex=r"^\d{4}-\d{2}-\d{2}$", description="ì„¤ë¦½ì¼ (YYYY-MM-DD)")
    region_code: str = Field(..., regex=r"^\d{2}$", description="ì§€ì—­ì½”ë“œ")

class RecommendationRequest(BaseModel):
    """ì¶”ì²œ ìš”ì²­ - ëª…í™•í•œ ì…ë ¥ ì •ì˜"""
    user_profile: UserProfile
    query: str = Field(..., min_length=1, max_length=500, description="ìì—°ì–´ ì¿¼ë¦¬")
    max_results: int = Field(10, ge=1, le=50, description="ìµœëŒ€ ê²°ê³¼ ìˆ˜")
    include_expired: bool = Field(False, description="ë§Œë£Œëœ ì •ì±… í¬í•¨ ì—¬ë¶€")

class PolicyRecommendation(BaseModel):
    """ì •ì±… ì¶”ì²œ ê²°ê³¼ - êµ¬ì²´ì  ì¶œë ¥ ì •ì˜"""
    policy_id: str = Field(..., description="ì •ì±… ê³ ìœ  ID")
    title: str = Field(..., description="ì •ì±…ëª…")
    relevance_score: float = Field(..., ge=0.0, le=1.0, description="ê´€ë ¨ì„± ì ìˆ˜")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="ì‹ ë¢°ë„ ì ìˆ˜")
    explanation: str = Field(..., min_length=10, description="ì¶”ì²œ ê·¼ê±° ì„¤ëª…")
```

#### ì…ì¶œë ¥ ê²€ì¦ ì‹œìŠ¤í…œ
```python
class SpecificityValidator:
    """êµ¬ì²´ì„± ì›ì¹™ ê²€ì¦ê¸°"""
    
    def validate_input(self, data: dict, schema: BaseModel) -> tuple[bool, List[str]]:
        """ì…ë ¥ ë°ì´í„°ì˜ êµ¬ì²´ì„± ê²€ì¦"""
        errors = []
        
        try:
            validated_data = schema.parse_obj(data)
            return True, []
        except ValidationError as e:
            for error in e.errors():
                field = ".".join(str(x) for x in error["loc"])
                errors.append(f"Field '{field}': {error['msg']}")
            return False, errors
    
    def validate_output(self, result: dict) -> dict:
        """ì¶œë ¥ ê²°ê³¼ì˜ êµ¬ì²´ì„± ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {
            "schema_compliance": self._check_schema_compliance(result),
            "field_completeness": self._calculate_completeness(result),
            "data_type_accuracy": self._check_data_types(result)
        }
        return metrics
```

### 2.2. Consistency (ì¼ê´€ì„±) êµ¬í˜„

#### ê²°ì •ë¡ ì  ì²˜ë¦¬ ë³´ì¥
```python
import hashlib
import json
from typing import Any

class ConsistencyManager:
    """ì¼ê´€ì„± ê´€ë¦¬ì"""
    
    def __init__(self):
        self.seed_value = 42  # ê³ ì • ì‹œë“œê°’
        self.cache = {}
    
    def generate_deterministic_hash(self, input_data: dict) -> str:
        """ì…ë ¥ ë°ì´í„°ì˜ ê²°ì •ë¡ ì  í•´ì‹œ ìƒì„±"""
        # ë”•ì…”ë„ˆë¦¬ë¥¼ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ í•´ì‹œ ìƒì„±
        sorted_data = json.dumps(input_data, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(sorted_data.encode()).hexdigest()
    
    def ensure_consistent_processing(self, input_data: dict, processor_func) -> Any:
        """ì¼ê´€ëœ ì²˜ë¦¬ ë³´ì¥"""
        input_hash = self.generate_deterministic_hash(input_data)
        
        # ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°˜í™˜
        if input_hash in self.cache:
            return self.cache[input_hash]
        
        # ì‹œë“œê°’ ì„¤ì •ìœ¼ë¡œ ì¼ê´€ëœ ëœë¤ ê²°ê³¼ ë³´ì¥
        import random
        import numpy as np
        random.seed(self.seed_value)
        np.random.seed(self.seed_value)
        
        # ì²˜ë¦¬ ì‹¤í–‰
        result = processor_func(input_data)
        
        # ê²°ê³¼ ìºì‹±
        self.cache[input_hash] = result
        return result
```

#### ì¼ê´€ì„± ë©”íŠ¸ë¦­ ì¸¡ì •
```python
class ConsistencyMetrics:
    """ì¼ê´€ì„± ë©”íŠ¸ë¦­ ì¸¡ì •"""
    
    def measure_reproducibility(self, input_data: dict, processor_func, iterations: int = 10) -> dict:
        """ì¬í˜„ì„± ì¸¡ì •"""
        results = []
        
        for _ in range(iterations):
            result = processor_func(input_data)
            results.append(result)
        
        # ê²°ê³¼ ì¼ê´€ì„± ë¶„ì„
        consistency_score = self._calculate_consistency_score(results)
        variance = self._calculate_variance(results)
        
        return {
            "consistency_score": consistency_score,
            "variance": variance,
            "identical_results": len(set(str(r) for r in results)) == 1
        }
```

### 2.3. Observability (ê´€ì°°ê°€ëŠ¥ì„±) êµ¬í˜„

#### í¬ê´„ì  ë¡œê¹… ì‹œìŠ¤í…œ
```python
import logging
import json
from datetime import datetime
from typing import Dict, Any
from contextlib import contextmanager

class ObservabilityLogger:
    """ê´€ì°°ê°€ëŠ¥ì„± ë¡œê±°"""
    
    def __init__(self, service_name: str):
        self.service_name = service_name
        self.logger = logging.getLogger(service_name)
        self.current_context = {}
    
    @contextmanager
    def trace_operation(self, operation_name: str, **context):
        """ì‘ì—… ì¶”ì  ì»¨í…ìŠ¤íŠ¸"""
        trace_id = self._generate_trace_id()
        start_time = datetime.utcnow()
        
        self.current_context.update({
            "trace_id": trace_id,
            "operation": operation_name,
            "start_time": start_time.isoformat(),
            **context
        })
        
        self.logger.info(f"Operation started: {operation_name}", extra=self.current_context)
        
        try:
            yield trace_id
            
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            self.logger.info(f"Operation completed: {operation_name}", extra={
                **self.current_context,
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "status": "success"
            })
            
        except Exception as e:
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            self.logger.error(f"Operation failed: {operation_name}", extra={
                **self.current_context,
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "status": "error",
                "error_message": str(e),
                "error_type": type(e).__name__
            })
            raise
    
    def log_decision_point(self, decision: str, factors: Dict[str, Any], result: Any):
        """ì˜ì‚¬ê²°ì • ì§€ì  ë¡œê¹…"""
        self.logger.info("Decision point", extra={
            **self.current_context,
            "decision_type": decision,
            "input_factors": factors,
            "decision_result": result,
            "timestamp": datetime.utcnow().isoformat()
        })
```

#### ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ
```python
from prometheus_client import Counter, Histogram, Gauge, Summary
from typing import Dict

class ObservabilityMetrics:
    """ê´€ì°°ê°€ëŠ¥ì„± ë©”íŠ¸ë¦­"""
    
    def __init__(self, service_name: str):
        self.service_name = service_name
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
        self.recommendation_requests = Counter(
            'recommendation_requests_total',
            'Total recommendation requests',
            ['service', 'user_type', 'query_type']
        )
        
        self.recommendation_latency = Histogram(
            'recommendation_duration_seconds',
            'Recommendation generation time',
            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
        )
        
        self.score_distribution = Histogram(
            'recommendation_scores',
            'Distribution of recommendation scores',
            buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
        )
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­
        self.explanation_completeness = Gauge(
            'explanation_completeness_ratio',
            'Ratio of complete explanations'
        )
        
        self.consistency_score = Gauge(
            'consistency_score',
            'Current consistency score'
        )
    
    def record_recommendation(self, user_type: str, query_type: str, 
                            duration: float, scores: List[float]):
        """ì¶”ì²œ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.recommendation_requests.labels(
            service=self.service_name,
            user_type=user_type,
            query_type=query_type
        ).inc()
        
        self.recommendation_latency.observe(duration)
        
        for score in scores:
            self.score_distribution.observe(score)
```

### 2.4. Reproducibility (ì¬í˜„ê°€ëŠ¥ì„±) êµ¬í˜„

#### í™˜ê²½ ìƒíƒœ ê´€ë¦¬
```python
import os
import json
from dataclasses import dataclass
from typing import Dict, Any

@dataclass
class EnvironmentSnapshot:
    """í™˜ê²½ ìŠ¤ëƒ…ìƒ·"""
    timestamp: str
    python_version: str
    package_versions: Dict[str, str]
    environment_variables: Dict[str, str]
    model_versions: Dict[str, str]
    random_seed: int
    
class ReproducibilityManager:
    """ì¬í˜„ê°€ëŠ¥ì„± ê´€ë¦¬ì"""
    
    def __init__(self):
        self.current_snapshot = None
    
    def capture_environment(self) -> EnvironmentSnapshot:
        """í˜„ì¬ í™˜ê²½ ìƒíƒœ ìº¡ì²˜"""
        import sys
        import pkg_resources
        from datetime import datetime
        
        # íŒ¨í‚¤ì§€ ë²„ì „ ìˆ˜ì§‘
        package_versions = {
            pkg.project_name: pkg.version 
            for pkg in pkg_resources.working_set
        }
        
        # í™˜ê²½ ë³€ìˆ˜ ìˆ˜ì§‘ (ë¯¼ê°í•œ ì •ë³´ ì œì™¸)
        safe_env_vars = {
            k: v for k, v in os.environ.items()
            if not any(sensitive in k.lower() for sensitive in ['password', 'key', 'secret', 'token'])
        }
        
        snapshot = EnvironmentSnapshot(
            timestamp=datetime.utcnow().isoformat(),
            python_version=sys.version,
            package_versions=package_versions,
            environment_variables=safe_env_vars,
            model_versions=self._get_model_versions(),
            random_seed=42  # ê³ ì • ì‹œë“œ
        )
        
        self.current_snapshot = snapshot
        return snapshot
    
    def restore_environment(self, snapshot: EnvironmentSnapshot):
        """í™˜ê²½ ìƒíƒœ ë³µì›"""
        import random
        import numpy as np
        
        # ëœë¤ ì‹œë“œ ë³µì›
        random.seed(snapshot.random_seed)
        np.random.seed(snapshot.random_seed)
        
        # í™˜ê²½ ë³€ìˆ˜ ë³µì›
        for key, value in snapshot.environment_variables.items():
            os.environ[key] = value
    
    def save_execution_context(self, input_data: Dict[str, Any], 
                             result: Any, execution_id: str):
        """ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì €ì¥"""
        context = {
            "execution_id": execution_id,
            "environment_snapshot": self.current_snapshot.__dict__,
            "input_data": input_data,
            "result": result,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        with open(f"execution_contexts/{execution_id}.json", "w") as f:
            json.dump(context, f, indent=2, ensure_ascii=False)
```

### 2.5. Explainability (ì„¤ëª…ê°€ëŠ¥ì„±) êµ¬í˜„

#### ì„¤ëª… ìƒì„± ì—”ì§„
```python
from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class ExplanationComponent:
    """ì„¤ëª… êµ¬ì„±ìš”ì†Œ"""
    factor: str
    weight: float
    contribution: float
    evidence: str
    confidence: float

class ExplanationGenerator:
    """ì„¤ëª… ìƒì„±ê¸°"""
    
    def __init__(self):
        self.explanation_templates = {
            "high_relevance": "ì´ ì •ì±…ì€ ê·€í•˜ì˜ {factor}ì™€ {match_percentage:.1%} ì¼ì¹˜í•˜ì—¬ ë†’ì€ ê´€ë ¨ì„±ì„ ë³´ì…ë‹ˆë‹¤.",
            "eligibility_match": "ê·€í•˜ëŠ” ë‹¤ìŒ ìê²© ìš”ê±´ì„ ì¶©ì¡±í•©ë‹ˆë‹¤: {requirements}",
            "score_breakdown": "ì´ ì ìˆ˜ {total_score:.2f}ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤: {breakdown}",
            "comparison": "ì´ ì •ì±…ì€ ë‹¤ë¥¸ ìœ ì‚¬í•œ ì •ì±…ë“¤ë³´ë‹¤ {advantage}ì—ì„œ ìš°ìˆ˜í•©ë‹ˆë‹¤."
        }
    
    def generate_explanation(self, recommendation: Dict[str, Any], 
                           user_profile: Dict[str, Any]) -> Dict[str, Any]:
        """í¬ê´„ì  ì„¤ëª… ìƒì„±"""
        
        # ì ìˆ˜ ë¶„í•´ ë¶„ì„
        score_breakdown = self._analyze_score_breakdown(recommendation)
        
        # ìê²© ìš”ê±´ ë§¤ì¹­ ë¶„ì„
        eligibility_analysis = self._analyze_eligibility_match(
            recommendation, user_profile
        )
        
        # ë¹„êµ ë¶„ì„
        comparative_analysis = self._generate_comparative_analysis(recommendation)
        
        # ì¦ê±° ìˆ˜ì§‘
        evidence = self._collect_evidence(recommendation, user_profile)
        
        # ì„¤ëª… êµ¬ì„±
        explanation = {
            "summary": self._generate_summary(recommendation, user_profile),
            "score_breakdown": score_breakdown,
            "eligibility_match": eligibility_analysis,
            "comparative_advantages": comparative_analysis,
            "supporting_evidence": evidence,
            "confidence_level": self._calculate_explanation_confidence(recommendation),
            "limitations": self._identify_limitations(recommendation)
        }
        
        return explanation
    
    def _analyze_score_breakdown(self, recommendation: Dict[str, Any]) -> Dict[str, Any]:
        """ì ìˆ˜ ë¶„í•´ ë¶„ì„"""
        score_components = recommendation.get("score_components", {})
        
        breakdown = {
            "total_score": recommendation.get("relevance_score", 0.0),
            "components": []
        }
        
        for component, score in score_components.items():
            breakdown["components"].append({
                "name": component,
                "score": score,
                "weight": score_components.get(f"{component}_weight", 1.0),
                "explanation": self._explain_component(component, score)
            })
        
        return breakdown
    
    def _generate_natural_language_explanation(self, explanation_data: Dict[str, Any]) -> str:
        """ìì—°ì–´ ì„¤ëª… ìƒì„±"""
        parts = []
        
        # ìš”ì•½ ì„¤ëª…
        parts.append(explanation_data["summary"])
        
        # ì ìˆ˜ ì„¤ëª…
        score_breakdown = explanation_data["score_breakdown"]
        parts.append(f"ì „ì²´ ì ìˆ˜ {score_breakdown['total_score']:.2f}ì ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë©ë‹ˆë‹¤:")
        
        for component in score_breakdown["components"]:
            parts.append(f"- {component['name']}: {component['score']:.2f}ì  ({component['explanation']})")
        
        # ìê²© ìš”ê±´ ì„¤ëª…
        eligibility = explanation_data["eligibility_match"]
        if eligibility["matched_requirements"]:
            parts.append("ê·€í•˜ê°€ ì¶©ì¡±í•˜ëŠ” ìê²© ìš”ê±´:")
            for req in eligibility["matched_requirements"]:
                parts.append(f"âœ“ {req}")
        
        return "\n".join(parts)
```

## 3. S.C.O.R.E. ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ

### 3.1. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
```python
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse
import json

class SCOREDashboard:
    """S.C.O.R.E. ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self, app: FastAPI):
        self.app = app
        self.metrics_collector = SCOREMetricsCollector()
        self._setup_routes()
    
    def _setup_routes(self):
        @self.app.get("/dashboard/score", response_class=HTMLResponse)
        async def score_dashboard():
            metrics = await self.metrics_collector.get_current_metrics()
            return self._render_dashboard(metrics)
        
        @self.app.get("/api/score/metrics")
        async def get_score_metrics():
            return await self.metrics_collector.get_current_metrics()
    
    def _render_dashboard(self, metrics: Dict[str, Any]) -> str:
        """ëŒ€ì‹œë³´ë“œ HTML ë Œë”ë§"""
        return f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>S.C.O.R.E. Framework Dashboard</title>
            <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        </head>
        <body>
            <h1>S.C.O.R.E. Framework Metrics</h1>
            
            <div class="metrics-grid">
                <div class="metric-card">
                    <h3>Specificity</h3>
                    <div class="metric-value">{metrics['specificity']['score']:.2%}</div>
                    <div class="metric-details">
                        Schema Compliance: {metrics['specificity']['schema_compliance']:.2%}<br>
                        Field Completeness: {metrics['specificity']['field_completeness']:.2%}
                    </div>
                </div>
                
                <div class="metric-card">
                    <h3>Consistency</h3>
                    <div class="metric-value">{metrics['consistency']['score']:.2%}</div>
                    <div class="metric-details">
                        Reproducibility: {metrics['consistency']['reproducibility']:.2%}<br>
                        Variance: {metrics['consistency']['variance']:.4f}
                    </div>
                </div>
                
                <div class="metric-card">
                    <h3>Observability</h3>
                    <div class="metric-value">{metrics['observability']['score']:.2%}</div>
                    <div class="metric-details">
                        Log Coverage: {metrics['observability']['log_coverage']:.2%}<br>
                        Trace Completeness: {metrics['observability']['trace_completeness']:.2%}
                    </div>
                </div>
                
                <div class="metric-card">
                    <h3>Reproducibility</h3>
                    <div class="metric-value">{metrics['reproducibility']['score']:.2%}</div>
                    <div class="metric-details">
                        Environment Consistency: {metrics['reproducibility']['env_consistency']:.2%}<br>
                        Result Stability: {metrics['reproducibility']['result_stability']:.2%}
                    </div>
                </div>
                
                <div class="metric-card">
                    <h3>Explainability</h3>
                    <div class="metric-value">{metrics['explainability']['score']:.2%}</div>
                    <div class="metric-details">
                        Explanation Completeness: {metrics['explainability']['completeness']:.2%}<br>
                        User Understanding: {metrics['explainability']['user_understanding']:.2%}
                    </div>
                </div>
            </div>
            
            <div id="score-trend-chart"></div>
            
            <script>
                // ì°¨íŠ¸ ë Œë”ë§ ì½”ë“œ
                const trendData = {json.dumps(metrics['trend_data'])};
                Plotly.newPlot('score-trend-chart', trendData);
            </script>
        </body>
        </html>
        """

class SCOREMetricsCollector:
    """S.C.O.R.E. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    async def get_current_metrics(self) -> Dict[str, Any]:
        """í˜„ì¬ S.C.O.R.E. ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        return {
            "specificity": await self._collect_specificity_metrics(),
            "consistency": await self._collect_consistency_metrics(),
            "observability": await self._collect_observability_metrics(),
            "reproducibility": await self._collect_reproducibility_metrics(),
            "explainability": await self._collect_explainability_metrics(),
            "overall_score": await self._calculate_overall_score(),
            "trend_data": await self._get_trend_data()
        }
```

## 4. S.C.O.R.E. í’ˆì§ˆ ê²Œì´íŠ¸

### 4.1. ìë™í™”ëœ í’ˆì§ˆ ê²€ì¦
```python
class SCOREQualityGate:
    """S.C.O.R.E. í’ˆì§ˆ ê²Œì´íŠ¸"""
    
    def __init__(self):
        self.thresholds = {
            "specificity": 0.95,
            "consistency": 0.90,
            "observability": 0.85,
            "reproducibility": 0.90,
            "explainability": 0.80
        }
    
    async def validate_deployment(self, service_name: str) -> Dict[str, Any]:
        """ë°°í¬ ì „ S.C.O.R.E. ê²€ì¦"""
        metrics = await self._collect_service_metrics(service_name)
        
        validation_results = {
            "service": service_name,
            "timestamp": datetime.utcnow().isoformat(),
            "passed": True,
            "results": {}
        }
        
        for metric, threshold in self.thresholds.items():
            score = metrics.get(metric, {}).get("score", 0.0)
            passed = score >= threshold
            
            validation_results["results"][metric] = {
                "score": score,
                "threshold": threshold,
                "passed": passed,
                "details": metrics.get(metric, {})
            }
            
            if not passed:
                validation_results["passed"] = False
        
        return validation_results
    
    async def continuous_monitoring(self, service_name: str):
        """ì§€ì†ì  S.C.O.R.E. ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                validation_result = await self.validate_deployment(service_name)
                
                if not validation_result["passed"]:
                    await self._trigger_alert(service_name, validation_result)
                
                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ê²€ì¦
                
            except Exception as e:
                logger.error(f"S.C.O.R.E. monitoring error: {e}")
                await asyncio.sleep(60)
```

---

**ğŸ“‹ ê´€ë ¨ ë¬¸ì„œ**
- [Interactive AI Core](./02_INTERACTIVE_AI_CORE.md)
- [Living Gateway](./03_LIVING_GATEWAY.md)
- [Rule Engine](./04_RULE_ENGINE.md)
- [API ëª…ì„¸ì„œ](../03_DATA_AND_APIS/02_API_CONTRACT.md)
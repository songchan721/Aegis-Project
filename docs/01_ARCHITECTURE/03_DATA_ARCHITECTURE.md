# ì´ì§€ìŠ¤(Aegis) ë°ì´í„° ì•„í‚¤í…ì²˜ ëª…ì„¸ì„œ

| í•­ëª© | ë‚´ìš© |
|------|------|
| ë¬¸ì„œ ID | AEG-ARC-20250917-1.0 |
| ë²„ì „ | 1.0 |
| ìµœì¢… ìˆ˜ì •ì¼ | 2025ë…„ 9ì›” 17ì¼ |
| ì‘ì„±ì | Dr. Aiden (ìˆ˜ì„ AI ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸) |
| ìƒíƒœ | í™•ì • (Finalized) |

## 1. ê°œìš” (Overview)

ë³¸ ë¬¸ì„œëŠ” ì´ì§€ìŠ¤ ì‹œìŠ¤í…œì˜ ë°ì´í„° ì•„í‚¤í…ì²˜ë¥¼ ì •ì˜í•œë‹¤. **Polyglot Persistence** íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ê° ë°ì´í„°ì˜ íŠ¹ì„±ì— ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©°, **ì´ì¤‘ íŠ¸ë™ íŒŒì´í”„ë¼ì¸**ì„ í†µí•´ ë°ì´í„° ì¼ê´€ì„±ê³¼ ì‹¤ì‹œê°„ì„±ì„ ëª¨ë‘ ë³´ì¥í•˜ëŠ” ì•„í‚¤í…ì²˜ë¥¼ êµ¬í˜„í•œë‹¤.

## 2. ë°ì´í„° ì•„í‚¤í…ì²˜ ì›ì¹™

### 2.1. í•µì‹¬ ì›ì¹™
- **ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì› (Single Source of Truth)**: PostgreSQLì´ ëª¨ë“  ë°ì´í„°ì˜ ìµœì¢… ê¶Œìœ„
- **ë°ì´í„° íŠ¹ì„±ë³„ ìµœì í™”**: ê° ë°ì´í„° íƒ€ì…ì— ìµœì í™”ëœ ì €ì¥ì†Œ ì‚¬ìš©
- **ìµœì¢…ì  ì¼ê´€ì„± (Eventual Consistency)**: ì‹¤ì‹œê°„ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜•
- **ì´ë²¤íŠ¸ ê¸°ë°˜ ë™ê¸°í™”**: CDCì™€ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°ì„ í†µí•œ ë°ì´í„° ì „íŒŒ

### 2.2. ë°ì´í„° ë¶„ë¥˜ ì²´ê³„

#### ë§ˆìŠ¤í„° ë°ì´í„° (Master Data)
- **ì •ì±… ì›ë¬¸ ë°ì´í„°**: ì •ë¶€/ì§€ìì²´ì—ì„œ ìˆ˜ì§‘í•œ ì›ë³¸ ì •ì±… ì •ë³´
- **ì‚¬ìš©ì í”„ë¡œí•„**: ê°œì¸í™”ë¥¼ ìœ„í•œ ì‚¬ìš©ì ê¸°ë³¸ ì •ë³´
- **ì¡°ì§ ì •ë³´**: ì •ì±… ë°œí–‰ ê¸°ê´€ ë° ê´€ë ¨ ì¡°ì§ ë°ì´í„°

#### íŒŒìƒ ë°ì´í„° (Derived Data)
- **ë²¡í„° ì„ë² ë”©**: ì •ì±… í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ì  í‘œí˜„
- **ì§€ì‹ ê·¸ë˜í”„**: ì •ì±… ê°„ ê´€ê³„ ë° ê·œì¹™
- **ì§‘ê³„ ë°ì´í„°**: í†µê³„ ë° ë¶„ì„ì„ ìœ„í•œ ìš”ì•½ ë°ì´í„°

#### ìš´ì˜ ë°ì´í„° (Operational Data)
- **ì¶”ì²œ ì´ë ¥**: ì‚¬ìš©ìë³„ ì¶”ì²œ ê²°ê³¼ ë° í”¼ë“œë°±
- **ì„¸ì…˜ ë°ì´í„°**: ì‚¬ìš©ì ì„¸ì…˜ ë° ì„ì‹œ ìƒíƒœ
- **ë¡œê·¸ ë°ì´í„°**: ì‹œìŠ¤í…œ ìš´ì˜ ë° ê°ì‚¬ë¥¼ ìœ„í•œ ë¡œê·¸

## 3. ë°ì´í„°ë² ì´ìŠ¤ ì•„í‚¤í…ì²˜

### 3.1. ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ê¸°ì¤€

```mermaid
graph TB
    subgraph "ë°ì´í„° íŠ¹ì„± ë¶„ì„"
        A[êµ¬ì¡°í™” ì •ë„] --> B{ì •í˜•/ë°˜ì •í˜•/ë¹„ì •í˜•}
        C[ê´€ê³„ ë³µì¡ë„] --> D{ë‹¨ìˆœ/ë³µì¡/ê·¸ë˜í”„}
        E[ê²€ìƒ‰ íŒ¨í„´] --> F{í‚¤-ê°’/SQL/ë²¡í„°/ê·¸ë˜í”„}
        G[ì¼ê´€ì„± ìš”êµ¬] --> H{ê°•í•œ/ì•½í•œ/ìµœì¢…ì }
    end
    
    subgraph "ë°ì´í„°ë² ì´ìŠ¤ ë§¤í•‘"
        B --> I[PostgreSQL]
        B --> J[MongoDB]
        D --> K[Neo4j]
        F --> L[Milvus]
        F --> M[Redis]
    end
    
    subgraph "ì‚¬ìš© ì‚¬ë¡€"
        I --> N[ì •ì±… ì›ë¬¸, ì‚¬ìš©ì ì •ë³´]
        J --> O[ë¡œê·¸, ë©”íƒ€ë°ì´í„°]
        K --> P[ì •ì±… ê´€ê³„, ê·œì¹™]
        L --> Q[ë²¡í„° ê²€ìƒ‰, RAG]
        M --> R[ìºì‹œ, ì„¸ì…˜]
    end
```

### 3.2. ë°ì´í„°ë² ì´ìŠ¤ë³„ ì—­í•  ì •ì˜

#### PostgreSQL (Primary Database)
```sql
-- ì—­í• : ì‹œìŠ¤í…œì˜ ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì› (SSoT)
-- íŠ¹ì§•: ACID íŠ¸ëœì­ì…˜, ê°•í•œ ì¼ê´€ì„±, ë³µì¡í•œ ì¿¼ë¦¬ ì§€ì›

-- ì£¼ìš” í…Œì´ë¸”
- policies: ì •ì±… ì›ë¬¸ ë° ë©”íƒ€ë°ì´í„°
- users: ì‚¬ìš©ì ê³„ì • ë° í”„ë¡œí•„
- organizations: ì •ì±… ë°œí–‰ ê¸°ê´€
- business_rules: ë™ì  ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™
- recommendation_history: ì¶”ì²œ ì´ë ¥
- data_ingestion_status: ë°ì´í„° ì²˜ë¦¬ ìƒíƒœ ì¶”ì 
```

#### Milvus (Vector Database)
```python
# ì—­í• : ëŒ€ê·œëª¨ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
# íŠ¹ì§•: ê³ ì„±ëŠ¥ ë²¡í„° ì¸ë±ì‹±, í•„í„°ë§ ì§€ì›

# ì£¼ìš” ì»¬ë ‰ì…˜
collections = {
    "policy_embeddings": {
        "dimension": 768,  # sentence-transformers ì„ë² ë”©
        "index_type": "HNSW",
        "metric_type": "L2"
    },
    "user_query_embeddings": {
        "dimension": 768,
        "index_type": "IVF_FLAT",
        "metric_type": "COSINE"
    }
}
```

#### Neo4j (Graph Database)
```cypher
// ì—­í• : ë³µì¡í•œ ê´€ê³„ ëª¨ë¸ë§ ë° ë…¼ë¦¬ì  ì¶”ë¡ 
// íŠ¹ì§•: ê·¸ë˜í”„ ìˆœíšŒ, íŒ¨í„´ ë§¤ì¹­, ì¶”ë¡  ì—”ì§„

// ì£¼ìš” ë…¸ë“œ íƒ€ì…
(:Policy)-[:TARGETS]->(:Region)
(:Policy)-[:REQUIRES]->(:Eligibility)
(:Policy)-[:CONFLICTS_WITH]->(:Policy)
(:User)-[:BELONGS_TO]->(:BusinessType)
(:BusinessType)-[:ELIGIBLE_FOR]->(:Policy)
```

#### Redis (In-Memory Cache)
```redis
# ì—­í• : ê³ ì„±ëŠ¥ ìºì‹± ë° ì„¸ì…˜ ê´€ë¦¬
# íŠ¹ì§•: ë©”ëª¨ë¦¬ ê¸°ë°˜, ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì¡° ì§€ì›

# ì£¼ìš” í‚¤ íŒ¨í„´
aegis:cache:policy:{policy_id}     # ì •ì±… ìƒì„¸ ìºì‹œ
aegis:session:{session_id}         # ì‚¬ìš©ì ì„¸ì…˜
aegis:search:{query_hash}          # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ
aegis:user:profile:{user_id}       # ì‚¬ìš©ì í”„ë¡œí•„ ìºì‹œ
```

## 4. ë°ì´í„° íë¦„ ì•„í‚¤í…ì²˜

### 4.1. ì´ì¤‘ íŠ¸ë™ ë°ì´í„° íŒŒì´í”„ë¼ì¸

```mermaid
graph TB
    subgraph "ë°ì´í„° ì†ŒìŠ¤"
        A[ê³µê³µë°ì´í„°í¬í„¸]
        B[ì§€ìì²´ ì›¹ì‚¬ì´íŠ¸]
        C[ê¸ˆìœµê¸°ê´€ API]
    end
    
    subgraph "ìˆ˜ì§‘ ê³„ì¸µ"
        D[Data Collectors]
        E[API Clients]
        F[Web Scrapers]
    end
    
    subgraph "ì²˜ë¦¬ ê³„ì¸µ"
        G[Raw Data Queue]
        H[Data Processors]
        I[Validation Engine]
    end
    
    subgraph "ì €ì¥ ê³„ì¸µ (SSoT)"
        J[(PostgreSQL)]
    end
    
    subgraph "CDC & ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°"
        K[Debezium CDC]
        L[Kafka Topics]
    end
    
    subgraph "íŒŒìƒ ë°ì´í„° ì €ì¥ì†Œ"
        M[(Milvus)]
        N[(Neo4j)]
        O[(Redis)]
    end
    
    subgraph "Hot Path (ì‹¤ì‹œê°„)"
        P[Kafka Consumers]
        Q[Real-time Processors]
    end
    
    subgraph "Cold Path (ë°°ì¹˜)"
        R[Apache Airflow]
        S[Batch Processors]
        T[Data Quality Checks]
    end
    
    A --> D
    B --> E
    C --> F
    
    D --> G
    E --> G
    F --> G
    
    G --> H
    H --> I
    I --> J
    
    J --> K
    K --> L
    
    L --> P
    P --> Q
    Q --> M
    Q --> N
    Q --> O
    
    J --> R
    R --> S
    S --> T
    T --> M
    T --> N
```

### 4.2. ë°ì´í„° ë™ê¸°í™” ì „ëµ

#### CDC (Change Data Capture) ì„¤ì •
```yaml
# debezium-connector-config.yml
name: "aegis-postgres-connector"
config:
  connector.class: "io.debezium.connector.postgresql.PostgresConnector"
  database.hostname: "postgres-primary"
  database.port: "5432"
  database.user: "debezium"
  database.password: "${DEBEZIUM_PASSWORD}"
  database.dbname: "aegis"
  database.server.name: "aegis-db"
  
  # ì¶”ì í•  í…Œì´ë¸” ì§€ì •
  table.include.list: "public.policies,public.users,public.business_rules"
  
  # ì´ë²¤íŠ¸ í˜•ì‹ ì„¤ì •
  transforms: "unwrap"
  transforms.unwrap.type: "io.debezium.transforms.ExtractNewRecordState"
  transforms.unwrap.drop.tombstones: "false"
  
  # Kafka í† í”½ ì„¤ì •
  topic.prefix: "aegis"
```

#### ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ì •ì˜
```json
{
  "schema": {
    "type": "struct",
    "fields": [
      {"field": "before", "type": "struct", "optional": true},
      {"field": "after", "type": "struct", "optional": true},
      {"field": "source", "type": "struct"},
      {"field": "op", "type": "string"},
      {"field": "ts_ms", "type": "int64"}
    ]
  },
  "payload": {
    "before": null,
    "after": {
      "policy_id": "uuid-string",
      "title": "ì •ì±…ëª…",
      "content": "ì •ì±… ë‚´ìš©",
      "metadata": {"category": "ì°½ì—…ì§€ì›"}
    },
    "source": {
      "version": "1.9.0",
      "connector": "postgresql",
      "name": "aegis-db",
      "ts_ms": 1663405368000,
      "snapshot": "false",
      "db": "aegis",
      "schema": "public",
      "table": "policies"
    },
    "op": "c",
    "ts_ms": 1663405368000
  }
}
```

## 5. ë°ì´í„° ëª¨ë¸ë§ ì „ëµ

### 5.1. ì •ê·œí™” vs ë¹„ì •ê·œí™” ì „ëµ

#### PostgreSQL: ì •ê·œí™” ìš°ì„ 
```sql
-- ì •ê·œí™”ëœ êµ¬ì¡°ë¡œ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
CREATE TABLE policies (
    policy_id UUID PRIMARY KEY,
    title VARCHAR(512) NOT NULL,
    issuing_organization_id UUID REFERENCES organizations(org_id),
    content TEXT NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE policy_regions (
    policy_id UUID REFERENCES policies(policy_id),
    region_code VARCHAR(10) REFERENCES regions(code),
    PRIMARY KEY (policy_id, region_code)
);
```

#### Milvus: ë¹„ì •ê·œí™”ëœ ë©”íƒ€ë°ì´í„°
```python
# ê²€ìƒ‰ ì„±ëŠ¥ì„ ìœ„í•œ ë¹„ì •ê·œí™”
policy_collection_schema = [
    FieldSchema(name="policy_id", dtype=DataType.VARCHAR, max_length=36),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
    
    # í•„í„°ë§ì„ ìœ„í•œ ë¹„ì •ê·œí™”ëœ ë©”íƒ€ë°ì´í„°
    FieldSchema(name="region_codes", dtype=DataType.VARCHAR, max_length=500),
    FieldSchema(name="industry_codes", dtype=DataType.VARCHAR, max_length=500),
    FieldSchema(name="business_types", dtype=DataType.VARCHAR, max_length=200),
    FieldSchema(name="funding_amount_min", dtype=DataType.INT64),
    FieldSchema(name="funding_amount_max", dtype=DataType.INT64),
]
```

### 5.2. ë°ì´í„° ë²„ì „ ê´€ë¦¬

#### ìŠ¤í‚¤ë§ˆ ì§„í™” ì „ëµ
```sql
-- ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬ í…Œì´ë¸”
CREATE TABLE schema_versions (
    version VARCHAR(20) PRIMARY KEY,
    description TEXT NOT NULL,
    applied_at TIMESTAMPTZ DEFAULT NOW(),
    rollback_script TEXT
);

-- ë°ì´í„° ë²„ì „ ê´€ë¦¬
CREATE TABLE policy_versions (
    policy_id UUID,
    version_number INTEGER,
    content JSONB NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    created_by VARCHAR(255),
    PRIMARY KEY (policy_id, version_number)
);
```

#### í˜¸í™˜ì„± ë³´ì¥ ì „ëµ
```python
class SchemaEvolution:
    """ìŠ¤í‚¤ë§ˆ ì§„í™” ê´€ë¦¬"""
    
    def __init__(self):
        self.compatibility_matrix = {
            "v1.0": ["v1.1", "v1.2"],
            "v1.1": ["v1.2", "v2.0"],
            "v2.0": ["v2.1"]
        }
    
    def is_compatible(self, from_version: str, to_version: str) -> bool:
        """ë²„ì „ ê°„ í˜¸í™˜ì„± ê²€ì‚¬"""
        return to_version in self.compatibility_matrix.get(from_version, [])
    
    def migrate_data(self, from_version: str, to_version: str, data: dict) -> dict:
        """ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        migration_path = self.get_migration_path(from_version, to_version)
        
        for step in migration_path:
            data = self.apply_migration_step(step, data)
        
        return data
```

## 6. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

### 6.1. ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­

```python
class DataQualityMetrics:
    """ë°ì´í„° í’ˆì§ˆ ì¸¡ì •"""
    
    def calculate_completeness(self, dataset: pd.DataFrame) -> float:
        """ì™„ì„±ë„ ì¸¡ì •"""
        total_cells = dataset.size
        non_null_cells = dataset.count().sum()
        return non_null_cells / total_cells
    
    def calculate_accuracy(self, dataset: pd.DataFrame, rules: List[ValidationRule]) -> float:
        """ì •í™•ì„± ì¸¡ì •"""
        total_records = len(dataset)
        valid_records = 0
        
        for _, record in dataset.iterrows():
            if all(rule.validate(record) for rule in rules):
                valid_records += 1
        
        return valid_records / total_records
    
    def calculate_consistency(self, primary_data: pd.DataFrame, 
                            derived_data: pd.DataFrame) -> float:
        """ì¼ê´€ì„± ì¸¡ì •"""
        # ê¸°ë³¸ ë°ì´í„°ì™€ íŒŒìƒ ë°ì´í„° ê°„ ì¼ê´€ì„± ê²€ì‚¬
        inconsistencies = 0
        total_comparisons = 0
        
        for primary_record in primary_data.itertuples():
            derived_record = derived_data[
                derived_data['source_id'] == primary_record.id
            ].iloc[0] if not derived_data.empty else None
            
            if derived_record is not None:
                total_comparisons += 1
                if not self.records_consistent(primary_record, derived_record):
                    inconsistencies += 1
        
        return 1 - (inconsistencies / total_comparisons) if total_comparisons > 0 else 1.0
```

### 6.2. ë°ì´í„° ê²€ì¦ íŒŒì´í”„ë¼ì¸

```python
class DataValidationPipeline:
    """ë°ì´í„° ê²€ì¦ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.validators = [
            SchemaValidator(),
            BusinessRuleValidator(),
            ReferentialIntegrityValidator(),
            DataQualityValidator()
        ]
    
    async def validate_policy_data(self, policy_data: dict) -> ValidationResult:
        """ì •ì±… ë°ì´í„° ê²€ì¦"""
        results = []
        
        for validator in self.validators:
            result = await validator.validate(policy_data)
            results.append(result)
            
            if result.severity == ValidationSeverity.ERROR:
                return ValidationResult(
                    is_valid=False,
                    errors=result.errors,
                    warnings=[r.warnings for r in results]
                )
        
        return ValidationResult(
            is_valid=True,
            warnings=[r.warnings for r in results if r.warnings]
        )

class SchemaValidator:
    """ìŠ¤í‚¤ë§ˆ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.schema = {
            "type": "object",
            "required": ["title", "content", "issuing_organization"],
            "properties": {
                "title": {"type": "string", "minLength": 1, "maxLength": 512},
                "content": {"type": "string", "minLength": 10},
                "issuing_organization": {"type": "string", "minLength": 1},
                "metadata": {"type": "object"}
            }
        }
    
    async def validate(self, data: dict) -> ValidationResult:
        """ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤í–‰"""
        try:
            jsonschema.validate(data, self.schema)
            return ValidationResult(is_valid=True)
        except jsonschema.ValidationError as e:
            return ValidationResult(
                is_valid=False,
                errors=[f"Schema validation failed: {e.message}"],
                severity=ValidationSeverity.ERROR
            )
```

## 7. ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 7.1. ì¸ë±ì‹± ì „ëµ

#### PostgreSQL ì¸ë±ìŠ¤ ì„¤ê³„
```sql
-- ë³µí•© ì¸ë±ìŠ¤: ìì£¼ í•¨ê»˜ ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼
CREATE INDEX idx_policies_active_region ON policies (is_active, target_regions) 
WHERE is_active = true;

-- ë¶€ë¶„ ì¸ë±ìŠ¤: ì¡°ê±´ë¶€ ì¸ë±ìŠ¤ë¡œ í¬ê¸° ìµœì í™”
CREATE INDEX idx_policies_recent ON policies (created_at) 
WHERE created_at > NOW() - INTERVAL '1 year';

-- GIN ì¸ë±ìŠ¤: ë°°ì—´ ë° JSONB ê²€ìƒ‰
CREATE INDEX idx_policies_metadata_gin ON policies USING GIN (metadata);
CREATE INDEX idx_policies_regions_gin ON policies USING GIN (target_regions);

-- í•¨ìˆ˜ ê¸°ë°˜ ì¸ë±ìŠ¤: ê³„ì‚°ëœ ê°’ì— ëŒ€í•œ ì¸ë±ìŠ¤
CREATE INDEX idx_policies_title_search ON policies 
USING GIN (to_tsvector('korean', title));
```

#### Milvus ì¸ë±ìŠ¤ ìµœì í™”
```python
# HNSW ì¸ë±ìŠ¤ íŒŒë¼ë¯¸í„° íŠœë‹
hnsw_index_params = {
    "metric_type": "L2",
    "index_type": "HNSW",
    "params": {
        "M": 16,              # ì—°ê²° ìˆ˜ (ë©”ëª¨ë¦¬ vs ì„±ëŠ¥)
        "efConstruction": 64, # êµ¬ì¶• ì‹œ íƒìƒ‰ ê¹Šì´
        "ef": 32             # ê²€ìƒ‰ ì‹œ íƒìƒ‰ ê¹Šì´
    }
}

# íŒŒí‹°ì…˜ ì „ëµ: ì§€ì—­ë³„ ë¶„í• ë¡œ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ
partitions = [
    "seoul",      # ì„œìš¸íŠ¹ë³„ì‹œ
    "gyeonggi",   # ê²½ê¸°ë„
    "busan",      # ë¶€ì‚°ê´‘ì—­ì‹œ
    "national",   # ì „êµ­ ëŒ€ìƒ
    "others"      # ê¸°íƒ€ ì§€ì—­
]
```

### 7.2. ìºì‹± ì „ëµ

#### ë‹¤ì¸µ ìºì‹± ì•„í‚¤í…ì²˜
```python
class MultiLevelCache:
    """ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.l1_cache = {}  # ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ ìºì‹œ
        self.l2_cache = redis.Redis()  # Redis ìºì‹œ
        self.l3_cache = None  # CDN ìºì‹œ (ì •ì  ì»¨í…ì¸ )
    
    async def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        # L1 ìºì‹œ í™•ì¸
        if key in self.l1_cache:
            return self.l1_cache[key]
        
        # L2 ìºì‹œ í™•ì¸
        l2_data = await self.l2_cache.get(key)
        if l2_data:
            # L1 ìºì‹œì— ì €ì¥
            self.l1_cache[key] = json.loads(l2_data)
            return self.l1_cache[key]
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        # L1 ìºì‹œ ì €ì¥
        self.l1_cache[key] = value
        
        # L2 ìºì‹œ ì €ì¥
        await self.l2_cache.setex(key, ttl, json.dumps(value, default=str))

# ìºì‹œ ë¬´íš¨í™” ì „ëµ
class CacheInvalidationStrategy:
    """ìºì‹œ ë¬´íš¨í™” ì „ëµ"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.invalidation_patterns = {
            "policy_updated": ["policy:{policy_id}", "search:*", "recommendations:*"],
            "user_profile_updated": ["user:profile:{user_id}", "recommendations:{user_id}:*"],
            "business_rules_updated": ["recommendations:*", "search:*"]
        }
    
    async def invalidate_on_event(self, event_type: str, event_data: dict):
        """ì´ë²¤íŠ¸ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”"""
        patterns = self.invalidation_patterns.get(event_type, [])
        
        for pattern in patterns:
            # íŒ¨í„´ì— ì´ë²¤íŠ¸ ë°ì´í„° ì ìš©
            cache_key = pattern.format(**event_data)
            
            if "*" in cache_key:
                # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ì²˜ë¦¬
                await self.invalidate_pattern(cache_key)
            else:
                # ë‹¨ì¼ í‚¤ ë¬´íš¨í™”
                await self.cache.delete(cache_key)
```

## 8. ë°ì´í„° ë³´ì•ˆ ë° í”„ë¼ì´ë²„ì‹œ

### 8.1. ë°ì´í„° ë¶„ë¥˜ ë° ë³´í˜¸ ìˆ˜ì¤€

```python
from enum import Enum

class DataClassification(Enum):
    PUBLIC = "public"           # ê³µê°œ ë°ì´í„°
    INTERNAL = "internal"       # ë‚´ë¶€ ë°ì´í„°
    CONFIDENTIAL = "confidential"  # ê¸°ë°€ ë°ì´í„°
    RESTRICTED = "restricted"   # ì œí•œ ë°ì´í„°

class DataProtectionStrategy:
    """ë°ì´í„° ë³´í˜¸ ì „ëµ"""
    
    def __init__(self):
        self.protection_rules = {
            DataClassification.PUBLIC: {
                "encryption": False,
                "access_control": "none",
                "audit_logging": False
            },
            DataClassification.INTERNAL: {
                "encryption": False,
                "access_control": "authentication",
                "audit_logging": True
            },
            DataClassification.CONFIDENTIAL: {
                "encryption": True,
                "access_control": "role_based",
                "audit_logging": True,
                "data_masking": True
            },
            DataClassification.RESTRICTED: {
                "encryption": True,
                "access_control": "attribute_based",
                "audit_logging": True,
                "data_masking": True,
                "field_level_encryption": True
            }
        }
    
    def get_protection_requirements(self, classification: DataClassification) -> dict:
        """ë°ì´í„° ë¶„ë¥˜ë³„ ë³´í˜¸ ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
        return self.protection_rules[classification]
```

### 8.2. ê°œì¸ì •ë³´ ë³´í˜¸ êµ¬í˜„

```python
class PIIProtection:
    """ê°œì¸ì •ë³´ ë³´í˜¸"""
    
    def __init__(self):
        self.encryptor = Fernet(settings.ENCRYPTION_KEY)
        self.hasher = hashlib.sha256
    
    def encrypt_pii(self, data: str) -> str:
        """ê°œì¸ì •ë³´ ì•”í˜¸í™”"""
        return self.encryptor.encrypt(data.encode()).decode()
    
    def decrypt_pii(self, encrypted_data: str) -> str:
        """ê°œì¸ì •ë³´ ë³µí˜¸í™”"""
        return self.encryptor.decrypt(encrypted_data.encode()).decode()
    
    def hash_identifier(self, identifier: str) -> str:
        """ì‹ë³„ì í•´ì‹œí™” (ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸ ë“±)"""
        return self.hasher(identifier.encode()).hexdigest()
    
    def mask_data(self, data: str, mask_char: str = "*") -> str:
        """ë°ì´í„° ë§ˆìŠ¤í‚¹"""
        if len(data) <= 4:
            return mask_char * len(data)
        
        # ì• 2ìë¦¬ì™€ ë’¤ 2ìë¦¬ë§Œ ë³´ì—¬ì£¼ê³  ë‚˜ë¨¸ì§€ëŠ” ë§ˆìŠ¤í‚¹
        return data[:2] + mask_char * (len(data) - 4) + data[-2:]

# ì‚¬ìš© ì˜ˆì‹œ
class UserProfileService:
    def __init__(self):
        self.pii_protection = PIIProtection()
    
    def store_user_profile(self, profile_data: dict) -> dict:
        """ì‚¬ìš©ì í”„ë¡œí•„ ì €ì¥ ì‹œ PII ë³´í˜¸ ì ìš©"""
        protected_profile = profile_data.copy()
        
        # ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸ í•´ì‹œí™”
        if 'business_registration_number' in protected_profile:
            protected_profile['business_registration_number'] = \
                self.pii_protection.hash_identifier(
                    protected_profile['business_registration_number']
                )
        
        # ìƒì„¸ ì£¼ì†Œ ì•”í˜¸í™”
        if 'detailed_address' in protected_profile:
            protected_profile['detailed_address'] = \
                self.pii_protection.encrypt_pii(
                    protected_profile['detailed_address']
                )
        
        return protected_profile
```

## 9. ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ê°€ëŠ¥ì„±

### 9.1. ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

```python
from prometheus_client import Gauge, Counter, Histogram

class DataQualityMonitoring:
    """ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        # ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­
        self.data_completeness = Gauge('data_completeness_ratio', 'Data completeness ratio', ['table'])
        self.data_accuracy = Gauge('data_accuracy_ratio', 'Data accuracy ratio', ['table'])
        self.data_freshness = Gauge('data_freshness_hours', 'Data freshness in hours', ['table'])
        
        # ë°ì´í„° ì²˜ë¦¬ ë©”íŠ¸ë¦­
        self.records_processed = Counter('data_records_processed_total', 'Total records processed', ['source', 'status'])
        self.processing_duration = Histogram('data_processing_duration_seconds', 'Data processing duration')
        
        # ë°ì´í„° ë™ê¸°í™” ë©”íŠ¸ë¦­
        self.sync_lag = Gauge('data_sync_lag_seconds', 'Data synchronization lag', ['source', 'target'])
        self.sync_errors = Counter('data_sync_errors_total', 'Data synchronization errors', ['source', 'target'])
    
    async def monitor_data_quality(self):
        """ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
        tables = ['policies', 'users', 'organizations']
        
        for table in tables:
            # ì™„ì„±ë„ ì¸¡ì •
            completeness = await self.calculate_completeness(table)
            self.data_completeness.labels(table=table).set(completeness)
            
            # ì •í™•ì„± ì¸¡ì •
            accuracy = await self.calculate_accuracy(table)
            self.data_accuracy.labels(table=table).set(accuracy)
            
            # ì‹ ì„ ë„ ì¸¡ì •
            freshness = await self.calculate_freshness(table)
            self.data_freshness.labels(table=table).set(freshness)
```

### 9.2. ë°ì´í„° ë¦¬ë‹ˆì§€ ì¶”ì 

```python
class DataLineageTracker:
    """ë°ì´í„° ë¦¬ë‹ˆì§€ ì¶”ì """
    
    def __init__(self):
        self.lineage_graph = nx.DiGraph()
    
    def track_data_transformation(self, source: str, target: str, 
                                transformation: str, metadata: dict):
        """ë°ì´í„° ë³€í™˜ ì¶”ì """
        self.lineage_graph.add_edge(
            source, target,
            transformation=transformation,
            timestamp=datetime.utcnow(),
            metadata=metadata
        )
    
    def get_data_lineage(self, data_id: str) -> dict:
        """íŠ¹ì • ë°ì´í„°ì˜ ë¦¬ë‹ˆì§€ ì¡°íšŒ"""
        upstream = list(nx.ancestors(self.lineage_graph, data_id))
        downstream = list(nx.descendants(self.lineage_graph, data_id))
        
        return {
            "data_id": data_id,
            "upstream_dependencies": upstream,
            "downstream_impacts": downstream,
            "transformation_path": self.get_transformation_path(data_id)
        }
    
    def impact_analysis(self, source_change: str) -> List[str]:
        """ë³€ê²½ ì˜í–¥ ë¶„ì„"""
        affected_data = list(nx.descendants(self.lineage_graph, source_change))
        return affected_data
```

---

**ğŸ“‹ ê´€ë ¨ ë¬¸ì„œ**
- [ì‹œìŠ¤í…œ ê°œìš”](./01_SYSTEM_OVERVIEW.md)
- [ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì„¤ê³„](./02_MICROSERVICES_DESIGN.md)
- [ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ](../03_DATA_AND_APIS/01_DATABASE_SCHEMA.md)
- [ì´ì¤‘ íŠ¸ë™ íŒŒì´í”„ë¼ì¸](../02_CORE_COMPONENTS/01_DUAL_TRACK_PIPELINE.md)
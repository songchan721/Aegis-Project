"""
Aegis Shared Library ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” shared libraryì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import asyncio
import time
import statistics
import json
from datetime import datetime
from typing import Dict, List, Any
import psutil
import gc

from aegis_shared.database import DatabaseManager, BaseRepository
from aegis_shared.auth import JWTHandler
from aegis_shared.cache import CacheClient
from aegis_shared.monitoring import MetricsCollector
from aegis_shared.logging import configure_logging, get_logger

class BenchmarkRunner:
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.results: Dict[str, Any] = {}
        self.system_info = self._get_system_info()
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        return {
            "cpu_count": psutil.cpu_count(),
            "memory_total": psutil.virtual_memory().total / (1024**3),  # GB
            "python_version": f"{psutil.sys.version_info.major}.{psutil.sys.version_info.minor}.{psutil.sys.version_info.micro}",
            "timestamp": datetime.utcnow().isoformat()
        }
    
    def measure_performance(self, name: str, func, iterations: int = 1000, *args, **kwargs):
        """ì„±ëŠ¥ ì¸¡ì •"""
        execution_times = []
        memory_usage = []
        
        # ì›Œë°ì—…
        for _ in range(10):
            if asyncio.iscoroutinefunction(func):
                asyncio.run(func(*args, **kwargs))
            else:
                func(*args, **kwargs)
        
        # ì‹¤ì œ ì¸¡ì •
        for i in range(iterations):
            gc.collect()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            start_time = time.perf_counter()
            
            if asyncio.iscoroutinefunction(func):
                asyncio.run(func(*args, **kwargs))
            else:
                func(*args, **kwargs)
            
            end_time = time.perf_counter()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            
            execution_times.append(end_time - start_time)
            memory_usage.append(end_memory - start_memory)
        
        # í†µê³„ ê³„ì‚°
        self.results[name] = {
            "iterations": iterations,
            "execution_time": {
                "mean": statistics.mean(execution_times),
                "median": statistics.median(execution_times),
                "min": min(execution_times),
                "max": max(execution_times),
                "p95": self._percentile(execution_times, 95),
                "p99": self._percentile(execution_times, 99),
                "stdev": statistics.stdev(execution_times) if len(execution_times) > 1 else 0
            },
            "memory_usage": {
                "mean": statistics.mean(memory_usage),
                "median": statistics.median(memory_usage),
                "min": min(memory_usage),
                "max": max(memory_usage)
            },
            "throughput": {
                "ops_per_second": iterations / sum(execution_times),
                "ms_per_op": statistics.mean(execution_times) * 1000
            }
        }
    
    def _percentile(self, data: List[float], percentile: int) -> float:
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]
    
    def run_all_benchmarks(self):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ Aegis Shared Library ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # JWT ë²¤ì¹˜ë§ˆí¬
        self._benchmark_jwt()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë²¤ì¹˜ë§ˆí¬
        self._benchmark_database()
        
        # ìºì‹œ ë²¤ì¹˜ë§ˆí¬
        self._benchmark_cache()
        
        # ë¡œê¹… ë²¤ì¹˜ë§ˆí¬
        self._benchmark_logging()
        
        # ëª¨ë‹ˆí„°ë§ ë²¤ì¹˜ë§ˆí¬
        self._benchmark_monitoring()
        
        # ë™ì‹œì„± ë²¤ì¹˜ë§ˆí¬
        self._benchmark_concurrency()
        
        print("âœ… ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    
    def _benchmark_jwt(self):
        """JWT ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ“Š JWT ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        jwt_handler = JWTHandler("benchmark-secret-key")
        
        # í† í° ìƒì„± ë²¤ì¹˜ë§ˆí¬
        def create_token():
            return jwt_handler.create_access_token({
                "user_id": "user-123",
                "email": "user@example.com",
                "role": "user",
                "permissions": ["read", "write"]
            })
        
        self.measure_performance("jwt_token_creation", create_token, 10000)
        
        # í† í° ê²€ì¦ ë²¤ì¹˜ë§ˆí¬
        token = create_token()
        
        def verify_token():
            return jwt_handler.verify_token(token)
        
        self.measure_performance("jwt_token_verification", verify_token, 10000)
    
    def _benchmark_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        async def database_operations():
            db_manager = DatabaseManager("sqlite+aiosqlite:///:memory:")
            async with db_manager.session() as session:
                await session.execute("SELECT 1")
            await db_manager.close()
        
        self.measure_performance("database_connection", database_operations, 100)
        
        # Repository ë²¤ì¹˜ë§ˆí¬
        from unittest.mock import AsyncMock, Mock
        
        async def repository_operations():
            mock_session = AsyncMock()
            mock_model = Mock()
            repo = BaseRepository(mock_session, mock_model)
            
            await repo.get_by_id("test-id")
            await repo.create(mock_model)
            await repo.update("test-id", name="test")
            await repo.delete("test-id")
        
        self.measure_performance("repository_crud", repository_operations, 1000)
    
    def _benchmark_cache(self):
        """ìºì‹œ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ“Š ìºì‹œ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        from unittest.mock import AsyncMock
        
        async def cache_operations():
            mock_redis = AsyncMock()
            mock_redis.get.return_value = b'{"key": "value"}'
            mock_redis.set.return_value = True
            mock_redis.delete.return_value = 1
            
            cache_client = CacheClient(mock_redis)
            
            await cache_client.set("test-key", {"data": "value"}, ttl=300)
            await cache_client.get("test-key")
            await cache_client.delete("test-key")
        
        self.measure_performance("cache_operations", cache_operations, 1000)
    
    def _benchmark_logging(self):
        """ë¡œê¹… ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ“Š ë¡œê¹… ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        configure_logging("benchmark-test", "INFO")
        logger = get_logger(__name__)
        
        def logging_operation():
            logger.info(
                "Benchmark log message",
                user_id="user-123",
                action="benchmark",
                data={"key": "value", "number": 42}
            )
        
        self.measure_performance("logging", logging_operation, 10000)
    
    def _benchmark_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        metrics = MetricsCollector()
        
        def monitoring_operations():
            metrics.increment_counter("benchmark_counter", labels={"type": "test"})
            metrics.observe_histogram("benchmark_histogram", value=0.123)
            metrics.set_gauge("benchmark_gauge", value=42)
        
        self.measure_performance("monitoring", monitoring_operations, 10000)
    
    def _benchmark_concurrency(self):
        """ë™ì‹œì„± ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ“Š ë™ì‹œì„± ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        jwt_handler = JWTHandler("benchmark-secret-key")
        
        async def concurrent_jwt_operations():
            tasks = []
            for i in range(100):
                async def jwt_task():
                    token = jwt_handler.create_access_token({
                        "user_id": f"user-{i}",
                        "email": f"user{i}@example.com"
                    })
                    return jwt_handler.verify_token(token)
                
                tasks.append(jwt_task())
            
            await asyncio.gather(*tasks)
        
        self.measure_performance("concurrent_jwt", concurrent_jwt_operations, 100)
    
    def generate_report(self) -> str:
        """ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            "benchmark_info": {
                "library": "aegis-shared",
                "version": "1.0.0",
                "timestamp": datetime.utcnow().isoformat()
            },
            "system_info": self.system_info,
            "results": self.results,
            "summary": self._generate_summary()
        }
        
        return json.dumps(report, indent=2)
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ìš”ì•½ ì •ë³´ ìƒì„±"""
        summary = {}
        
        for name, result in self.results.items():
            summary[name] = {
                "avg_ms": result["execution_time"]["mean"] * 1000,
                "ops_per_sec": result["throughput"]["ops_per_second"],
                "p95_ms": result["execution_time"]["p95"] * 1000,
                "memory_mb": result["memory_usage"]["mean"]
            }
        
        return summary
    
    def print_summary(self):
        """ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ¯ AEGIS SHARED LIBRARY ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("="*80)
        
        print(f"ğŸ“‹ ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   CPU ì½”ì–´: {self.system_info['cpu_count']}")
        print(f"   ë©”ëª¨ë¦¬: {self.system_info['memory_total']:.1f} GB")
        print(f"   Python: {self.system_info['python_version']}")
        print()
        
        print("ğŸ“Š ì„±ëŠ¥ ê²°ê³¼:")
        for name, result in self.results.items():
            avg_ms = result["execution_time"]["mean"] * 1000
            ops_per_sec = result["throughput"]["ops_per_second"]
            p95_ms = result["execution_time"]["p95"] * 1000
            
            print(f"   {name:25} | {avg_ms:8.3f} ms | {ops_per_sec:10,.0f} ops/sec | P95: {p95_ms:8.3f} ms")
        
        print("\nğŸ¯ ì„±ëŠ¥ ë“±ê¸‰:")
        self._print_performance_grades()
        
        print("="*80)
    
    def _print_performance_grades(self):
        """ì„±ëŠ¥ ë“±ê¸‰ ì¶œë ¥"""
        grades = {
            "jwt_token_creation": {"excellent": 0.1, "good": 0.5, "acceptable": 1.0},
            "jwt_token_verification": {"excellent": 0.1, "good": 0.5, "acceptable": 1.0},
            "database_connection": {"excellent": 10, "good": 50, "acceptable": 100},
            "repository_crud": {"excellent": 0.1, "good": 0.5, "acceptable": 1.0},
            "cache_operations": {"excellent": 0.1, "good": 0.5, "acceptable": 1.0},
            "logging": {"excellent": 0.01, "good": 0.05, "acceptable": 0.1},
            "monitoring": {"excellent": 0.01, "good": 0.05, "acceptable": 0.1}
        }
        
        for name, result in self.results.items():
            if name not in grades:
                continue
            
            avg_ms = result["execution_time"]["mean"] * 1000
            thresholds = grades[name]
            
            if avg_ms <= thresholds["excellent"]:
                grade = "ğŸŸ¢ EXCELLENT"
            elif avg_ms <= thresholds["good"]:
                grade = "ğŸŸ¡ GOOD"
            elif avg_ms <= thresholds["acceptable"]:
                grade = "ğŸŸ  ACCEPTABLE"
            else:
                grade = "ğŸ”´ NEEDS IMPROVEMENT"
            
            print(f"   {name:25} | {grade}")
    
    def save_report(self, filename: str = None):
        """ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥"""
        if filename is None:
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            filename = f"benchmark_report_{timestamp}.json"
        
        with open(filename, 'w') as f:
            f.write(self.generate_report())
        
        print(f"ğŸ“„ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ì €ì¥: {filename}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Aegis Shared Library ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...\n")
    
    runner = BenchmarkRunner()
    runner.run_all_benchmarks()
    runner.print_summary()
    runner.save_report()
    
    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
# Logging & Audit Service Design Document

## 1. Overview

Logging & Audit ServiceëŠ” ì´ì§€ìŠ¤(Aegis) ì‹œìŠ¤í…œì˜ **ì¤‘ì•™ ë¡œê¹… ë° ê°ì‚¬ ì¶”ì ** ì„œë¹„ìŠ¤ë¡œ, ëª¨ë“  ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ ìˆ˜ì§‘, ì €ì¥, ë¶„ì„ ë° ê°ì‚¬ ì¶”ì ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

### 1.1 í•µì‹¬ ì±…ì„

1. **ì¤‘ì•™ ë¡œê·¸ ìˆ˜ì§‘** - ëª¨ë“  ì„œë¹„ìŠ¤ë¡œë¶€í„° êµ¬ì¡°í™”ëœ ë¡œê·¸ ìˆ˜ì§‘
2. **ê°ì‚¬ ì¶”ì  (Audit Trail)** - ì‚¬ìš©ì í–‰ë™ ë° ì‹œìŠ¤í…œ ë³€ê²½ ì¶”ì 
3. **ë¡œê·¸ ì €ì¥ ë° ì¸ë±ì‹±** - Elasticsearch ê¸°ë°˜ ë¡œê·¸ ì €ì¥ ë° ê²€ìƒ‰
4. **ë¡œê·¸ ë¶„ì„ ë° ì•Œë¦¼** - ì´ìƒ íŒ¨í„´ ê°ì§€ ë° ì‹¤ì‹œê°„ ì•Œë¦¼
5. **ê·œì • ì¤€ìˆ˜ (Compliance)** - GDPR, ê°œì¸ì •ë³´ë³´í˜¸ë²• ë“± ê·œì • ì¤€ìˆ˜
6. **ë¡œê·¸ ë³´ê´€ ë° ì•„ì¹´ì´ë¹™** - ë¡œê·¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬

### 1.2 ì—­í•  ëª…í™•í™” (ì¤‘ìš”)

**Logging-Audit Serviceì˜ ì—­í• :**
- âœ… ëª¨ë“  ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ ì¤‘ì•™ ìˆ˜ì§‘ ë° ì €ì¥
- âœ… ê°ì‚¬ ì´ë²¤íŠ¸ ì¶”ì  ë° ë¶„ì„
- âœ… ë¡œê·¸ ê²€ìƒ‰ ë° ë¶„ì„ API ì œê³µ
- âœ… ê·œì • ì¤€ìˆ˜ë¥¼ ìœ„í•œ ê°ì‚¬ ë³´ê³ ì„œ ìƒì„±
- âœ… ë³´ì•ˆ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼
- âœ… ë¡œê·¸ ë³´ê´€ ì •ì±… ê´€ë¦¬

**Logging-Audit Serviceê°€ í•˜ì§€ ì•ŠëŠ” ê²ƒ:**
- âŒ ê°œë³„ ì„œë¹„ìŠ¤ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (ê° ì„œë¹„ìŠ¤ ë‹´ë‹¹)
- âŒ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§ (Prometheus/Grafana ë‹´ë‹¹)
- âŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (APM ë„êµ¬ ë‹´ë‹¹)
- âŒ ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ (Infrastructure Service ë‹´ë‹¹)

### 1.3 ì„¤ê³„ ì›ì¹™

1. **ì¤‘ì•™í™”**: ëª¨ë“  ë¡œê·¸ë¥¼ ì¤‘ì•™ì—ì„œ ìˆ˜ì§‘ ë° ê´€ë¦¬
2. **í™•ì¥ì„±**: ëŒ€ìš©ëŸ‰ ë¡œê·¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìˆ˜í‰ í™•ì¥ ì„¤ê³„
3. **ì‹¤ì‹œê°„ì„±**: ì‹¤ì‹œê°„ ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„
4. **ë³´ì•ˆì„±**: ë¡œê·¸ ë°ì´í„° ì•”í˜¸í™” ë° ì ‘ê·¼ ì œì–´
5. **ê·œì • ì¤€ìˆ˜**: GDPR, ê°œì¸ì •ë³´ë³´í˜¸ë²• ë“± ì¤€ìˆ˜
6. **ê´€ì°°ì„±**: ë¡œê¹… ì‹œìŠ¤í…œ ìì²´ì˜ ëª¨ë‹ˆí„°ë§

### 1.4 ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì™€ì˜ ì—­í•  êµ¬ë¶„

| ì„œë¹„ìŠ¤ | ì—­í•  | í†µì‹  ë°©ì‹ |
|--------|------|------------|
| **All Services** | êµ¬ì¡°í™”ëœ ë¡œê·¸ ìƒì„± | Filebeat â†’ Logstash |
| **Logging-Audit Service** | ë¡œê·¸ ìˆ˜ì§‘, ì €ì¥, ë¶„ì„ | Elasticsearch ì €ì¥ |
| **User Service** | ì‚¬ìš©ì ì¸ì¦ ì •ë³´ ì œê³µ | API í˜¸ì¶œ |
| **Policy Service** | ì •ì±… ë³€ê²½ ê°ì‚¬ | ê°ì‚¬ ë¡œê·¸ |
| **Infrastructure** | ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ | Prometheus |

---

## 2. Shared Library Integration

ì´ ì„œë¹„ìŠ¤ëŠ” `aegis-shared` ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì ê·¹ í™œìš©í•©ë‹ˆë‹¤.

### 2.1 ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆ

```python
from aegis_shared.database import BaseRepository, DatabaseManager
from aegis_shared.auth import JWTHandler, AuthMiddleware, RBAC
from aegis_shared.logging import get_logger, add_context, LogLevel
from aegis_shared.monitoring import track_metrics
from aegis_shared.errors import ErrorCode, ServiceException
from aegis_shared.cache import cache_result, RedisClient
from aegis_shared.config import get_config
```

### 2.2 ì™œ Shared Libraryë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?

#### 2.2.1 êµ¬ì¡°í™”ëœ ë¡œê¹… í‘œì¤€í™”

**Before (shared-library ì—†ì´):**
```python
# âŒ ë¬¸ì œì : ê° ì„œë¹„ìŠ¤ë§ˆë‹¤ ë‹¤ë¥¸ ë¡œê·¸ í˜•ì‹, ì¼ê´€ì„± ë¶€ì¡±
import logging

logger = logging.getLogger(__name__)
logger.info(f"User {user_id} logged in from {ip_address}")  # ë¹„êµ¬ì¡°í™”
logger.error(f"Database error: {error}")  # ê²€ìƒ‰ ì–´ë ¤ì›€
```

**After (shared-library ì‚¬ìš©):**
```python
# âœ… ì¥ì : êµ¬ì¡°í™”ëœ ë¡œê¹…, ìë™ ì»¨í…ìŠ¤íŠ¸, í‘œì¤€í™”
from aegis_shared.logging import get_logger, add_context

logger = get_logger(__name__)

# êµ¬ì¡°í™”ëœ ë¡œê¹…
logger.info(
    "user_login_success",
    user_id=user_id,
    ip_address=ip_address,
    user_agent=request.headers.get("user-agent"),
    session_id=session_id
)

# ìë™ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (request_id, service_name ë“±)
logger.error(
    "database_connection_failed",
    error_code="DB_001",
    database="postgresql",
    retry_count=3
)
```

**íš¨ê³¼:**
- ëª¨ë“  ì„œë¹„ìŠ¤ì—ì„œ ì¼ê´€ëœ ë¡œê·¸ í˜•ì‹
- Elasticsearchì—ì„œ ì‰½ê²Œ ê²€ìƒ‰ ê°€ëŠ¥
- ìë™ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (request_id, user_id, service_name)
- ë¡œê·¸ ë ˆë²¨ í‘œì¤€í™”

#### 2.2.2 BaseRepositoryë¡œ ê°ì‚¬ ë¡œê·¸ ê´€ë¦¬

**Before:**
```python
# âŒ ë°˜ë³µì ì¸ CRUD ì½”ë“œ, íŠ¸ëœì­ì…˜ ê´€ë¦¬ ë³µì¡
class AuditLogRepository:
    def __init__(self, session):
        self.session = session
    
    async def create_audit_log(self, log_data: dict):
        audit_log = AuditLog(**log_data)
        self.session.add(audit_log)
        await self.session.commit()
        return audit_log
    
    async def find_by_user_id(self, user_id: str):
        result = await self.session.execute(
            select(AuditLog).where(AuditLog.user_id == user_id)
        )
        return result.scalars().all()
```

**After:**
```python
# âœ… BaseRepository ìƒì†ìœ¼ë¡œ CRUD ìë™í™”
from aegis_shared.database import BaseRepository

class AuditLogRepository(BaseRepository[AuditLog]):
    def __init__(self, session: AsyncSession):
        super().__init__(session, AuditLog)
    
    # create, get_by_id, update, delete, list, count ìë™ ì œê³µ
    
    # ë„ë©”ì¸ íŠ¹í™” ë©”ì„œë“œë§Œ ì¶”ê°€
    async def find_by_user_and_action(
        self,
        user_id: str,
        action: str,
        start_date: datetime,
        end_date: datetime
    ) -> List[AuditLog]:
        return await self.list(
            filters={
                "user_id": user_id,
                "action": action,
                "created_at__gte": start_date,
                "created_at__lte": end_date
            }
        )
```

**íš¨ê³¼:**
- ì½”ë“œ ë¼ì¸ ìˆ˜ 70% ê°ì†Œ
- íŠ¸ëœì­ì…˜ ê´€ë¦¬ ìë™í™”
- ì—ëŸ¬ ì²˜ë¦¬ í‘œì¤€í™”
- í˜ì´ì§•, í•„í„°ë§ ìë™ ì§€ì›

---

## 3. Architecture

### 3.1 ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "All Services"
        USER_SVC[User Service]
        POLICY_SVC[Policy Service]
        SEARCH_SVC[Search Service]
        REC_SVC[Recommendation Service]
        API_GW[API Gateway]
    end
    
    subgraph "Log Collection Layer"
        FILEBEAT[Filebeat<br/>ë¡œê·¸ ìˆ˜ì§‘ Agent]
        LOGSTASH[Logstash<br/>ë¡œê·¸ íŒŒì‹± ë° ë³€í™˜]
    end
    
    subgraph "Logging-Audit Service"
        subgraph "API Layer"
            SEARCH_API[Log Search API<br/>ë¡œê·¸ ê²€ìƒ‰ ë° ì¡°íšŒ]
            AUDIT_API[Audit API<br/>ê°ì‚¬ ì¶”ì  ì¡°íšŒ]
            REPORT_API[Report API<br/>ê°ì‚¬ ë³´ê³ ì„œ ìƒì„±]
            ADMIN_API[Admin API<br/>ë¡œê·¸ ê´€ë¦¬]
        end
        
        subgraph "Business Logic Layer"
            LOG_SERVICE[Log Search Service]
            AUDIT_SERVICE[Audit Service]
            ALERT_SERVICE[Alert Service]
            REPORT_SERVICE[Report Service]
        end
        
        subgraph "Repository Layer"
            AUDIT_REPO[Audit Repository<br/>PostgreSQL]
            ALERT_REPO[Alert Repository<br/>PostgreSQL]
        end
    end
    
    subgraph "Data Storage"
        ELASTICSEARCH[(Elasticsearch<br/>ë¡œê·¸ ì €ì¥ ë° ê²€ìƒ‰)]
        POSTGRESQL[(PostgreSQL<br/>ê°ì‚¬ ë©”íƒ€ë°ì´í„°)]
        REDIS[(Redis<br/>ìºì‹œ ë° ì„¸ì…˜)]
    end
    
    subgraph "External Systems"
        KIBANA[Kibana<br/>Dashboard]
        ALERTMANAGER[Alertmanager<br/>ì•Œë¦¼ ë°œì†¡]
        PROMETHEUS[Prometheus<br/>ë©”íŠ¸ë¦­ ìˆ˜ì§‘]
    end
    
    %% ë¡œê·¸ ìˆ˜ì§‘ íë¦„
    USER_SVC -->|ë¡œê·¸ íŒŒì¼| FILEBEAT
    POLICY_SVC -->|ë¡œê·¸ íŒŒì¼| FILEBEAT
    SEARCH_SVC -->|ë¡œê·¸ íŒŒì¼| FILEBEAT
    REC_SVC -->|ë¡œê·¸ íŒŒì¼| FILEBEAT
    API_GW -->|ë¡œê·¸ íŒŒì¼| FILEBEAT
    
    FILEBEAT --> LOGSTASH
    LOGSTASH --> ELASTICSEARCH
    
    %% API íë¦„
    SEARCH_API --> LOG_SERVICE
    AUDIT_API --> AUDIT_SERVICE
    REPORT_API --> REPORT_SERVICE
    ADMIN_API --> LOG_SERVICE
    
    %% ì„œë¹„ìŠ¤ ë ˆì´ì–´
    LOG_SERVICE --> ELASTICSEARCH
    AUDIT_SERVICE --> AUDIT_REPO
    ALERT_SERVICE --> ALERT_REPO
    REPORT_SERVICE --> AUDIT_REPO
    
    %% ì €ì¥ì†Œ ì—°ê²°
    AUDIT_REPO --> POSTGRESQL
    ALERT_REPO --> POSTGRESQL
    
    %% ìºì‹±
    LOG_SERVICE --> REDIS
    AUDIT_SERVICE --> REDIS
    
    %% ì™¸ë¶€ ì‹œìŠ¤í…œ
    ELASTICSEARCH --> KIBANA
    ALERT_SERVICE --> ALERTMANAGER
    LOGGING_AUDIT_SVC --> PROMETHEUS
```

### 3.2 ë¡œê·¸ ìˆ˜ì§‘ íë¦„

```mermaid
sequenceDiagram
    participant Services as All Services
    participant Filebeat as Filebeat
    participant Logstash as Logstash
    participant Elasticsearch as Elasticsearch
    participant LoggingService as Logging-Audit Service
    participant Alertmanager as Alertmanager
    
    Note over Services,Alertmanager: 1. ì„œë¹„ìŠ¤ì—ì„œ ë¡œê·¸ ìƒì„±
    Services->>Services: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‹¤í–‰
    Services->>Services: êµ¬ì¡°í™”ëœ ë¡œê·¸ ìƒì„± (shared-library)
    Services->>Filebeat: ë¡œê·¸ íŒŒì¼ ìƒì„±
    
    Note over Services,Alertmanager: 2. ë¡œê·¸ ìˆ˜ì§‘ ë° ì²˜ë¦¬
    Filebeat->>Logstash: ë¡œê·¸ ì „ì†¡
    Logstash->>Logstash: ë¡œê·¸ íŒŒì‹± ë° ì •ê·œí™”
    Logstash->>Logstash: ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹
    
    Note over Services,Alertmanager: 3. ë¡œê·¸ ì €ì¥ ë° ì¸ë±ì‹±
    Logstash->>Elasticsearch: ë¡œê·¸ ë°ì´í„° ì €ì¥
    
    Note over Services,Alertmanager: 4. ê°ì‚¬ ë¡œê·¸ ì²˜ë¦¬
    LoggingService->>Elasticsearch: ê°ì‚¬ ë¡œê·¸ ì¡°íšŒ
    LoggingService->>LoggingService: ê°ì‚¬ ë©”íƒ€ë°ì´í„° ìƒì„±
    LoggingService->>PostgreSQL: ê°ì‚¬ ë©”íƒ€ë°ì´í„° ì €ì¥
    
    Note over Services,Alertmanager: 5. ì•Œë¦¼ ë° ëŒ€ì‹œë³´ë“œ
    LoggingService->>Alertmanager: ì´ìƒ íŒ¨í„´ ì•Œë¦¼
    Elasticsearch->>Kibana: Kibana ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
```

---

## 4. Components and Interfaces

### 4.1 API Layer

#### 4.1.1 Log Search API

```python
from fastapi import APIRouter, Depends, HTTPException, status, Query
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
from aegis_shared.auth import require_role, get_current_user
from aegis_shared.monitoring import track_metrics
from aegis_shared.logging import get_logger
from aegis_shared.errors import ErrorCode, ServiceException

router = APIRouter(prefix="/api/v1/logs", tags=["logs"])
logger = get_logger(__name__)

@router.get(
    "/search",
    response_model=PaginatedLogResponse
)
@track_metrics("log_search")
@require_role([" admin", "moderator", "analyst"])
async def search_logs(
    q: Optional[str] = Query(None, description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    service: Optional[str] = Query(None, description="ì„œë¹„ìŠ¤ í•„í„°"),
    level: Optional[str] = Query(None, description="ë¡œê·¸ ë ˆë²¨ í•„í„°"),
    start_time: Optional[datetime] = Query(None, description="ì‹œì‘ ì‹œê°„"),
    end_time: Optional[datetime] = Query(None, description="ì¢…ë£Œ ì‹œê°„"),
    page: int = Query(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    size: int = Query(20, ge=1, le=1000, description="í˜ì´ì§€ í¬ê¸°"),
    log_service: LogSearchService = Depends(get_log_search_service),
    current_user: dict = Depends(get_current_user)
):
    """
    ë¡œê·¸ ê²€ìƒ‰
    
    - ê´€ë¦¬ì, ëª¨ë”ë ˆì´í„°, ë¶„ì„ê°€ë§Œ ì ‘ê·¼ ê°€ëŠ¥
    - Elasticsearch ê¸°ë°˜ ì „ë¬¸ ê²€ìƒ‰
    - ì‹œê°„ ë²”ìœ„, ì„œë¹„ìŠ¤, ë¡œê·¸ ë ˆë²¨ í•„í„°ë§ ì§€ì›
    """
    try:
        search_params = {
            'query': q,
            'service': service,
            'level': level,
            'start_time': start_time or datetime.utcnow() - timedelta(hours=24),
            'end_time': end_time or datetime.utcnow(),
            'page': page,
            'size': size
        }
        
        result = await log_service.search_logs(search_params)
        
        logger.info(
            "log_search_completed",
            user_id=current_user.get('user_id'),
            query=q,
            result_count=result['total']
        )
        
        return PaginatedLogResponse(
            items=result['logs'],
            total=result['total'],
            page=page,
            size=size,
            total_pages=(result['total'] + size - 1) // size
        )
        
    except Exception as e:
        logger.error(
            "log_search_failed",
            user_id=current_user.get('user_id'),
            search_params=search_params,
            error=str(e),
            exc_info=True
        )
        raise ServiceException(
            error_code=ErrorCode.LOG_SEARCH_FAILED,
            message="Log search failed",
            details={"error": str(e)}
        )

@router.get(
    "/recent/{service}",
    response_model=List[LogEntry]
)
@track_metrics("recent_logs")
@require_role(["admin", "moderator", "analyst"])
async def get_recent_logs(
    service: str,
    limit: int = Query(50, ge=1, le=500),
    log_service: LogSearchService = Depends(get_log_search_service)
):
    """íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ìµœê·¼ ë¡œê·¸ ì¡°íšŒ (ìºì‹œ ê¸°ë°˜)"""
    return await log_service.get_recent_logs(service, limit)

@router.get(
    "/errors",
    response_model=PaginatedErrorLogResponse
)
@track_metrics("error_logs")
@require_role(["admin", "moderator"])
async def get_error_logs(
    service: Optional[str] = Query(None),
    error_code: Optional[str] = Query(None),
    start_time: Optional[datetime] = Query(None),
    end_time: Optional[datetime] = Query(None),
    page: int = Query(1, ge=1),
    size: int = Query(20, ge=1, le=100),
    log_service: LogSearchService = Depends(get_log_search_service)
):
    """ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ ë° ë¶„ì„"""
    search_params = {
        'service': service,
        'error_code': error_code,
        'start_time': start_time or datetime.utcnow() - timedelta(hours=24),
        'end_time': end_time or datetime.utcnow(),
        'page': page,
        'size': size
    }
    
    result = await log_service.search_error_logs(search_params)
    
    return PaginatedErrorLogResponse(
        items=result['errors'],
        total=result['total'],
        page=page,
        size=size,
        error_statistics=result['statistics']
    )
```

#### 4.1.2 Audit API

```python
@router.get(
    "/audit/trail/{user_id}",
    response_model=UserAuditTrailResponse
)
@track_metrics("audit_trail")
@require_role(["admin", "auditor"])
async def get_user_audit_trail(
    user_id: str,
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None),
    action_type: Optional[str] = Query(None),
    page: int = Query(1, ge=1),
    size: int = Query(50, ge=1, le=500),
    audit_service: AuditService = Depends(get_audit_service),
    current_user: dict = Depends(get_current_user)
):
    """
    ì‚¬ìš©ì ê°ì‚¬ ì¶”ì  ì¡°íšŒ
    
    - íŠ¹ì • ì‚¬ìš©ìì˜ ëª¨ë“  í–‰ë™ ì´ë ¥ ì¡°íšŒ
    - ê´€ë¦¬ì, ê°ì‚¬ìë§Œ ì ‘ê·¼ ê°€ëŠ¥
    """
    search_params = {
        'user_id': user_id,
        'start_date': start_date or datetime.utcnow() - timedelta(days=30),
        'end_date': end_date or datetime.utcnow(),
        'action_type': action_type,
        'page': page,
        'size': size
    }
    
    result = await audit_service.get_user_audit_trail(search_params)
    
    logger.info(
        "audit_trail_retrieved",
        auditor_id=current_user.get('user_id'),
        target_user_id=user_id,
        result_count=result['total']
    )
    
    return UserAuditTrailResponse(
        user_id=user_id,
        audit_entries=result['entries'],
        total=result['total'],
        page=page,
        size=size,
        summary=result['summary']
    )

@router.get(
    "/audit/reports/compliance",
    response_model=ComplianceReportResponse
)
@track_metrics("compliance_report")
@require_role(["admin", "compliance_officer"])
async def generate_compliance_report(
    report_type: str = Query(..., description="ë³´ê³ ì„œ íƒ€ì… (gdpr, privacy, security)"),
    start_date: datetime = Query(..., description="ì‹œì‘ ë‚ ì§œ"),
    end_date: datetime = Query(..., description="ì¢…ë£Œ ë‚ ì§œ"),
    format: str = Query("json", description="ì¶œë ¥ í˜•ì‹ (json, pdf, csv)"),
    audit_service: AuditService = Depends(get_audit_service),
    current_user: dict = Depends(get_current_user)
):
    """
    ê·œì • ì¤€ìˆ˜ ë³´ê³ ì„œ ìƒì„±
    
    - GDPR, ê°œì¸ì •ë³´ë³´í˜¸ë²• ë“± ê·œì • ì¤€ìˆ˜ ë³´ê³ ì„œ
    - ê´€ë¦¬ì, ì»´í”Œë¼ì´ì–¸ìŠ¤ ë‹´ë‹¹ìë§Œ ì ‘ê·¼ ê°€ëŠ¥
    """
    report_params = {
        'report_type': report_type,
        'start_date': start_date,
        'end_date': end_date,
        'format': format
    }
    
    report = await audit_service.generate_compliance_report(report_params)
    
    logger.info(
        "compliance_report_generated",
        officer_id=current_user.get('user_id'),
        report_type=report_type,
        report_id=report['id']
    )
    
    return ComplianceReportResponse(
        report_id=report['id'],
        report_type=report_type,
        generated_at=datetime.utcnow(),
        data=report['data'],
        download_url=report.get('download_url')
    )
```

### 4.2 Business Logic Layer

#### 4.2.1 Log Search Service

```python
from elasticsearch import AsyncElasticsearch
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from aegis_shared.cache import cache_result, RedisClient
from aegis_shared.logging import get_logger
from aegis_shared.monitoring import track_metrics
from aegis_shared.errors import ErrorCode, ServiceException

logger = get_logger(__name__)

class LogSearchService:
    """ë¡œê·¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    def __init__(
        self,
        elasticsearch_client: AsyncElasticsearch,
        redis_client: RedisClient
    ):
        self.es_client = elasticsearch_client
        self.redis_client = redis_client
    
    @track_metrics("log_search_query")
    async def search_logs(
        self,
        search_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë¡œê·¸ ê²€ìƒ‰ (Elasticsearch ê¸°ë°˜)"""
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            query = self.build_search_query(search_params)
            
            # ì¸ë±ìŠ¤ íŒ¨í„´ ìƒì„± (ë‚ ì§œ ë²”ìœ„ ê¸°ë°˜)
            index_pattern = self.build_index_pattern(
                search_params['start_time'],
                search_params['end_time']
            )
            
            # Elasticsearch ê²€ìƒ‰ ì‹¤í–‰
            response = await self.es_client.search(
                index=index_pattern,
                body={
                    'query': query,
                    'sort': [{'timestamp': {'order': 'desc'}}],
                    'from': (search_params['page'] - 1) * search_params['size'],
                    'size': search_params['size'],
                    'highlight': {
                        'fields': {
                            'message': {},
                            'event_type': {}
                        }
                    }
                }
            )
            
            # ê²°ê³¼ íŒŒì‹±
            logs = []
            for hit in response['hits']['hits']:
                log_entry = hit['_source']
                log_entry['_id'] = hit['_id']
                log_entry['_score'] = hit['_score']
                
                # í•˜ì´ë¼ì´íŠ¸ ì¶”ê°€
                if 'highlight' in hit:
                    log_entry['_highlight'] = hit['highlight']
                
                logs.append(log_entry)
            
            logger.info(
                "log_search_completed",
                query=search_params.get('query'),
                result_count=len(logs),
                total=response['hits']['total']['value']
            )
            
            return {
                'logs': logs,
                'total': response['hits']['total']['value'],
                'took': response['took'],
                'max_score': response['hits']['max_score']
            }
            
        except Exception as e:
            logger.error(
                "log_search_failed",
                search_params=search_params,
                error=str(e),
                exc_info=True
            )
            raise ServiceException(
                error_code=ErrorCode.LOG_SEARCH_FAILED,
                message="Log search failed",
                details={"error": str(e)}
            )
    
    def build_search_query(self, search_params: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±"""
        must_clauses = []
        filter_clauses = []
        
        # ì‹œê°„ ë²”ìœ„ í•„í„°
        filter_clauses.append({
            'range': {
                'timestamp': {
                    'gte': search_params['start_time'].isoformat(),
                    'lte': search_params['end_time'].isoformat()
                }
            }
        })
        
        # ì„œë¹„ìŠ¤ í•„í„°
        if search_params.get('service'):
            filter_clauses.append({
                'term': {'service.keyword': search_params['service']}
            })
        
        # ë¡œê·¸ ë ˆë²¨ í•„í„°
        if search_params.get('level'):
            filter_clauses.append({
                'term': {'level.keyword': search_params['level']}
            })
        
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰
        if search_params.get('query'):
            must_clauses.append({
                'multi_match': {
                    'query': search_params['query'],
                    'fields': ['message^2', 'event_type', 'data.*'],
                    'type': 'best_fields',
                    'fuzziness': 'AUTO'
                }
            })
        
        # ì¿¼ë¦¬ ì¡°í•©
        if must_clauses:
            return {
                'bool': {
                    'must': must_clauses,
                    'filter': filter_clauses
                }
            }
        else:
            return {
                'bool': {
                    'filter': filter_clauses
                }
            }
    
    def build_index_pattern(
        self,
        start_time: datetime,
        end_time: datetime
    ) -> str:
        """ë‚ ì§œ ë²”ìœ„ì— ë”°ë¥¸ ì¸ë±ìŠ¤ íŒ¨í„´ ìƒì„±"""
        # ë‚ ì§œ ë²”ìœ„ê°€ í•˜ë£¨ ì´ë‚´ë©´ íŠ¹ì • ì¸ë±ìŠ¤
        if (end_time - start_time).days <= 1:
            return f"aegis-logs-{start_time.strftime('%Y.%m.%d')}"
        
        # ì—¬ëŸ¬ ë‚ ì§œì— ê±¸ì¹˜ë©´ ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´
        return "aegis-logs-*"
    
    @cache_result(ttl=300, key_prefix="recent_logs")
    async def get_recent_logs(
        self,
        service: str,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """ìµœê·¼ ë¡œê·¸ ì¡°íšŒ (ìºì‹œ ìš°ì„ )"""
        try:
            # Elasticsearch ê²€ìƒ‰
            response = await self.es_client.search(
                index="aegis-logs-*",
                body={
                    'query': {
                        'bool': {
                            'filter': [
                                {'term': {'service.keyword': service}},
                                {
                                    'range': {
                                        'timestamp': {
                                            'gte': 'now-1h'
                                        }
                                    }
                                }
                            ]
                        }
                    },
                    'sort': [{'timestamp': {'order': 'desc'}}],
                    'size': limit
                }
            )
            
            logs = [hit['_source'] for hit in response['hits']['hits']]
            
            logger.debug(
                "recent_logs_retrieved",
                service=service,
                count=len(logs)
            )
            
            return logs
            
        except Exception as e:
            logger.error(
                "recent_logs_retrieval_failed",
                service=service,
                error=str(e),
                exc_info=True
            )
            raise ServiceException(
                error_code=ErrorCode.LOG_RETRIEVAL_FAILED,
                message="Recent logs retrieval failed",
                details={"service": service, "error": str(e)}
            )
```

#### 4.2.2 Audit Service

```python
from aegis_shared.database import BaseRepository
from typing import Dict, Any, List
from datetime import datetime

class AuditService:
    """ê°ì‚¬ ì„œë¹„ìŠ¤"""
    
    def __init__(
        self,
        audit_repository: 'AuditLogRepository',
        elasticsearch_client: AsyncElasticsearch,
        redis_client: RedisClient
    ):
        self.audit_repo = audit_repository
        self.es_client = elasticsearch_client
        self.redis_client = redis_client
    
    async def get_user_audit_trail(
        self,
        search_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì ê°ì‚¬ ì¶”ì  ì¡°íšŒ"""
        try:
            # Elasticsearchì—ì„œ ê°ì‚¬ ë¡œê·¸ ì¡°íšŒ
            query = {
                'bool': {
                    'filter': [
                        {'term': {'user_id.keyword': search_params['user_id']}},
                        {
                            'range': {
                                'timestamp': {
                                    'gte': search_params['start_date'].isoformat(),
                                    'lte': search_params['end_date'].isoformat()
                                }
                            }
                        }
                    ]
                }
            }
            
            if search_params.get('action_type'):
                query['bool']['filter'].append({
                    'term': {'action_type.keyword': search_params['action_type']}
                })
            
            response = await self.es_client.search(
                index="aegis-audit-*",
                body={
                    'query': query,
                    'sort': [{'timestamp': {'order': 'desc'}}],
                    'from': (search_params['page'] - 1) * search_params['size'],
                    'size': search_params['size']
                }
            )
            
            entries = [hit['_source'] for hit in response['hits']['hits']]
            total = response['hits']['total']['value']
            
            # ìš”ì•½ ì •ë³´ ìƒì„±
            summary = await self.generate_audit_summary(
                search_params['user_id'],
                search_params['start_date'],
                search_params['end_date']
            )
            
            logger.info(
                "audit_trail_retrieved",
                user_id=search_params['user_id'],
                entry_count=len(entries),
                total=total
            )
            
            return {
                'entries': entries,
                'total': total,
                'summary': summary
            }
            
        except Exception as e:
            logger.error(
                "audit_trail_retrieval_failed",
                user_id=search_params['user_id'],
                error=str(e),
                exc_info=True
            )
            raise ServiceException(
                error_code=ErrorCode.AUDIT_TRAIL_FAILED,
                message="Audit trail retrieval failed",
                details={"error": str(e)}
            )
    
    async def generate_audit_summary(
        self,
        user_id: str,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """ê°ì‚¬ ìš”ì•½ ì •ë³´ ìƒì„±"""
        # ì§‘ê³„ ì¿¼ë¦¬ë¡œ ìš”ì•½ ì •ë³´ ìƒì„±
        response = await self.es_client.search(
            index="aegis-audit-*",
            body={
                'query': {
                    'bool': {
                        'filter': [
                            {'term': {'user_id.keyword': user_id}},
                            {
                                'range': {
                                    'timestamp': {
                                        'gte': start_date.isoformat(),
                                        'lte': end_date.isoformat()
                                    }
                                }
                            }
                        ]
                    }
                },
                'aggs': {
                    'action_types': {
                        'terms': {'field': 'action_type.keyword'}
                    },
                    'services': {
                        'terms': {'field': 'service.keyword'}
                    }
                },
                'size': 0
            }
        )
        
        return {
            'total_actions': response['hits']['total']['value'],
            'action_types': response['aggregations']['action_types']['buckets'],
            'services': response['aggregations']['services']['buckets']
        }
    
    async def generate_compliance_report(
        self,
        report_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê·œì • ì¤€ìˆ˜ ë³´ê³ ì„œ ìƒì„±"""
        report_type = report_params['report_type']
        
        if report_type == 'gdpr':
            return await self.generate_gdpr_report(report_params)
        elif report_type == 'privacy':
            return await self.generate_privacy_report(report_params)
        elif report_type == 'security':
            return await self.generate_security_report(report_params)
        else:
            raise ServiceException(
                error_code=ErrorCode.INVALID_REPORT_TYPE,
                message=f"Invalid report type: {report_type}"
            )
    
    async def generate_gdpr_report(
        self,
        report_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """GDPR ì¤€ìˆ˜ ë³´ê³ ì„œ ìƒì„±"""
        # GDPR ê´€ë ¨ ê°ì‚¬ ë¡œê·¸ ìˆ˜ì§‘
        # - ê°œì¸ì •ë³´ ì ‘ê·¼ ì´ë ¥
        # - ê°œì¸ì •ë³´ ìˆ˜ì • ì´ë ¥
        # - ê°œì¸ì •ë³´ ì‚­ì œ ìš”ì²­ ì´ë ¥
        # - ë°ì´í„° ì´ë™ê¶Œ ìš”ì²­ ì´ë ¥
        
        report_id = f"gdpr-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
        
        logger.info(
            "gdpr_report_generated",
            report_id=report_id,
            start_date=report_params['start_date'],
            end_date=report_params['end_date']
        )
        
        return {
            'id': report_id,
            'data': {
                # ë³´ê³ ì„œ ë°ì´í„°
            },
            'download_url': f"/api/v1/audit/reports/{report_id}/download"
        }
```

### 4.3 Repository Layer

```python
from aegis_shared.database import BaseRepository
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Optional
from datetime import datetime

class AuditLogRepository(BaseRepository[AuditLog]):
    """ê°ì‚¬ ë¡œê·¸ ë¦¬í¬ì§€í† ë¦¬"""
    
    def __init__(self, session: AsyncSession):
        super().__init__(session, AuditLog)
    
    async def find_by_user_and_action(
        self,
        user_id: str,
        action: str,
        start_date: datetime,
        end_date: datetime
    ) -> List[AuditLog]:
        """ì‚¬ìš©ì ë° ì•¡ì…˜ë³„ ê°ì‚¬ ë¡œê·¸ ì¡°íšŒ"""
        return await self.list(
            filters={
                "user_id": user_id,
                "action": action,
                "created_at__gte": start_date,
                "created_at__lte": end_date
            },
            order_by=[("created_at", "desc")]
        )
    
    async def find_by_resource(
        self,
        resource_type: str,
        resource_id: str,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> List[AuditLog]:
        """ë¦¬ì†ŒìŠ¤ë³„ ê°ì‚¬ ë¡œê·¸ ì¡°íšŒ"""
        filters = {
            "resource_type": resource_type,
            "resource_id": resource_id
        }
        
        if start_date:
            filters["created_at__gte"] = start_date
        if end_date:
            filters["created_at__lte"] = end_date
        
        return await self.list(
            filters=filters,
            order_by=[("created_at", "desc")]
        )
    
    async def count_by_action_type(
        self,
        action_type: str,
        start_date: datetime,
        end_date: datetime
    ) -> int:
        """ì•¡ì…˜ íƒ€ì…ë³„ ê°ì‚¬ ë¡œê·¸ ìˆ˜ ì¡°íšŒ"""
        return await self.count(
            filters={
                "action_type": action_type,
                "created_at__gte": start_date,
                "created_at__lte": end_date
            }
        )
```

---



## 5. Error Handling

### 5.1 ì¤‘ì•™ ì—ëŸ¬ ì½”ë“œ ì‚¬ìš©

```python
from aegis_shared.errors import ErrorCode, ServiceException

# Logging-Audit Service ì „ìš© ì—ëŸ¬ ì½”ë“œ
class LoggingErrorCode(ErrorCode):
    # ë¡œê·¸ ê²€ìƒ‰ ê´€ë ¨ (3000-3099)
    LOG_SEARCH_FAILED = ("LOG_3001", "Log search failed")
    LOG_RETRIEVAL_FAILED = ("LOG_3002", "Log retrieval failed")
    INVALID_SEARCH_QUERY = ("LOG_3003", "Invalid search query")
    ELASTICSEARCH_CONNECTION_FAILED = ("LOG_3004", "Elasticsearch connection failed")
    
    # ê°ì‚¬ ë¡œê·¸ ê´€ë ¨ (3100-3199)
    AUDIT_TRAIL_FAILED = ("LOG_3101", "Audit trail retrieval failed")
    AUDIT_LOG_CREATION_FAILED = ("LOG_3102", "Audit log creation failed")
    INVALID_AUDIT_PARAMS = ("LOG_3103", "Invalid audit parameters")
    
    # ë³´ê³ ì„œ ìƒì„± ê´€ë ¨ (3200-3299)
    REPORT_GENERATION_FAILED = ("LOG_3201", "Report generation failed")
    INVALID_REPORT_TYPE = ("LOG_3202", "Invalid report type")
    REPORT_EXPORT_FAILED = ("LOG_3203", "Report export failed")
    
    # ì•Œë¦¼ ê´€ë ¨ (3300-3399)
    ALERT_CREATION_FAILED = ("LOG_3301", "Alert creation failed")
    ALERT_DELIVERY_FAILED = ("LOG_3302", "Alert delivery failed")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ (3400-3499)
    AUDIT_DB_CONNECTION_FAILED = ("LOG_3401", "Audit database connection failed")
    AUDIT_DB_QUERY_FAILED = ("LOG_3402", "Audit database query failed")
```

### 5.2 ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´

#### 5.2.1 API Layer ì—ëŸ¬ ì²˜ë¦¬

```python
from fastapi import HTTPException, status
from aegis_shared.errors import ServiceException
from aegis_shared.logging import get_logger

logger = get_logger(__name__)

@router.get("/logs/search")
async def search_logs(...):
    try:
        result = await log_service.search_logs(search_params)
        return result
        
    except ServiceException as e:
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì—ëŸ¬
        logger.error(
            "service_exception_occurred",
            error_code=e.error_code,
            message=e.message,
            details=e.details
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "error_code": e.error_code,
                "message": e.message,
                "details": e.details
            }
        )
    
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬
        logger.error(
            "unexpected_error_occurred",
            error=str(e),
            exc_info=True
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error_code": "LOG_9999",
                "message": "Internal server error",
                "details": {}
            }
        )
```

#### 5.2.2 Service Layer ì—ëŸ¬ ì²˜ë¦¬

```python
class LogSearchService:
    async def search_logs(self, search_params: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # Elasticsearch ê²€ìƒ‰
            response = await self.es_client.search(...)
            return self.parse_search_response(response)
            
        except elasticsearch.exceptions.ConnectionError as e:
            logger.error(
                "elasticsearch_connection_failed",
                error=str(e),
                exc_info=True
            )
            raise ServiceException(
                error_code=LoggingErrorCode.ELASTICSEARCH_CONNECTION_FAILED,
                message="Failed to connect to Elasticsearch",
                details={"error": str(e)}
            )
        
        except elasticsearch.exceptions.RequestError as e:
            logger.error(
                "invalid_search_query",
                query=search_params,
                error=str(e),
                exc_info=True
            )
            raise ServiceException(
                error_code=LoggingErrorCode.INVALID_SEARCH_QUERY,
                message="Invalid search query",
                details={"query": search_params, "error": str(e)}
            )
        
        except Exception as e:
            logger.error(
                "log_search_failed",
                error=str(e),
                exc_info=True
            )
            raise ServiceException(
                error_code=LoggingErrorCode.LOG_SEARCH_FAILED,
                message="Log search failed",
                details={"error": str(e)}
            )
```

### 5.3 ì¬ì‹œë„ ì „ëµ

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
import elasticsearch

class LogSearchService:
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(elasticsearch.exceptions.ConnectionError),
        reraise=True
    )
    async def search_logs_with_retry(
        self,
        search_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë¡œê·¸ ê²€ìƒ‰"""
        logger.info(
            "attempting_log_search",
            search_params=search_params
        )
        
        try:
            response = await self.es_client.search(...)
            
            logger.info(
                "log_search_succeeded",
                result_count=response['hits']['total']['value']
            )
            
            return self.parse_search_response(response)
            
        except elasticsearch.exceptions.ConnectionError as e:
            logger.warning(
                "elasticsearch_connection_failed_retrying",
                error=str(e)
            )
            raise  # ì¬ì‹œë„
        
        except Exception as e:
            logger.error(
                "log_search_failed_no_retry",
                error=str(e),
                exc_info=True
            )
            raise ServiceException(
                error_code=LoggingErrorCode.LOG_SEARCH_FAILED,
                message="Log search failed",
                details={"error": str(e)}
            )
```

### 5.4 Circuit Breaker íŒ¨í„´

```python
from circuitbreaker import circuit
from aegis_shared.logging import get_logger

logger = get_logger(__name__)

class ElasticsearchClient:
    @circuit(
        failure_threshold=5,
        recovery_timeout=60,
        expected_exception=elasticsearch.exceptions.ConnectionError
    )
    async def search(self, index: str, body: Dict[str, Any]) -> Dict[str, Any]:
        """Circuit Breakerê°€ ì ìš©ëœ Elasticsearch ê²€ìƒ‰"""
        try:
            response = await self.es_client.search(
                index=index,
                body=body
            )
            
            logger.debug(
                "elasticsearch_search_succeeded",
                index=index,
                took=response.get('took')
            )
            
            return response
            
        except elasticsearch.exceptions.ConnectionError as e:
            logger.error(
                "elasticsearch_connection_error",
                error=str(e),
                exc_info=True
            )
            # Circuit Breakerê°€ ì´ ì˜ˆì™¸ë¥¼ ê°ì§€í•˜ê³  íšŒë¡œë¥¼ ì—´ìŒ
            raise
        
        except Exception as e:
            logger.error(
                "elasticsearch_search_error",
                error=str(e),
                exc_info=True
            )
            raise
```

### 5.5 ì—ëŸ¬ ë¡œê¹… ë° ì•Œë¦¼

```python
class AlertService:
    async def handle_critical_error(
        self,
        error: Exception,
        context: Dict[str, Any]
    ):
        """ì¹˜ëª…ì  ì—ëŸ¬ ì²˜ë¦¬ ë° ì•Œë¦¼"""
        
        # ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡
        logger.critical(
            "critical_error_occurred",
            error_type=type(error).__name__,
            error_message=str(error),
            context=context,
            exc_info=True
        )
        
        # Slack ì•Œë¦¼ ë°œì†¡
        await self.send_slack_alert(
            channel="#critical-alerts",
            message=f"ğŸš¨ Critical Error in Logging-Audit Service\n"
                   f"Error: {type(error).__name__}\n"
                   f"Message: {str(error)}\n"
                   f"Context: {context}",
            severity="critical"
        )
        
        # PagerDuty ì•Œë¦¼ (ìš´ì˜íŒ€ í˜¸ì¶œ)
        await self.send_pagerduty_alert(
            severity="critical",
            summary=f"Critical error in Logging-Audit Service: {type(error).__name__}",
            details=context
        )
```

---



## 6. Production Considerations

### 6.1 í™•ì¥ì„± (Scalability)

#### 6.1.1 Horizontal Pod Autoscaler (HPA)

```yaml
# kubernetes/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: logging-audit-service-hpa
  namespace: aegis
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: logging-audit-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 4
        periodSeconds: 30
      selectPolicy: Max
```

#### 6.1.2 Elasticsearch í´ëŸ¬ìŠ¤í„° í™•ì¥

```yaml
# elasticsearch/elasticsearch-cluster.yaml
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: aegis-logs
  namespace: aegis
spec:
  version: 8.11.0
  nodeSets:
  # Master ë…¸ë“œ (í´ëŸ¬ìŠ¤í„° ê´€ë¦¬)
  - name: master
    count: 3
    config:
      node.roles: ["master"]
      xpack.security.enabled: true
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 4Gi
              cpu: 2
            limits:
              memory: 4Gi
              cpu: 2
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
        storageClassName: fast-ssd
  
  # Data ë…¸ë“œ (ë°ì´í„° ì €ì¥ ë° ê²€ìƒ‰)
  - name: data
    count: 5
    config:
      node.roles: ["data", "ingest"]
      xpack.security.enabled: true
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 16Gi
              cpu: 4
            limits:
              memory: 16Gi
              cpu: 4
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 500Gi
        storageClassName: fast-ssd
  
  # Coordinating ë…¸ë“œ (ê²€ìƒ‰ ìš”ì²­ ì¡°ì •)
  - name: coordinating
    count: 3
    config:
      node.roles: []
      xpack.security.enabled: true
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 8Gi
              cpu: 2
            limits:
              memory: 8Gi
              cpu: 2
```

### 6.2 ì¥ì•  ë³µêµ¬ (Fault Tolerance)

#### 6.2.1 Circuit Breaker êµ¬í˜„

```python
from circuitbreaker import circuit
from typing import Dict, Any, Optional
import asyncio

class ResilientElasticsearchClient:
    """ì¥ì•  ë³µêµ¬ ê¸°ëŠ¥ì´ ìˆëŠ” Elasticsearch í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(
        self,
        primary_client: AsyncElasticsearch,
        fallback_client: Optional[AsyncElasticsearch] = None
    ):
        self.primary_client = primary_client
        self.fallback_client = fallback_client
        self.circuit_open = False
    
    @circuit(
        failure_threshold=5,
        recovery_timeout=60,
        expected_exception=elasticsearch.exceptions.ConnectionError
    )
    async def search(
        self,
        index: str,
        body: Dict[str, Any],
        timeout: int = 30
    ) -> Dict[str, Any]:
        """Circuit Breakerê°€ ì ìš©ëœ ê²€ìƒ‰"""
        try:
            response = await asyncio.wait_for(
                self.primary_client.search(index=index, body=body),
                timeout=timeout
            )
            
            logger.debug(
                "elasticsearch_search_succeeded",
                index=index,
                took=response.get('took')
            )
            
            return response
            
        except asyncio.TimeoutError:
            logger.warning(
                "elasticsearch_search_timeout",
                index=index,
                timeout=timeout
            )
            
            # Fallback í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
            if self.fallback_client:
                return await self.fallback_with_cache(index, body)
            
            raise ServiceException(
                error_code=LoggingErrorCode.ELASTICSEARCH_TIMEOUT,
                message="Elasticsearch search timeout"
            )
        
        except elasticsearch.exceptions.ConnectionError as e:
            logger.error(
                "elasticsearch_connection_failed",
                error=str(e),
                exc_info=True
            )
            
            # Fallback í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
            if self.fallback_client:
                return await self.fallback_with_cache(index, body)
            
            raise
    
    async def fallback_with_cache(
        self,
        index: str,
        body: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ìºì‹œë¥¼ ì‚¬ìš©í•œ Fallback"""
        logger.info(
            "using_fallback_with_cache",
            index=index
        )
        
        # Redis ìºì‹œì—ì„œ ìµœê·¼ ê²°ê³¼ ì¡°íšŒ
        cache_key = f"search_cache:{index}:{hash(str(body))}"
        cached_result = await self.redis_client.get(cache_key)
        
        if cached_result:
            logger.info("returning_cached_result")
            return json.loads(cached_result)
        
        # Fallback í´ë¼ì´ì–¸íŠ¸ ì‹œë„
        if self.fallback_client:
            try:
                response = await self.fallback_client.search(
                    index=index,
                    body=body
                )
                return response
            except Exception as e:
                logger.error(
                    "fallback_client_failed",
                    error=str(e)
                )
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        raise ServiceException(
            error_code=LoggingErrorCode.ALL_ELASTICSEARCH_FAILED,
            message="All Elasticsearch clusters failed"
        )
```

#### 6.2.2 ë°ì´í„° ë³µì œ ë° ë°±ì—…

```yaml
# elasticsearch/snapshot-policy.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-snapshot-policy
  namespace: aegis
data:
  snapshot-policy.json: |
    {
      "schedule": "0 2 * * *",
      "name": "<aegis-logs-{now/d}>",
      "repository": "s3_backup",
      "config": {
        "indices": ["aegis-logs-*", "aegis-audit-*"],
        "ignore_unavailable": false,
        "include_global_state": false,
        "partial": false
      },
      "retention": {
        "expire_after": "30d",
        "min_count": 7,
        "max_count": 90
      }
    }
```

```python
# ë°±ì—… ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
class BackupVerificationService:
    """ë°±ì—… ê²€ì¦ ì„œë¹„ìŠ¤"""
    
    async def verify_daily_backup(self, snapshot_name: str):
        """ì¼ì¼ ë°±ì—… ê²€ì¦"""
        try:
            # ìŠ¤ëƒ…ìƒ· ìƒíƒœ í™•ì¸
            snapshot_info = await self.es_client.snapshot.get(
                repository='s3_backup',
                snapshot=snapshot_name
            )
            
            if snapshot_info['snapshots'][0]['state'] != 'SUCCESS':
                logger.error(
                    "backup_verification_failed",
                    snapshot_name=snapshot_name,
                    state=snapshot_info['snapshots'][0]['state']
                )
                
                # ì•Œë¦¼ ë°œì†¡
                await self.alert_service.send_alert(
                    severity="critical",
                    message=f"Backup verification failed: {snapshot_name}"
                )
                
                return False
            
            logger.info(
                "backup_verification_succeeded",
                snapshot_name=snapshot_name,
                indices_count=len(snapshot_info['snapshots'][0]['indices'])
            )
            
            return True
            
        except Exception as e:
            logger.error(
                "backup_verification_error",
                snapshot_name=snapshot_name,
                error=str(e),
                exc_info=True
            )
            return False
```

### 6.3 ìºì‹± (Caching)

#### 6.3.1 ë‹¤ì¸µ ìºì‹± ì „ëµ

```python
from aegis_shared.cache import cache_result, RedisClient
from typing import Dict, Any, Optional
import hashlib

class CachedLogSearchService:
    """ìºì‹±ì´ ì ìš©ëœ ë¡œê·¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    def __init__(
        self,
        elasticsearch_client: AsyncElasticsearch,
        redis_client: RedisClient
    ):
        self.es_client = elasticsearch_client
        self.redis_client = redis_client
    
    async def search_logs(
        self,
        search_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """3ë‹¨ê³„ ìºì‹± ì „ëµ"""
        
        # 1ë‹¨ê³„: ë©”ëª¨ë¦¬ ìºì‹œ (ìµœê·¼ ê²€ìƒ‰ ê²°ê³¼)
        cache_key = self.generate_cache_key(search_params)
        
        # 2ë‹¨ê³„: Redis ìºì‹œ (ê³µìœ  ìºì‹œ)
        cached_result = await self.get_from_redis_cache(cache_key)
        if cached_result:
            logger.debug(
                "cache_hit_redis",
                cache_key=cache_key
            )
            return cached_result
        
        # 3ë‹¨ê³„: Elasticsearch ê²€ìƒ‰
        result = await self.search_from_elasticsearch(search_params)
        
        # ê²°ê³¼ ìºì‹± (TTL: 5ë¶„)
        await self.cache_to_redis(cache_key, result, ttl=300)
        
        return result
    
    def generate_cache_key(self, search_params: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¥¼ ì •ê·œí™”í•˜ì—¬ í•´ì‹œ ìƒì„±
        normalized_params = {
            'query': search_params.get('query', ''),
            'service': search_params.get('service', ''),
            'level': search_params.get('level', ''),
            'start_time': search_params['start_time'].isoformat(),
            'end_time': search_params['end_time'].isoformat(),
            'page': search_params.get('page', 1),
            'size': search_params.get('size', 20)
        }
        
        params_str = json.dumps(normalized_params, sort_keys=True)
        params_hash = hashlib.md5(params_str.encode()).hexdigest()
        
        return f"log_search:{params_hash}"
    
    async def get_from_redis_cache(
        self,
        cache_key: str
    ) -> Optional[Dict[str, Any]]:
        """Redis ìºì‹œì—ì„œ ì¡°íšŒ"""
        try:
            cached_data = await self.redis_client.get(cache_key)
            if cached_data:
                return json.loads(cached_data)
            return None
        except Exception as e:
            logger.warning(
                "redis_cache_get_failed",
                cache_key=cache_key,
                error=str(e)
            )
            return None
    
    async def cache_to_redis(
        self,
        cache_key: str,
        data: Dict[str, Any],
        ttl: int = 300
    ):
        """Redis ìºì‹œì— ì €ì¥"""
        try:
            await self.redis_client.setex(
                cache_key,
                ttl,
                json.dumps(data)
            )
            
            logger.debug(
                "cached_to_redis",
                cache_key=cache_key,
                ttl=ttl
            )
        except Exception as e:
            logger.warning(
                "redis_cache_set_failed",
                cache_key=cache_key,
                error=str(e)
            )
    
    @cache_result(ttl=3600, key_prefix="frequent_searches")
    async def get_frequent_searches(self, user_id: str) -> List[Dict[str, Any]]:
        """ìì£¼ ì‚¬ìš©í•˜ëŠ” ê²€ìƒ‰ ì¡°íšŒ (1ì‹œê°„ ìºì‹±)"""
        # PostgreSQLì—ì„œ ì¡°íšŒ
        return await self.search_history_repo.get_frequent_searches(user_id)
```

#### 6.3.2 ìºì‹œ ë¬´íš¨í™” ì „ëµ

```python
class CacheInvalidationService:
    """ìºì‹œ ë¬´íš¨í™” ì„œë¹„ìŠ¤"""
    
    async def invalidate_log_caches(
        self,
        service: Optional[str] = None,
        pattern: Optional[str] = None
    ):
        """ë¡œê·¸ ìºì‹œ ë¬´íš¨í™”"""
        try:
            if pattern:
                # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìºì‹œ ì‚­ì œ
                keys = await self.redis_client.keys(f"log_search:*{pattern}*")
                if keys:
                    await self.redis_client.delete(*keys)
                    logger.info(
                        "cache_invalidated_by_pattern",
                        pattern=pattern,
                        keys_deleted=len(keys)
                    )
            
            elif service:
                # íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ìºì‹œ ì‚­ì œ
                keys = await self.redis_client.keys(f"log_search:*{service}*")
                if keys:
                    await self.redis_client.delete(*keys)
                    logger.info(
                        "cache_invalidated_by_service",
                        service=service,
                        keys_deleted=len(keys)
                    )
            
            else:
                # ëª¨ë“  ë¡œê·¸ ê²€ìƒ‰ ìºì‹œ ì‚­ì œ
                keys = await self.redis_client.keys("log_search:*")
                if keys:
                    await self.redis_client.delete(*keys)
                    logger.info(
                        "all_log_caches_invalidated",
                        keys_deleted=len(keys)
                    )
        
        except Exception as e:
            logger.error(
                "cache_invalidation_failed",
                error=str(e),
                exc_info=True
            )
```

### 6.4 ëª¨ë‹ˆí„°ë§ (Monitoring)

#### 6.4.1 Prometheus ë©”íŠ¸ë¦­

```python
from prometheus_client import Counter, Histogram, Gauge
from aegis_shared.monitoring import track_metrics

# ë¡œê·¸ ê²€ìƒ‰ ë©”íŠ¸ë¦­
log_search_requests_total = Counter(
    'logging_audit_log_search_requests_total',
    'Total number of log search requests',
    ['service', 'status']
)

log_search_duration_seconds = Histogram(
    'logging_audit_log_search_duration_seconds',
    'Log search duration in seconds',
    ['service'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

# Elasticsearch ë©”íŠ¸ë¦­
elasticsearch_query_duration_seconds = Histogram(
    'logging_audit_elasticsearch_query_duration_seconds',
    'Elasticsearch query duration in seconds',
    buckets=[0.05, 0.1, 0.5, 1.0, 2.0, 5.0]
)

elasticsearch_connection_errors_total = Counter(
    'logging_audit_elasticsearch_connection_errors_total',
    'Total number of Elasticsearch connection errors'
)

# ê°ì‚¬ ë¡œê·¸ ë©”íŠ¸ë¦­
audit_logs_created_total = Counter(
    'logging_audit_audit_logs_created_total',
    'Total number of audit logs created',
    ['action_type', 'service']
)

# ìºì‹œ ë©”íŠ¸ë¦­
cache_hits_total = Counter(
    'logging_audit_cache_hits_total',
    'Total number of cache hits',
    ['cache_type']
)

cache_misses_total = Counter(
    'logging_audit_cache_misses_total',
    'Total number of cache misses',
    ['cache_type']
)

# ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
active_log_searches = Gauge(
    'logging_audit_active_log_searches',
    'Number of active log searches'
)

elasticsearch_cluster_health = Gauge(
    'logging_audit_elasticsearch_cluster_health',
    'Elasticsearch cluster health (0=red, 1=yellow, 2=green)'
)
```

#### 6.4.2 ë©”íŠ¸ë¦­ ìˆ˜ì§‘ êµ¬í˜„

```python
class MetricsCollector:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    @track_metrics("log_search")
    async def search_logs_with_metrics(
        self,
        search_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì´ í¬í•¨ëœ ë¡œê·¸ ê²€ìƒ‰"""
        
        service = search_params.get('service', 'all')
        
        # í™œì„± ê²€ìƒ‰ ìˆ˜ ì¦ê°€
        active_log_searches.inc()
        
        start_time = time.time()
        
        try:
            result = await self.search_logs(search_params)
            
            # ì„±ê³µ ë©”íŠ¸ë¦­
            log_search_requests_total.labels(
                service=service,
                status='success'
            ).inc()
            
            # ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤
            if result.get('from_cache'):
                cache_hits_total.labels(cache_type='redis').inc()
            else:
                cache_misses_total.labels(cache_type='redis').inc()
            
            return result
            
        except Exception as e:
            # ì‹¤íŒ¨ ë©”íŠ¸ë¦­
            log_search_requests_total.labels(
                service=service,
                status='error'
            ).inc()
            
            raise
        
        finally:
            # ê²€ìƒ‰ ì‹œê°„ ê¸°ë¡
            duration = time.time() - start_time
            log_search_duration_seconds.labels(service=service).observe(duration)
            
            # í™œì„± ê²€ìƒ‰ ìˆ˜ ê°ì†Œ
            active_log_searches.dec()
    
    async def collect_elasticsearch_health(self):
        """Elasticsearch í´ëŸ¬ìŠ¤í„° ìƒíƒœ ìˆ˜ì§‘"""
        try:
            health = await self.es_client.cluster.health()
            
            health_value = {
                'red': 0,
                'yellow': 1,
                'green': 2
            }.get(health['status'], 0)
            
            elasticsearch_cluster_health.set(health_value)
            
            logger.debug(
                "elasticsearch_health_collected",
                status=health['status'],
                active_shards=health['active_shards']
            )
            
        except Exception as e:
            logger.error(
                "elasticsearch_health_collection_failed",
                error=str(e)
            )
            elasticsearch_cluster_health.set(0)  # red
```

### 6.5 ë³´ì•ˆ (Security)

#### 6.5.1 ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹

```python
import re
from typing import Dict, Any

class SensitiveDataMasker:
    """ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹"""
    
    # ë¯¼ê° ì •ë³´ íŒ¨í„´
    PATTERNS = {
        'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
        'phone': re.compile(r'\b\d{3}[-.]?\d{3,4}[-.]?\d{4}\b'),
        'ssn': re.compile(r'\b\d{6}-\d{7}\b'),  # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸
        'credit_card': re.compile(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'),
        'password': re.compile(r'(password|pwd|passwd)[\s:=]+\S+', re.IGNORECASE),
        'token': re.compile(r'(token|api_key|secret)[\s:=]+\S+', re.IGNORECASE)
    }
    
    def mask_log_entry(self, log_entry: Dict[str, Any]) -> Dict[str, Any]:
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ì˜ ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹"""
        masked_entry = log_entry.copy()
        
        # ë©”ì‹œì§€ í•„ë“œ ë§ˆìŠ¤í‚¹
        if 'message' in masked_entry:
            masked_entry['message'] = self.mask_text(masked_entry['message'])
        
        # ë°ì´í„° í•„ë“œ ë§ˆìŠ¤í‚¹
        if 'data' in masked_entry and isinstance(masked_entry['data'], dict):
            masked_entry['data'] = self.mask_dict(masked_entry['data'])
        
        # íŠ¹ì • í•„ë“œ ì™„ì „ ì œê±°
        sensitive_fields = ['password', 'token', 'api_key', 'secret']
        for field in sensitive_fields:
            if field in masked_entry:
                masked_entry[field] = '***REDACTED***'
        
        return masked_entry
    
    def mask_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹"""
        masked_text = text
        
        for pattern_name, pattern in self.PATTERNS.items():
            if pattern_name == 'email':
                masked_text = pattern.sub(
                    lambda m: self.mask_email(m.group(0)),
                    masked_text
                )
            elif pattern_name == 'phone':
                masked_text = pattern.sub('***-****-****', masked_text)
            elif pattern_name == 'ssn':
                masked_text = pattern.sub('******-*******', masked_text)
            elif pattern_name == 'credit_card':
                masked_text = pattern.sub('****-****-****-****', masked_text)
            else:
                masked_text = pattern.sub('***MASKED***', masked_text)
        
        return masked_text
    
    def mask_email(self, email: str) -> str:
        """ì´ë©”ì¼ ë¶€ë¶„ ë§ˆìŠ¤í‚¹"""
        parts = email.split('@')
        if len(parts) == 2:
            username = parts[0]
            domain = parts[1]
            
            if len(username) > 2:
                masked_username = username[0] + '*' * (len(username) - 2) + username[-1]
            else:
                masked_username = '*' * len(username)
            
            return f"{masked_username}@{domain}"
        
        return '***@***.***'
    
    def mask_dict(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ì˜ ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹"""
        masked_data = {}
        
        for key, value in data.items():
            if isinstance(value, str):
                masked_data[key] = self.mask_text(value)
            elif isinstance(value, dict):
                masked_data[key] = self.mask_dict(value)
            elif isinstance(value, list):
                masked_data[key] = [
                    self.mask_text(item) if isinstance(item, str) else item
                    for item in value
                ]
            else:
                masked_data[key] = value
        
        return masked_data
```

#### 6.5.2 ì ‘ê·¼ ì œì–´ (RBAC)

```python
from aegis_shared.auth import require_role, get_current_user
from typing import List

class LogAccessControl:
    """ë¡œê·¸ ì ‘ê·¼ ì œì–´"""
    
    # ì—­í• ë³„ ì ‘ê·¼ ê¶Œí•œ
    ROLE_PERMISSIONS = {
        'admin': ['read_all_logs', 'read_audit_logs', 'export_logs', 'delete_logs'],
        'moderator': ['read_all_logs', 'read_audit_logs', 'export_logs'],
        'analyst': ['read_all_logs', 'export_logs'],
        'developer': ['read_service_logs'],  # ìì‹ ì˜ ì„œë¹„ìŠ¤ ë¡œê·¸ë§Œ
        'auditor': ['read_audit_logs', 'export_audit_logs']
    }
    
    def check_log_access(
        self,
        user_role: str,
        action: str,
        service: str = None
    ) -> bool:
        """ë¡œê·¸ ì ‘ê·¼ ê¶Œí•œ í™•ì¸"""
        
        # ì—­í• ë³„ ê¶Œí•œ í™•ì¸
        permissions = self.ROLE_PERMISSIONS.get(user_role, [])
        
        if action not in permissions:
            logger.warning(
                "log_access_denied",
                user_role=user_role,
                action=action,
                service=service
            )
            return False
        
        # ê°œë°œìëŠ” ìì‹ ì˜ ì„œë¹„ìŠ¤ ë¡œê·¸ë§Œ ì ‘ê·¼ ê°€ëŠ¥
        if user_role == 'developer' and service:
            # ì‚¬ìš©ìì˜ ì„œë¹„ìŠ¤ ëª©ë¡ í™•ì¸ (ì‹¤ì œë¡œëŠ” DBì—ì„œ ì¡°íšŒ)
            user_services = self.get_user_services(user_role)
            if service not in user_services:
                logger.warning(
                    "service_access_denied",
                    user_role=user_role,
                    service=service
                )
                return False
        
        return True

@router.get("/logs/search")
@require_role(["admin", "moderator", "analyst", "developer"])
async def search_logs(
    service: Optional[str] = None,
    current_user: dict = Depends(get_current_user),
    access_control: LogAccessControl = Depends(get_access_control)
):
    """ì ‘ê·¼ ì œì–´ê°€ ì ìš©ëœ ë¡œê·¸ ê²€ìƒ‰"""
    
    # ì ‘ê·¼ ê¶Œí•œ í™•ì¸
    if not access_control.check_log_access(
        user_role=current_user['role'],
        action='read_all_logs',
        service=service
    ):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied to logs"
        )
    
    # ë¡œê·¸ ê²€ìƒ‰ ìˆ˜í–‰
    result = await log_service.search_logs(...)
    
    return result
```

---



## 7. Data Models

### 7.1 PostgreSQL ë°ì´í„° ëª¨ë¸

#### 7.1.1 Audit Log Model

```python
from sqlalchemy import Column, String, DateTime, JSON, Integer, Index, Text
from sqlalchemy.dialects.postgresql import UUID
from aegis_shared.database import Base
import uuid
from datetime import datetime

class AuditLog(Base):
    """ê°ì‚¬ ë¡œê·¸ ëª¨ë¸"""
    
    __tablename__ = "audit_logs"
    
    # ê¸°ë³¸ í•„ë“œ
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    timestamp = Column(DateTime, nullable=False, default=datetime.utcnow, index=True)
    
    # ì‚¬ìš©ì ì •ë³´
    user_id = Column(String(255), nullable=False, index=True)
    user_role = Column(String(50), nullable=True)
    session_id = Column(String(255), nullable=True)
    
    # ì•¡ì…˜ ì •ë³´
    service = Column(String(100), nullable=False, index=True)
    action_type = Column(String(100), nullable=False, index=True)
    action = Column(String(255), nullable=False)
    result = Column(String(50), nullable=False, default='success')  # success, failure, partial
    
    # ë¦¬ì†ŒìŠ¤ ì •ë³´
    resource_type = Column(String(100), nullable=True, index=True)
    resource_id = Column(String(255), nullable=True, index=True)
    
    # ë³€ê²½ ì •ë³´
    changes = Column(JSON, nullable=True)  # Before/After ê°’
    details = Column(JSON, nullable=True)  # ì¶”ê°€ ìƒì„¸ ì •ë³´
    
    # ë„¤íŠ¸ì›Œí¬ ì •ë³´
    ip_address = Column(String(45), nullable=True)  # IPv6 ì§€ì›
    user_agent = Column(Text, nullable=True)
    
    # ë©”íƒ€ë°ì´í„°
    request_id = Column(String(255), nullable=True, index=True)
    correlation_id = Column(String(255), nullable=True, index=True)
    
    # ì¸ë±ìŠ¤
    __table_args__ = (
        Index('idx_audit_user_timestamp', 'user_id', 'timestamp'),
        Index('idx_audit_service_action', 'service', 'action_type'),
        Index('idx_audit_resource', 'resource_type', 'resource_id'),
        Index('idx_audit_timestamp_desc', timestamp.desc()),
    )
    
    def __repr__(self):
        return f"<AuditLog(id={self.id}, user_id={self.user_id}, action={self.action})>"
    
    def to_dict(self) -> dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'id': str(self.id),
            'timestamp': self.timestamp.isoformat(),
            'user_id': self.user_id,
            'user_role': self.user_role,
            'session_id': self.session_id,
            'service': self.service,
            'action_type': self.action_type,
            'action': self.action,
            'result': self.result,
            'resource_type': self.resource_type,
            'resource_id': self.resource_id,
            'changes': self.changes,
            'details': self.details,
            'ip_address': self.ip_address,
            'user_agent': self.user_agent,
            'request_id': self.request_id,
            'correlation_id': self.correlation_id
        }
```

#### 7.1.2 Alert Model

```python
class Alert(Base):
    """ì•Œë¦¼ ëª¨ë¸"""
    
    __tablename__ = "alerts"
    
    # ê¸°ë³¸ í•„ë“œ
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow, index=True)
    updated_at = Column(DateTime, nullable=False, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # ì•Œë¦¼ ì •ë³´
    alert_type = Column(String(100), nullable=False, index=True)  # error_spike, security_event, etc.
    severity = Column(String(20), nullable=False, index=True)  # critical, warning, info
    title = Column(String(500), nullable=False)
    message = Column(Text, nullable=False)
    
    # ì†ŒìŠ¤ ì •ë³´
    service = Column(String(100), nullable=False, index=True)
    source_type = Column(String(100), nullable=True)  # log, metric, audit
    source_id = Column(String(255), nullable=True)
    
    # ìƒíƒœ ì •ë³´
    status = Column(String(50), nullable=False, default='open', index=True)  # open, acknowledged, resolved
    acknowledged_by = Column(String(255), nullable=True)
    acknowledged_at = Column(DateTime, nullable=True)
    resolved_by = Column(String(255), nullable=True)
    resolved_at = Column(DateTime, nullable=True)
    
    # ì•Œë¦¼ ë°œì†¡ ì •ë³´
    notification_channels = Column(JSON, nullable=True)  # ['slack', 'email', 'pagerduty']
    notification_status = Column(JSON, nullable=True)  # ì±„ë„ë³„ ë°œì†¡ ìƒíƒœ
    
    # ì¶”ê°€ ì •ë³´
    metadata = Column(JSON, nullable=True)
    tags = Column(JSON, nullable=True)
    
    # ì¸ë±ìŠ¤
    __table_args__ = (
        Index('idx_alert_severity_status', 'severity', 'status'),
        Index('idx_alert_service_type', 'service', 'alert_type'),
        Index('idx_alert_created_desc', created_at.desc()),
    )
    
    def __repr__(self):
        return f"<Alert(id={self.id}, type={self.alert_type}, severity={self.severity})>"
```

#### 7.1.3 Search History Model

```python
class SearchHistory(Base):
    """ê²€ìƒ‰ ì´ë ¥ ëª¨ë¸"""
    
    __tablename__ = "search_history"
    
    # ê¸°ë³¸ í•„ë“œ
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow, index=True)
    
    # ì‚¬ìš©ì ì •ë³´
    user_id = Column(String(255), nullable=False, index=True)
    
    # ê²€ìƒ‰ ì •ë³´
    search_query = Column(Text, nullable=False)
    search_params = Column(JSON, nullable=False)  # ì „ì²´ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
    
    # ê²°ê³¼ ì •ë³´
    result_count = Column(Integer, nullable=True)
    execution_time_ms = Column(Integer, nullable=True)
    
    # ë©”íƒ€ë°ì´í„°
    saved = Column(Boolean, default=False, index=True)  # ì €ì¥ëœ ê²€ìƒ‰
    search_name = Column(String(255), nullable=True)  # ì €ì¥ëœ ê²€ìƒ‰ ì´ë¦„
    
    # ì¸ë±ìŠ¤
    __table_args__ = (
        Index('idx_search_user_created', 'user_id', 'created_at'),
        Index('idx_search_saved', 'user_id', 'saved'),
    )
```

### 7.2 Elasticsearch ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆ

#### 7.2.1 Application Logs Index

```json
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 2,
    "index": {
      "refresh_interval": "5s",
      "max_result_window": 10000
    },
    "analysis": {
      "analyzer": {
        "log_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["lowercase", "stop"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "timestamp": {
        "type": "date",
        "format": "strict_date_optional_time||epoch_millis"
      },
      "service": {
        "type": "keyword"
      },
      "level": {
        "type": "keyword"
      },
      "event_type": {
        "type": "keyword"
      },
      "message": {
        "type": "text",
        "analyzer": "log_analyzer",
        "fields": {
          "keyword": {
            "type": "keyword",
            "ignore_above": 256
          }
        }
      },
      "request_id": {
        "type": "keyword"
      },
      "user_id": {
        "type": "keyword"
      },
      "session_id": {
        "type": "keyword"
      },
      "ip_address": {
        "type": "ip"
      },
      "user_agent": {
        "type": "text",
        "fields": {
          "keyword": {
            "type": "keyword",
            "ignore_above": 512
          }
        }
      },
      "data": {
        "type": "object",
        "enabled": true
      },
      "error": {
        "type": "object",
        "properties": {
          "code": {
            "type": "keyword"
          },
          "message": {
            "type": "text"
          },
          "stack_trace": {
            "type": "text"
          }
        }
      },
      "performance": {
        "type": "object",
        "properties": {
          "duration_ms": {
            "type": "long"
          },
          "cpu_usage": {
            "type": "float"
          },
          "memory_usage": {
            "type": "long"
          }
        }
      },
      "geo": {
        "type": "object",
        "properties": {
          "country": {
            "type": "keyword"
          },
          "city": {
            "type": "keyword"
          },
          "location": {
            "type": "geo_point"
          }
        }
      },
      "environment": {
        "type": "keyword"
      },
      "version": {
        "type": "keyword"
      }
    }
  }
}
```

#### 7.2.2 Audit Logs Index

```json
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 2,
    "index": {
      "refresh_interval": "10s"
    }
  },
  "mappings": {
    "properties": {
      "timestamp": {
        "type": "date",
        "format": "strict_date_optional_time||epoch_millis"
      },
      "user_id": {
        "type": "keyword"
      },
      "user_role": {
        "type": "keyword"
      },
      "session_id": {
        "type": "keyword"
      },
      "service": {
        "type": "keyword"
      },
      "action_type": {
        "type": "keyword"
      },
      "action": {
        "type": "keyword"
      },
      "result": {
        "type": "keyword"
      },
      "resource_type": {
        "type": "keyword"
      },
      "resource_id": {
        "type": "keyword"
      },
      "changes": {
        "type": "object",
        "properties": {
          "before": {
            "type": "object",
            "enabled": true
          },
          "after": {
            "type": "object",
            "enabled": true
          }
        }
      },
      "details": {
        "type": "object",
        "enabled": true
      },
      "ip_address": {
        "type": "ip"
      },
      "user_agent": {
        "type": "text",
        "fields": {
          "keyword": {
            "type": "keyword",
            "ignore_above": 512
          }
        }
      },
      "request_id": {
        "type": "keyword"
      },
      "correlation_id": {
        "type": "keyword"
      }
    }
  }
}
```

### 7.3 Pydantic ëª¨ë¸ (API)

#### 7.3.1 Request Models

```python
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any
from datetime import datetime
from enum import Enum

class LogLevel(str, Enum):
    """ë¡œê·¸ ë ˆë²¨"""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

class LogSearchRequest(BaseModel):
    """ë¡œê·¸ ê²€ìƒ‰ ìš”ì²­"""
    query: Optional[str] = Field(None, description="ê²€ìƒ‰ ì¿¼ë¦¬")
    service: Optional[str] = Field(None, description="ì„œë¹„ìŠ¤ í•„í„°")
    level: Optional[LogLevel] = Field(None, description="ë¡œê·¸ ë ˆë²¨ í•„í„°")
    start_time: Optional[datetime] = Field(None, description="ì‹œì‘ ì‹œê°„")
    end_time: Optional[datetime] = Field(None, description="ì¢…ë£Œ ì‹œê°„")
    page: int = Field(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸")
    size: int = Field(20, ge=1, le=1000, description="í˜ì´ì§€ í¬ê¸°")
    
    @validator('end_time')
    def validate_time_range(cls, v, values):
        """ì‹œê°„ ë²”ìœ„ ê²€ì¦"""
        if v and 'start_time' in values and values['start_time']:
            if v < values['start_time']:
                raise ValueError("end_time must be after start_time")
        return v
    
    class Config:
        schema_extra = {
            "example": {
                "query": "error",
                "service": "user-service",
                "level": "ERROR",
                "start_time": "2025-10-07T00:00:00Z",
                "end_time": "2025-10-07T23:59:59Z",
                "page": 1,
                "size": 20
            }
        }

class AuditTrailRequest(BaseModel):
    """ê°ì‚¬ ì¶”ì  ìš”ì²­"""
    user_id: str = Field(..., description="ì‚¬ìš©ì ID")
    start_date: Optional[datetime] = Field(None, description="ì‹œì‘ ë‚ ì§œ")
    end_date: Optional[datetime] = Field(None, description="ì¢…ë£Œ ë‚ ì§œ")
    action_type: Optional[str] = Field(None, description="ì•¡ì…˜ íƒ€ì… í•„í„°")
    page: int = Field(1, ge=1)
    size: int = Field(50, ge=1, le=500)

class ComplianceReportRequest(BaseModel):
    """ê·œì • ì¤€ìˆ˜ ë³´ê³ ì„œ ìš”ì²­"""
    report_type: str = Field(..., description="ë³´ê³ ì„œ íƒ€ì… (gdpr, privacy, security)")
    start_date: datetime = Field(..., description="ì‹œì‘ ë‚ ì§œ")
    end_date: datetime = Field(..., description="ì¢…ë£Œ ë‚ ì§œ")
    format: str = Field("json", description="ì¶œë ¥ í˜•ì‹ (json, pdf, csv)")
    
    @validator('report_type')
    def validate_report_type(cls, v):
        """ë³´ê³ ì„œ íƒ€ì… ê²€ì¦"""
        allowed_types = ['gdpr', 'privacy', 'security']
        if v not in allowed_types:
            raise ValueError(f"report_type must be one of {allowed_types}")
        return v
```

#### 7.3.2 Response Models

```python
class LogEntry(BaseModel):
    """ë¡œê·¸ ì—”íŠ¸ë¦¬"""
    id: str
    timestamp: datetime
    service: str
    level: str
    event_type: str
    message: str
    request_id: Optional[str] = None
    user_id: Optional[str] = None
    data: Optional[Dict[str, Any]] = None
    
    class Config:
        schema_extra = {
            "example": {
                "id": "log-123",
                "timestamp": "2025-10-07T10:00:00Z",
                "service": "user-service",
                "level": "INFO",
                "event_type": "user.login",
                "message": "User logged in successfully",
                "request_id": "req-456",
                "user_id": "user-789",
                "data": {
                    "ip_address": "192.168.1.1",
                    "user_agent": "Mozilla/5.0..."
                }
            }
        }

class PaginatedLogResponse(BaseModel):
    """í˜ì´ì§€ë„¤ì´ì…˜ëœ ë¡œê·¸ ì‘ë‹µ"""
    items: List[LogEntry]
    total: int
    page: int
    size: int
    total_pages: int
    
    class Config:
        schema_extra = {
            "example": {
                "items": [],
                "total": 1000,
                "page": 1,
                "size": 20,
                "total_pages": 50
            }
        }

class AuditLogEntry(BaseModel):
    """ê°ì‚¬ ë¡œê·¸ ì—”íŠ¸ë¦¬"""
    id: str
    timestamp: datetime
    user_id: str
    user_role: Optional[str] = None
    service: str
    action_type: str
    action: str
    result: str
    resource_type: Optional[str] = None
    resource_id: Optional[str] = None
    changes: Optional[Dict[str, Any]] = None
    details: Optional[Dict[str, Any]] = None
    ip_address: Optional[str] = None

class UserAuditTrailResponse(BaseModel):
    """ì‚¬ìš©ì ê°ì‚¬ ì¶”ì  ì‘ë‹µ"""
    user_id: str
    audit_entries: List[AuditLogEntry]
    total: int
    page: int
    size: int
    summary: Dict[str, Any]
    
    class Config:
        schema_extra = {
            "example": {
                "user_id": "user-123",
                "audit_entries": [],
                "total": 150,
                "page": 1,
                "size": 50,
                "summary": {
                    "total_actions": 150,
                    "action_types": [
                        {"key": "login", "doc_count": 50},
                        {"key": "update", "doc_count": 30}
                    ],
                    "services": [
                        {"key": "user-service", "doc_count": 80},
                        {"key": "policy-service", "doc_count": 70}
                    ]
                }
            }
        }

class ComplianceReportResponse(BaseModel):
    """ê·œì • ì¤€ìˆ˜ ë³´ê³ ì„œ ì‘ë‹µ"""
    report_id: str
    report_type: str
    generated_at: datetime
    data: Dict[str, Any]
    download_url: Optional[str] = None
```

### 7.4 ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ (Kafka)

```python
from aegis_shared.messaging import VersionedEvent
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, Dict, Any

class LogEventSchema(BaseModel):
    """ë¡œê·¸ ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ"""
    timestamp: datetime
    service: str
    level: str
    event_type: str
    message: str
    request_id: Optional[str] = None
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    data: Optional[Dict[str, Any]] = None

class AuditEventSchema(BaseModel):
    """ê°ì‚¬ ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ"""
    timestamp: datetime
    user_id: str
    user_role: Optional[str] = None
    service: str
    action_type: str
    action: str
    result: str
    resource_type: Optional[str] = None
    resource_id: Optional[str] = None
    changes: Optional[Dict[str, Any]] = None
    details: Optional[Dict[str, Any]] = None
    ip_address: Optional[str] = None
    user_agent: Optional[str] = None

# ë²„ì „ ê´€ë¦¬ëœ ì´ë²¤íŠ¸
LogEvent_v1 = VersionedEvent[LogEventSchema](
    event_type="log.created",
    version="1.0.0",
    schema=LogEventSchema
)

AuditEvent_v1 = VersionedEvent[AuditEventSchema](
    event_type="audit.created",
    version="1.0.0",
    schema=AuditEventSchema
)
```

---



## 8. Service Integration

### 8.1 ì„œë¹„ìŠ¤ ì˜ì¡´ì„± ë§¤íŠ¸ë¦­ìŠ¤

| ì˜ì¡´ ì„œë¹„ìŠ¤ | í†µì‹  ë°©ì‹ | ëª©ì  | í•„ìˆ˜/ì„ íƒ | íƒ€ì„ì•„ì›ƒ |
|------------|----------|------|----------|---------|
| User Service | API í˜¸ì¶œ | ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ | ì„ íƒ | 5s |
| All Services | Filebeat â†’ Logstash | ë¡œê·¸ ìˆ˜ì§‘ | í•„ìˆ˜ | N/A |
| Elasticsearch | ì§ì ‘ ì—°ê²° | ë¡œê·¸ ì €ì¥ ë° ê²€ìƒ‰ | í•„ìˆ˜ | 30s |
| PostgreSQL | ì§ì ‘ ì—°ê²° | ê°ì‚¬ ë©”íƒ€ë°ì´í„° ì €ì¥ | í•„ìˆ˜ | 10s |
| Redis | ì§ì ‘ ì—°ê²° | ìºì‹± | ì„ íƒ | 2s |
| Prometheus | ë©”íŠ¸ë¦­ ë…¸ì¶œ | ëª¨ë‹ˆí„°ë§ | í•„ìˆ˜ | N/A |

### 8.2 ë¡œê·¸ ìˆ˜ì§‘ (Filebeat â†’ Logstash â†’ Elasticsearch)

#### 8.2.1 Filebeat ì„¤ì •

```yaml
# filebeat.yml
filebeat.inputs:
# ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸
- type: log
  enabled: true
  paths:
    - /var/log/aegis/*/application.log
    - /var/log/aegis/*/application-*.log
  json.keys_under_root: true
  json.add_error_key: true
  json.message_key: message
  fields:
    log_type: application
    environment: ${ENVIRONMENT:production}
  fields_under_root: true
  multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'
  multiline.negate: true
  multiline.match: after

# ê°ì‚¬ ë¡œê·¸
- type: log
  enabled: true
  paths:
    - /var/log/aegis/*/audit.log
    - /var/log/aegis/*/audit-*.log
  json.keys_under_root: true
  json.add_error_key: true
  fields:
    log_type: audit
    retention_days: 2555  # 7ë…„ ë³´ê´€
  fields_under_root: true

# ì—ëŸ¬ ë¡œê·¸
- type: log
  enabled: true
  paths:
    - /var/log/aegis/*/error.log
    - /var/log/aegis/*/error-*.log
  json.keys_under_root: true
  json.add_error_key: true
  fields:
    log_type: error
    priority: high
  fields_under_root: true

# Kubernetes ì»¨í…Œì´ë„ˆ ë¡œê·¸
- type: container
  enabled: true
  paths:
    - /var/log/containers/*aegis*.log
  processors:
  - add_kubernetes_metadata:
      host: ${NODE_NAME}
      matchers:
      - logs_path:
          logs_path: "/var/log/containers/"

# Output to Logstash
output.logstash:
  hosts: ["logstash-service:5044"]
  loadbalance: true
  worker: 2
  compression_level: 3
  bulk_max_size: 2048
  timeout: 30s

# Processors
processors:
- add_host_metadata:
    when.not.contains.tags: forwarded
- add_cloud_metadata: ~
- add_docker_metadata: ~
- add_kubernetes_metadata:
    host: ${NODE_NAME}
    matchers:
    - logs_path:
        logs_path: "/var/log/containers/"

# Logging
logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

#### 8.2.2 Logstash íŒŒì´í”„ë¼ì¸

```ruby
# logstash/pipeline/aegis-logs.conf
input {
  beats {
    port => 5044
    codec => json
  }
}

filter {
  # JSON íŒŒì‹±
  if [message] =~ /^\{/ {
    json {
      source => "message"
      target => "parsed"
    }
    
    # íŒŒì‹±ëœ í•„ë“œë¥¼ ìµœìƒìœ„ë¡œ ì´ë™
    ruby {
      code => "
        if event.get('parsed')
          event.get('parsed').each { |k, v|
            event.set(k, v)
          }
          event.remove('parsed')
        end
      "
    }
  }
  
  # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±
  date {
    match => [ "timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ", "yyyy-MM-dd HH:mm:ss" ]
    target => "@timestamp"
  }
  
  # ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹
  mutate {
    gsub => [
      "message", "password[\"']?\s*[:=]\s*[\"']?[^\"'\s]+", "password=***MASKED***",
      "message", "token[\"']?\s*[:=]\s*[\"']?[^\"'\s]+", "token=***MASKED***",
      "message", "api_key[\"']?\s*[:=]\s*[\"']?[^\"'\s]+", "api_key=***MASKED***",
      "message", "secret[\"']?\s*[:=]\s*[\"']?[^\"'\s]+", "secret=***MASKED***"
    ]
  }
  
  # ì´ë©”ì¼ ë§ˆìŠ¤í‚¹
  ruby {
    code => "
      message = event.get('message')
      if message
        message = message.gsub(/\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/) { |email|
          parts = email.split('@')
          username = parts[0]
          domain = parts[1]
          masked_username = username[0] + '*' * (username.length - 2) + username[-1]
          masked_username + '@' + domain
        }
        event.set('message', message)
      end
    "
  }
  
  # ë¡œê·¸ ë ˆë²¨ë³„ ì‹¬ê°ë„ ê³„ì‚°
  if [level] == "CRITICAL" or [level] == "FATAL" {
    mutate { add_field => { "severity" => 5 } }
  } else if [level] == "ERROR" {
    mutate { add_field => { "severity" => 4 } }
  } else if [level] == "WARNING" or [level] == "WARN" {
    mutate { add_field => { "severity" => 3 } }
  } else if [level] == "INFO" {
    mutate { add_field => { "severity" => 2 } }
  } else {
    mutate { add_field => { "severity" => 1 } }
  }
  
  # ì§€ë¦¬ì  ì •ë³´ ì¶”ê°€
  if [ip_address] {
    geoip {
      source => "ip_address"
      target => "geo"
      fields => ["city_name", "country_name", "location", "region_name"]
    }
  }
  
  # ì‚¬ìš©ì ì—ì´ì „íŠ¸ íŒŒì‹±
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "ua"
    }
  }
  
  # ì„œë¹„ìŠ¤ ì´ë¦„ ì¶”ì¶œ (Kubernetes í™˜ê²½)
  if [kubernetes][container][name] {
    mutate {
      add_field => { "service" => "%{[kubernetes][container][name]}" }
    }
  }
  
  # í™˜ê²½ ì •ë³´ ì¶”ê°€
  if ![environment] {
    mutate {
      add_field => { "environment" => "production" }
    }
  }
}

output {
  # ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸
  if [log_type] == "application" {
    elasticsearch {
      hosts => ["elasticsearch-service:9200"]
      index => "aegis-logs-%{+YYYY.MM.dd}"
      template_name => "aegis-logs"
      template => "/usr/share/logstash/templates/aegis-logs.json"
      template_overwrite => true
      user => "${ELASTICSEARCH_USER}"
      password => "${ELASTICSEARCH_PASSWORD}"
    }
  }
  
  # ê°ì‚¬ ë¡œê·¸ (ë³„ë„ ì¸ë±ìŠ¤, ì¥ê¸° ë³´ê´€)
  if [log_type] == "audit" {
    elasticsearch {
      hosts => ["elasticsearch-service:9200"]
      index => "aegis-audit-%{+YYYY.MM}"
      template_name => "aegis-audit"
      template => "/usr/share/logstash/templates/aegis-audit.json"
      template_overwrite => true
      user => "${ELASTICSEARCH_USER}"
      password => "${ELASTICSEARCH_PASSWORD}"
    }
  }
  
  # ì—ëŸ¬ ë¡œê·¸ (ë³„ë„ ì¸ë±ìŠ¤, ë¹ ë¥¸ ê²€ìƒ‰)
  if [log_type] == "error" or [level] == "ERROR" or [level] == "CRITICAL" {
    elasticsearch {
      hosts => ["elasticsearch-service:9200"]
      index => "aegis-errors-%{+YYYY.MM.dd}"
      template_name => "aegis-errors"
      template => "/usr/share/logstash/templates/aegis-errors.json"
      template_overwrite => true
      user => "${ELASTICSEARCH_USER}"
      password => "${ELASTICSEARCH_PASSWORD}"
    }
  }
  
  # ê°œë°œ í™˜ê²½ì—ì„œëŠ” stdoutë„ ì¶œë ¥
  if [environment] == "development" {
    stdout {
      codec => rubydebug
    }
  }
}
```

### 8.3 API í˜¸ì¶œ (User Service)

```python
from typing import Optional, Dict, Any
import httpx
from aegis_shared.logging import get_logger
from aegis_shared.errors import ServiceException

logger = get_logger(__name__)

class UserServiceClient:
    """User Service API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, base_url: str, timeout: int = 5):
        self.base_url = base_url
        self.timeout = timeout
        self.client = httpx.AsyncClient(
            base_url=base_url,
            timeout=timeout,
            headers={"Content-Type": "application/json"}
        )
    
    async def get_user_info(
        self,
        user_id: str,
        auth_token: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ"""
        try:
            headers = {}
            if auth_token:
                headers["Authorization"] = f"Bearer {auth_token}"
            
            response = await self.client.get(
                f"/api/v1/users/{user_id}",
                headers=headers
            )
            
            if response.status_code == 200:
                user_data = response.json()
                
                logger.debug(
                    "user_info_retrieved",
                    user_id=user_id
                )
                
                return user_data
            
            elif response.status_code == 404:
                logger.warning(
                    "user_not_found",
                    user_id=user_id
                )
                return None
            
            else:
                logger.error(
                    "user_service_error",
                    user_id=user_id,
                    status_code=response.status_code
                )
                return None
        
        except httpx.TimeoutException:
            logger.warning(
                "user_service_timeout",
                user_id=user_id,
                timeout=self.timeout
            )
            return None
        
        except Exception as e:
            logger.error(
                "user_service_request_failed",
                user_id=user_id,
                error=str(e),
                exc_info=True
            )
            return None
    
    async def close(self):
        """í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ"""
        await self.client.aclose()
```

### 8.4 ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°

#### 8.4.1 PostgreSQL ì—°ê²° ì„¤ì •

```python
from aegis_shared.database import DatabaseManager
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

class DatabaseConfig:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
    
    def __init__(self):
        self.db_manager = DatabaseManager()
        
        # PostgreSQL ì—°ê²° ì„¤ì •
        self.engine = create_async_engine(
            self.db_manager.get_connection_string(),
            echo=False,
            pool_size=20,
            max_overflow=10,
            pool_timeout=30,
            pool_recycle=3600,
            pool_pre_ping=True
        )
        
        self.async_session_factory = sessionmaker(
            self.engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
    
    async def get_session(self) -> AsyncSession:
        """ì„¸ì…˜ ìƒì„±"""
        async with self.async_session_factory() as session:
            try:
                yield session
                await session.commit()
            except Exception:
                await session.rollback()
                raise
            finally:
                await session.close()
```

#### 8.4.2 Elasticsearch ì—°ê²° ì„¤ì •

```python
from elasticsearch import AsyncElasticsearch
from typing import Optional

class ElasticsearchConfig:
    """Elasticsearch ì„¤ì •"""
    
    def __init__(
        self,
        hosts: list,
        username: Optional[str] = None,
        password: Optional[str] = None
    ):
        self.client = AsyncElasticsearch(
            hosts=hosts,
            basic_auth=(username, password) if username and password else None,
            verify_certs=True,
            ca_certs="/path/to/ca.crt",
            request_timeout=30,
            max_retries=3,
            retry_on_timeout=True
        )
    
    async def health_check(self) -> bool:
        """Elasticsearch ìƒíƒœ í™•ì¸"""
        try:
            health = await self.client.cluster.health()
            
            logger.info(
                "elasticsearch_health_check",
                status=health['status'],
                number_of_nodes=health['number_of_nodes'],
                active_shards=health['active_shards']
            )
            
            return health['status'] in ['yellow', 'green']
        
        except Exception as e:
            logger.error(
                "elasticsearch_health_check_failed",
                error=str(e),
                exc_info=True
            )
            return False
    
    async def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        await self.client.close()
```

#### 8.4.3 Redis ì—°ê²° ì„¤ì •

```python
from aegis_shared.cache import RedisClient
import redis.asyncio as redis

class RedisConfig:
    """Redis ì„¤ì •"""
    
    def __init__(
        self,
        host: str,
        port: int = 6379,
        db: int = 0,
        password: Optional[str] = None
    ):
        self.client = redis.Redis(
            host=host,
            port=port,
            db=db,
            password=password,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5,
            retry_on_timeout=True,
            health_check_interval=30
        )
    
    async def health_check(self) -> bool:
        """Redis ìƒíƒœ í™•ì¸"""
        try:
            await self.client.ping()
            
            logger.debug("redis_health_check_passed")
            
            return True
        
        except Exception as e:
            logger.error(
                "redis_health_check_failed",
                error=str(e),
                exc_info=True
            )
            return False
    
    async def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        await self.client.close()
```

### 8.5 Prometheus ë©”íŠ¸ë¦­ ë…¸ì¶œ

```python
from prometheus_client import make_asgi_app, Counter, Histogram, Gauge
from fastapi import FastAPI

# FastAPI ì•±ì— Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
app = FastAPI()

# Prometheus ë©”íŠ¸ë¦­ ì•±
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

# ë©”íŠ¸ë¦­ ì •ì˜
http_requests_total = Counter(
    'logging_audit_http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'logging_audit_http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint']
)

# ë¯¸ë“¤ì›¨ì–´ë¡œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
@app.middleware("http")
async def metrics_middleware(request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    
    http_requests_total.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    http_request_duration_seconds.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)
    
    return response
```

### 8.6 Health Check ì—”ë“œí¬ì¸íŠ¸

```python
from fastapi import APIRouter, status
from typing import Dict, Any

router = APIRouter(prefix="/health", tags=["health"])

@router.get("", status_code=status.HTTP_200_OK)
async def health_check(
    db_config: DatabaseConfig = Depends(get_db_config),
    es_config: ElasticsearchConfig = Depends(get_es_config),
    redis_config: RedisConfig = Depends(get_redis_config)
) -> Dict[str, Any]:
    """í—¬ìŠ¤ ì²´í¬"""
    
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "checks": {}
    }
    
    # PostgreSQL ì²´í¬
    try:
        async with db_config.engine.connect() as conn:
            await conn.execute("SELECT 1")
        health_status["checks"]["postgresql"] = "healthy"
    except Exception as e:
        health_status["checks"]["postgresql"] = f"unhealthy: {str(e)}"
        health_status["status"] = "degraded"
    
    # Elasticsearch ì²´í¬
    if await es_config.health_check():
        health_status["checks"]["elasticsearch"] = "healthy"
    else:
        health_status["checks"]["elasticsearch"] = "unhealthy"
        health_status["status"] = "degraded"
    
    # Redis ì²´í¬
    if await redis_config.health_check():
        health_status["checks"]["redis"] = "healthy"
    else:
        health_status["checks"]["redis"] = "unhealthy"
        health_status["status"] = "degraded"
    
    return health_status

@router.get("/ready", status_code=status.HTTP_200_OK)
async def readiness_check() -> Dict[str, str]:
    """ì¤€ë¹„ ìƒíƒœ ì²´í¬ (Kubernetes Readiness Probe)"""
    return {"status": "ready"}

@router.get("/live", status_code=status.HTTP_200_OK)
async def liveness_check() -> Dict[str, str]:
    """ìƒì¡´ ìƒíƒœ ì²´í¬ (Kubernetes Liveness Probe)"""
    return {"status": "alive"}
```

---



## 9. Integration Testing Strategy

### 9.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
import pytest
from unittest.mock import Mock, AsyncMock, patch
from datetime import datetime, timedelta

class TestLogSearchService:
    """ë¡œê·¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def log_service(self):
        """ë¡œê·¸ ì„œë¹„ìŠ¤ í”½ìŠ¤ì²˜"""
        es_client = AsyncMock()
        redis_client = AsyncMock()
        return LogSearchService(es_client, redis_client)
    
    @pytest.mark.asyncio
    async def test_search_logs_success(self, log_service):
        """ë¡œê·¸ ê²€ìƒ‰ ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        # Given
        search_params = {
            'query': 'error',
            'service': 'user-service',
            'start_time': datetime.utcnow() - timedelta(hours=1),
            'end_time': datetime.utcnow(),
            'page': 1,
            'size': 20
        }
        
        log_service.es_client.search.return_value = {
            'hits': {
                'total': {'value': 10},
                'hits': [
                    {'_source': {'message': 'test error'}, '_id': '1', '_score': 1.0}
                ]
            },
            'took': 50
        }
        
        # When
        result = await log_service.search_logs(search_params)
        
        # Then
        assert result['total'] == 10
        assert len(result['logs']) == 1
        assert result['logs'][0]['message'] == 'test error'
    
    @pytest.mark.asyncio
    async def test_search_logs_elasticsearch_error(self, log_service):
        """Elasticsearch ì—ëŸ¬ í…ŒìŠ¤íŠ¸"""
        # Given
        search_params = {...}
        log_service.es_client.search.side_effect = elasticsearch.exceptions.ConnectionError()
        
        # When/Then
        with pytest.raises(ServiceException) as exc_info:
            await log_service.search_logs(search_params)
        
        assert exc_info.value.error_code == LoggingErrorCode.ELASTICSEARCH_CONNECTION_FAILED
```

### 9.2 í†µí•© í…ŒìŠ¤íŠ¸

```python
import pytest
from httpx import AsyncClient
from fastapi import status

@pytest.mark.integration
class TestLogSearchAPI:
    """ë¡œê·¸ ê²€ìƒ‰ API í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    async def client(self):
        """í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸"""
        async with AsyncClient(app=app, base_url="http://test") as ac:
            yield ac
    
    @pytest.mark.asyncio
    async def test_search_logs_endpoint(self, client, auth_token):
        """ë¡œê·¸ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
        # Given
        headers = {"Authorization": f"Bearer {auth_token}"}
        params = {
            "query": "error",
            "service": "user-service",
            "page": 1,
            "size": 20
        }
        
        # When
        response = await client.get(
            "/api/v1/logs/search",
            headers=headers,
            params=params
        )
        
        # Then
        assert response.status_code == status.HTTP_200_OK
        data = response.json()
        assert "items" in data
        assert "total" in data
        assert "page" in data
    
    @pytest.mark.asyncio
    async def test_search_logs_unauthorized(self, client):
        """ì¸ì¦ ì—†ì´ ë¡œê·¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        # When
        response = await client.get("/api/v1/logs/search")
        
        # Then
        assert response.status_code == status.HTTP_401_UNAUTHORIZED
```

### 9.3 E2E í…ŒìŠ¤íŠ¸

```python
@pytest.mark.e2e
class TestLogCollectionFlow:
    """ë¡œê·¸ ìˆ˜ì§‘ íë¦„ E2E í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_log_collection_to_search(self):
        """ë¡œê·¸ ìƒì„±ë¶€í„° ê²€ìƒ‰ê¹Œì§€ ì „ì²´ íë¦„ í…ŒìŠ¤íŠ¸"""
        # 1. ë¡œê·¸ ìƒì„±
        logger.info("test_log_message", test_id="e2e-test-123")
        
        # 2. Elasticsearch ì¸ë±ì‹± ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
        await asyncio.sleep(30)
        
        # 3. ë¡œê·¸ ê²€ìƒ‰
        response = await client.get(
            "/api/v1/logs/search",
            params={"query": "test_id:e2e-test-123"}
        )
        
        # 4. ê²€ì¦
        assert response.status_code == 200
        data = response.json()
        assert data['total'] > 0
        assert any('e2e-test-123' in str(log) for log in data['items'])
```

---

## 10. Performance Benchmarks

### 10.1 ì„±ëŠ¥ ëª©í‘œ

| ë©”íŠ¸ë¦­ | ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|--------|------|----------|
| ë¡œê·¸ ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„ (p95) | < 1ì´ˆ | Prometheus |
| ë¡œê·¸ ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„ (p99) | < 3ì´ˆ | Prometheus |
| ë¡œê·¸ ì¸ë±ì‹± ì§€ì—° | < 30ì´ˆ | Elasticsearch |
| API ì²˜ë¦¬ëŸ‰ (TPS) | > 1000 req/s | Load Testing |
| Elasticsearch ì¿¼ë¦¬ ì‹œê°„ | < 500ms | Elasticsearch |
| ìºì‹œ íˆíŠ¸ìœ¨ | > 70% | Redis |

### 10.2 ë¶€í•˜ í…ŒìŠ¤íŠ¸

```python
from locust import HttpUser, task, between

class LogSearchUser(HttpUser):
    """ë¡œê·¸ ê²€ìƒ‰ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
    
    wait_time = between(1, 3)
    
    def on_start(self):
        """í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œ ì¸ì¦"""
        response = self.client.post("/api/v1/auth/login", json={
            "username": "test_user",
            "password": "test_password"
        })
        self.token = response.json()['access_token']
    
    @task(3)
    def search_logs(self):
        """ë¡œê·¸ ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ 3)"""
        self.client.get(
            "/api/v1/logs/search",
            headers={"Authorization": f"Bearer {self.token}"},
            params={
                "query": "error",
                "service": "user-service",
                "page": 1,
                "size": 20
            }
        )
    
    @task(1)
    def get_recent_logs(self):
        """ìµœê·¼ ë¡œê·¸ ì¡°íšŒ (ê°€ì¤‘ì¹˜ 1)"""
        self.client.get(
            "/api/v1/logs/recent/user-service",
            headers={"Authorization": f"Bearer {self.token}"}
        )
```

---

## 11. Monitoring

### 11.1 Prometheus ë©”íŠ¸ë¦­

```yaml
# prometheus/alerts/logging-audit.yml
groups:
- name: logging_audit_alerts
  interval: 30s
  rules:
  # ë†’ì€ ì—ëŸ¬ìœ¨
  - alert: HighErrorRate
    expr: |
      rate(logging_audit_log_search_requests_total{status="error"}[5m]) > 0.05
    for: 5m
    labels:
      severity: warning
      service: logging-audit
    annotations:
      summary: "High error rate in Logging-Audit Service"
      description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
  
  # ëŠë¦° ì‘ë‹µ ì‹œê°„
  - alert: SlowLogSearch
    expr: |
      histogram_quantile(0.95, 
        rate(logging_audit_log_search_duration_seconds_bucket[5m])
      ) > 1.0
    for: 10m
    labels:
      severity: warning
      service: logging-audit
    annotations:
      summary: "Slow log search performance"
      description: "P95 latency is {{ $value }}s (threshold: 1s)"
  
  # Elasticsearch ì—°ê²° ì‹¤íŒ¨
  - alert: ElasticsearchConnectionFailure
    expr: |
      rate(logging_audit_elasticsearch_connection_errors_total[5m]) > 0
    for: 2m
    labels:
      severity: critical
      service: logging-audit
    annotations:
      summary: "Elasticsearch connection failures detected"
      description: "Connection error rate: {{ $value }}/s"
  
  # Elasticsearch í´ëŸ¬ìŠ¤í„° ìƒíƒœ
  - alert: ElasticsearchClusterUnhealthy
    expr: |
      logging_audit_elasticsearch_cluster_health < 1
    for: 5m
    labels:
      severity: critical
      service: logging-audit
    annotations:
      summary: "Elasticsearch cluster is unhealthy"
      description: "Cluster status is RED"
  
  # ë‚®ì€ ìºì‹œ íˆíŠ¸ìœ¨
  - alert: LowCacheHitRate
    expr: |
      rate(logging_audit_cache_hits_total[10m]) / 
      (rate(logging_audit_cache_hits_total[10m]) + rate(logging_audit_cache_misses_total[10m])) < 0.5
    for: 15m
    labels:
      severity: warning
      service: logging-audit
    annotations:
      summary: "Low cache hit rate"
      description: "Cache hit rate is {{ $value | humanizePercentage }} (threshold: 50%)"
```

### 11.2 Grafana ëŒ€ì‹œë³´ë“œ

```json
{
  "dashboard": {
    "title": "Logging & Audit Service Dashboard",
    "panels": [
      {
        "title": "Log Search Requests",
        "targets": [
          {
            "expr": "rate(logging_audit_log_search_requests_total[5m])",
            "legendFormat": "{{status}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Log Search Latency (P95, P99)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(logging_audit_log_search_duration_seconds_bucket[5m]))",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, rate(logging_audit_log_search_duration_seconds_bucket[5m]))",
            "legendFormat": "P99"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Elasticsearch Query Duration",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(logging_audit_elasticsearch_query_duration_seconds_bucket[5m]))",
            "legendFormat": "P95"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Cache Hit Rate",
        "targets": [
          {
            "expr": "rate(logging_audit_cache_hits_total[5m]) / (rate(logging_audit_cache_hits_total[5m]) + rate(logging_audit_cache_misses_total[5m]))",
            "legendFormat": "Hit Rate"
          }
        ],
        "type": "gauge"
      },
      {
        "title": "Active Log Searches",
        "targets": [
          {
            "expr": "logging_audit_active_log_searches",
            "legendFormat": "Active Searches"
          }
        ],
        "type": "stat"
      },
      {
        "title": "Elasticsearch Cluster Health",
        "targets": [
          {
            "expr": "logging_audit_elasticsearch_cluster_health",
            "legendFormat": "Health (0=red, 1=yellow, 2=green)"
          }
        ],
        "type": "stat"
      }
    ]
  }
}
```

---

## 12. API Specification (OpenAPI)

```yaml
openapi: 3.0.3
info:
  title: Logging & Audit Service API
  version: 1.0.0
  description: ì¤‘ì•™ ë¡œê¹… ë° ê°ì‚¬ ì¶”ì  ì„œë¹„ìŠ¤ API

servers:
  - url: https://api.aegis.com/logging-audit/v1
    description: Production
  - url: https://staging-api.aegis.com/logging-audit/v1
    description: Staging

security:
  - BearerAuth: []

paths:
  /logs/search:
    get:
      summary: ë¡œê·¸ ê²€ìƒ‰
      tags: [Logs]
      security:
        - BearerAuth: []
      parameters:
        - name: q
          in: query
          schema:
            type: string
          description: ê²€ìƒ‰ ì¿¼ë¦¬
        - name: service
          in: query
          schema:
            type: string
          description: ì„œë¹„ìŠ¤ í•„í„°
        - name: level
          in: query
          schema:
            type: string
            enum: [DEBUG, INFO, WARNING, ERROR, CRITICAL]
          description: ë¡œê·¸ ë ˆë²¨ í•„í„°
        - name: start_time
          in: query
          schema:
            type: string
            format: date-time
          description: ì‹œì‘ ì‹œê°„
        - name: end_time
          in: query
          schema:
            type: string
            format: date-time
          description: ì¢…ë£Œ ì‹œê°„
        - name: page
          in: query
          schema:
            type: integer
            minimum: 1
            default: 1
        - name: size
          in: query
          schema:
            type: integer
            minimum: 1
            maximum: 1000
            default: 20
      responses:
        '200':
          description: ì„±ê³µ
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PaginatedLogResponse'
        '400':
          description: ì˜ëª»ëœ ìš”ì²­
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '401':
          description: ì¸ì¦ ì‹¤íŒ¨
        '403':
          description: ê¶Œí•œ ì—†ìŒ
        '500':
          description: ì„œë²„ ì—ëŸ¬

  /audit/trail/{user_id}:
    get:
      summary: ì‚¬ìš©ì ê°ì‚¬ ì¶”ì  ì¡°íšŒ
      tags: [Audit]
      security:
        - BearerAuth: []
      parameters:
        - name: user_id
          in: path
          required: true
          schema:
            type: string
        - name: start_date
          in: query
          schema:
            type: string
            format: date-time
        - name: end_date
          in: query
          schema:
            type: string
            format: date-time
        - name: action_type
          in: query
          schema:
            type: string
        - name: page
          in: query
          schema:
            type: integer
            default: 1
        - name: size
          in: query
          schema:
            type: integer
            default: 50
      responses:
        '200':
          description: ì„±ê³µ
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/UserAuditTrailResponse'

components:
  securitySchemes:
    BearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT

  schemas:
    LogEntry:
      type: object
      properties:
        id:
          type: string
        timestamp:
          type: string
          format: date-time
        service:
          type: string
        level:
          type: string
        event_type:
          type: string
        message:
          type: string
        request_id:
          type: string
        user_id:
          type: string
        data:
          type: object

    PaginatedLogResponse:
      type: object
      properties:
        items:
          type: array
          items:
            $ref: '#/components/schemas/LogEntry'
        total:
          type: integer
        page:
          type: integer
        size:
          type: integer
        total_pages:
          type: integer

    ErrorResponse:
      type: object
      properties:
        error_code:
          type: string
        message:
          type: string
        details:
          type: object
        timestamp:
          type: string
          format: date-time
```

---

## 13. Database Schema

### 13.1 PostgreSQL ìŠ¤í‚¤ë§ˆ

```sql
-- Audit Logs í…Œì´ë¸”
CREATE TABLE audit_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    user_id VARCHAR(255) NOT NULL,
    user_role VARCHAR(50),
    session_id VARCHAR(255),
    service VARCHAR(100) NOT NULL,
    action_type VARCHAR(100) NOT NULL,
    action VARCHAR(255) NOT NULL,
    result VARCHAR(50) NOT NULL DEFAULT 'success',
    resource_type VARCHAR(100),
    resource_id VARCHAR(255),
    changes JSONB,
    details JSONB,
    ip_address VARCHAR(45),
    user_agent TEXT,
    request_id VARCHAR(255),
    correlation_id VARCHAR(255),
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_audit_user_timestamp ON audit_logs(user_id, timestamp DESC);
CREATE INDEX idx_audit_service_action ON audit_logs(service, action_type);
CREATE INDEX idx_audit_resource ON audit_logs(resource_type, resource_id);
CREATE INDEX idx_audit_timestamp_desc ON audit_logs(timestamp DESC);
CREATE INDEX idx_audit_request_id ON audit_logs(request_id);

-- Alerts í…Œì´ë¸”
CREATE TABLE alerts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    alert_type VARCHAR(100) NOT NULL,
    severity VARCHAR(20) NOT NULL,
    title VARCHAR(500) NOT NULL,
    message TEXT NOT NULL,
    service VARCHAR(100) NOT NULL,
    source_type VARCHAR(100),
    source_id VARCHAR(255),
    status VARCHAR(50) NOT NULL DEFAULT 'open',
    acknowledged_by VARCHAR(255),
    acknowledged_at TIMESTAMP WITH TIME ZONE,
    resolved_by VARCHAR(255),
    resolved_at TIMESTAMP WITH TIME ZONE,
    notification_channels JSONB,
    notification_status JSONB,
    metadata JSONB,
    tags JSONB
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_alert_severity_status ON alerts(severity, status);
CREATE INDEX idx_alert_service_type ON alerts(service, alert_type);
CREATE INDEX idx_alert_created_desc ON alerts(created_at DESC);

-- Search History í…Œì´ë¸”
CREATE TABLE search_history (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    user_id VARCHAR(255) NOT NULL,
    search_query TEXT NOT NULL,
    search_params JSONB NOT NULL,
    result_count INTEGER,
    execution_time_ms INTEGER,
    saved BOOLEAN DEFAULT FALSE,
    search_name VARCHAR(255)
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_search_user_created ON search_history(user_id, created_at DESC);
CREATE INDEX idx_search_saved ON search_history(user_id, saved) WHERE saved = TRUE;
```

### 13.2 ë§ˆì´ê·¸ë ˆì´ì…˜ (Alembic)

```python
"""create audit logs table

Revision ID: 001
Revises: 
Create Date: 2025-10-07 10:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '001'
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    # audit_logs í…Œì´ë¸” ìƒì„±
    op.create_table(
        'audit_logs',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('timestamp', sa.DateTime(timezone=True), nullable=False),
        sa.Column('user_id', sa.String(255), nullable=False),
        sa.Column('user_role', sa.String(50)),
        sa.Column('session_id', sa.String(255)),
        sa.Column('service', sa.String(100), nullable=False),
        sa.Column('action_type', sa.String(100), nullable=False),
        sa.Column('action', sa.String(255), nullable=False),
        sa.Column('result', sa.String(50), nullable=False),
        sa.Column('resource_type', sa.String(100)),
        sa.Column('resource_id', sa.String(255)),
        sa.Column('changes', postgresql.JSONB),
        sa.Column('details', postgresql.JSONB),
        sa.Column('ip_address', sa.String(45)),
        sa.Column('user_agent', sa.Text),
        sa.Column('request_id', sa.String(255)),
        sa.Column('correlation_id', sa.String(255)),
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False)
    )
    
    # ì¸ë±ìŠ¤ ìƒì„±
    op.create_index('idx_audit_user_timestamp', 'audit_logs', ['user_id', 'timestamp'])
    op.create_index('idx_audit_service_action', 'audit_logs', ['service', 'action_type'])
    op.create_index('idx_audit_resource', 'audit_logs', ['resource_type', 'resource_id'])
    op.create_index('idx_audit_timestamp_desc', 'audit_logs', [sa.text('timestamp DESC')])
    op.create_index('idx_audit_request_id', 'audit_logs', ['request_id'])

def downgrade():
    op.drop_table('audit_logs')
```

---

## 14. Configuration Management

### 14.1 í™˜ê²½ ë³€ìˆ˜

```python
from pydantic import BaseSettings, Field
from typing import Optional

class Settings(BaseSettings):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •"""
    
    # ì• í”Œë¦¬ì¼€ì´ì…˜
    APP_NAME: str = "logging-audit-service"
    APP_VERSION: str = "1.0.0"
    ENVIRONMENT: str = Field("production", env="ENVIRONMENT")
    DEBUG: bool = Field(False, env="DEBUG")
    
    # API
    API_HOST: str = Field("0.0.0.0", env="API_HOST")
    API_PORT: int = Field(8000, env="API_PORT")
    API_PREFIX: str = "/api/v1"
    
    # PostgreSQL
    POSTGRES_HOST: str = Field(..., env="POSTGRES_HOST")
    POSTGRES_PORT: int = Field(5432, env="POSTGRES_PORT")
    POSTGRES_DB: str = Field("aegis_logging", env="POSTGRES_DB")
    POSTGRES_USER: str = Field(..., env="POSTGRES_USER")
    POSTGRES_PASSWORD: str = Field(..., env="POSTGRES_PASSWORD")
    POSTGRES_POOL_SIZE: int = Field(20, env="POSTGRES_POOL_SIZE")
    POSTGRES_MAX_OVERFLOW: int = Field(10, env="POSTGRES_MAX_OVERFLOW")
    
    # Elasticsearch
    ELASTICSEARCH_HOSTS: list = Field(["elasticsearch:9200"], env="ELASTICSEARCH_HOSTS")
    ELASTICSEARCH_USER: Optional[str] = Field(None, env="ELASTICSEARCH_USER")
    ELASTICSEARCH_PASSWORD: Optional[str] = Field(None, env="ELASTICSEARCH_PASSWORD")
    ELASTICSEARCH_TIMEOUT: int = Field(30, env="ELASTICSEARCH_TIMEOUT")
    
    # Redis
    REDIS_HOST: str = Field("redis", env="REDIS_HOST")
    REDIS_PORT: int = Field(6379, env="REDIS_PORT")
    REDIS_DB: int = Field(0, env="REDIS_DB")
    REDIS_PASSWORD: Optional[str] = Field(None, env="REDIS_PASSWORD")
    
    # JWT
    JWT_SECRET_KEY: str = Field(..., env="JWT_SECRET_KEY")
    JWT_ALGORITHM: str = Field("HS256", env="JWT_ALGORITHM")
    
    # ë¡œê¹…
    LOG_LEVEL: str = Field("INFO", env="LOG_LEVEL")
    LOG_FORMAT: str = Field("json", env="LOG_FORMAT")
    
    # ë©”íŠ¸ë¦­
    METRICS_ENABLED: bool = Field(True, env="METRICS_ENABLED")
    METRICS_PORT: int = Field(9090, env="METRICS_PORT")
    
    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```

### 14.2 Kubernetes ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: logging-audit-config
  namespace: aegis
data:
  APP_NAME: "logging-audit-service"
  ENVIRONMENT: "production"
  API_HOST: "0.0.0.0"
  API_PORT: "8000"
  POSTGRES_HOST: "postgresql-service"
  POSTGRES_PORT: "5432"
  POSTGRES_DB: "aegis_logging"
  ELASTICSEARCH_HOSTS: '["elasticsearch-service:9200"]'
  REDIS_HOST: "redis-service"
  REDIS_PORT: "6379"
  LOG_LEVEL: "INFO"
  METRICS_ENABLED: "true"
```

### 14.3 Kubernetes Secret

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: logging-audit-secrets
  namespace: aegis
type: Opaque
stringData:
  POSTGRES_USER: "logging_user"
  POSTGRES_PASSWORD: "secure_password_here"
  ELASTICSEARCH_USER: "elastic"
  ELASTICSEARCH_PASSWORD: "elastic_password_here"
  REDIS_PASSWORD: "redis_password_here"
  JWT_SECRET_KEY: "jwt_secret_key_here"
```

---

## 15. Logging Strategy

### 15.1 ë¡œê¹… ë ˆë²¨

- **DEBUG**: ê°œë°œ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš©, ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
- **INFO**: ì¼ë°˜ì ì¸ ì •ë³´ì„± ë©”ì‹œì§€, ì£¼ìš” ì´ë²¤íŠ¸
- **WARNING**: ê²½ê³  ë©”ì‹œì§€, ì ì¬ì  ë¬¸ì œ
- **ERROR**: ì—ëŸ¬ ë©”ì‹œì§€, ë³µêµ¬ ê°€ëŠ¥í•œ ì—ëŸ¬
- **CRITICAL**: ì¹˜ëª…ì  ì—ëŸ¬, ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”

### 15.2 ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸

ëª¨ë“  ë¡œê·¸ì— ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸:
- `request_id`: ìš”ì²­ ì¶”ì  ID
- `user_id`: ì‚¬ìš©ì ID (ì¸ì¦ëœ ê²½ìš°)
- `service`: ì„œë¹„ìŠ¤ ì´ë¦„
- `environment`: í™˜ê²½ (dev/staging/prod)
- `version`: ì„œë¹„ìŠ¤ ë²„ì „
- `timestamp`: ISO8601 í˜•ì‹ íƒ€ì„ìŠ¤íƒ¬í”„

### 15.3 ë¡œê·¸ ë³´ê´€ ì •ì±…

| ë¡œê·¸ íƒ€ì… | ë³´ê´€ ê¸°ê°„ | ìŠ¤í† ë¦¬ì§€ |
|----------|----------|---------|
| Application Logs | 30ì¼ | Elasticsearch Hot |
| Error Logs | 90ì¼ | Elasticsearch Hot â†’ Warm |
| Audit Logs | 7ë…„ | Elasticsearch + PostgreSQL |
| Security Logs | 1ë…„ | Elasticsearch + Archive |

---



